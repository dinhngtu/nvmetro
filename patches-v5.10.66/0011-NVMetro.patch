From c9db3c34403559cfbd5dee7957f44528a5d8f97c Mon Sep 17 00:00:00 2001
From: Tu Dinh <dinhngoc.tu@irit.fr>
Date: Thu, 13 Jan 2022 17:23:34 +0000
Subject: [PATCH 11/11] NVMetro

---
 drivers/nvme/host/core.c            |  52 +--
 drivers/nvme/host/nvme.h            |   1 +
 drivers/nvme/host/pci.c             |  11 +-
 drivers/nvme/mdev/Kconfig           |  21 ++
 drivers/nvme/mdev/Makefile          |   4 +-
 drivers/nvme/mdev/adm.c             | 356 ++++++++++---------
 drivers/nvme/mdev/bpf.c             |  67 ++++
 drivers/nvme/mdev/bpf_sm.c          | 127 +++++++
 drivers/nvme/mdev/bpf_sm.h          |  19 +
 drivers/nvme/mdev/events.c          |  30 +-
 drivers/nvme/mdev/host.c            | 167 +++++----
 drivers/nvme/mdev/instance.c        | 253 +++++++------
 drivers/nvme/mdev/io.c              | 528 ++++++++++++++++------------
 drivers/nvme/mdev/iothreads.c       | 163 +++++++++
 drivers/nvme/mdev/iothreads.h       |  57 +++
 drivers/nvme/mdev/irq.c             |  39 +-
 drivers/nvme/mdev/mdev.h            |  14 +-
 drivers/nvme/mdev/mmio.c            | 125 +++----
 drivers/nvme/mdev/notifyfd.c        | 429 ++++++++++++++++++++++
 drivers/nvme/mdev/notifyfd.h        |  44 +++
 drivers/nvme/mdev/pci.c             |  66 ++--
 drivers/nvme/mdev/priv.h            | 322 ++++++++++-------
 drivers/nvme/mdev/udata.c           |  84 +++--
 drivers/nvme/mdev/vcq.c             |  81 +++--
 drivers/nvme/mdev/vctrl.c           | 292 +++++++++------
 drivers/nvme/mdev/viommu.c          | 106 +++---
 drivers/nvme/mdev/vns.c             | 115 +++---
 drivers/nvme/mdev/vsq.c             | 101 ++++--
 drivers/vfio/vfio_iommu_type1.c     |  22 +-
 include/linux/bpf_nvme_mdev.h       |  74 ++++
 include/linux/bpf_types.h           |   4 +
 include/linux/notifier.h            |  11 +-
 include/uapi/linux/bpf.h            |   1 +
 include/uapi/linux/nvme_mdev.h      |  43 +++
 kernel/fork.c                       |   1 +
 kernel/notifier.c                   |  19 +-
 samples/bpf/.gitignore              |   1 +
 samples/bpf/Makefile                |   7 +
 samples/bpf/nmbpf_encrypt_ip_kern.c |  47 +++
 samples/bpf/nmbpf_encrypt_kern.c    |  41 +++
 samples/bpf/nmbpf_passthrough.c     |  12 +
 samples/bpf/nmbpf_replicate.c       |  18 +
 samples/bpf/nvme_mdev_kern.c        |  18 +
 samples/bpf/nvme_mdev_user.c        | 100 ++++++
 tools/bpf/bpftool/prog.c            |   1 +
 tools/include/uapi/linux/bpf.h      |   1 +
 tools/lib/bpf/libbpf.c              |   2 +-
 tools/lib/bpf/libbpf_probes.c       |   1 +
 48 files changed, 2864 insertions(+), 1234 deletions(-)
 create mode 100644 drivers/nvme/mdev/bpf.c
 create mode 100644 drivers/nvme/mdev/bpf_sm.c
 create mode 100644 drivers/nvme/mdev/bpf_sm.h
 create mode 100644 drivers/nvme/mdev/iothreads.c
 create mode 100644 drivers/nvme/mdev/iothreads.h
 create mode 100644 drivers/nvme/mdev/notifyfd.c
 create mode 100644 drivers/nvme/mdev/notifyfd.h
 create mode 100644 include/linux/bpf_nvme_mdev.h
 create mode 100644 include/uapi/linux/nvme_mdev.h
 create mode 100644 samples/bpf/nmbpf_encrypt_ip_kern.c
 create mode 100644 samples/bpf/nmbpf_encrypt_kern.c
 create mode 100644 samples/bpf/nmbpf_passthrough.c
 create mode 100644 samples/bpf/nmbpf_replicate.c
 create mode 100644 samples/bpf/nvme_mdev_kern.c
 create mode 100644 samples/bpf/nvme_mdev_user.c

diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 5712e9dcfabc..38508159fec3 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -90,10 +90,12 @@ static struct class *nvme_class;
 static struct class *nvme_subsys_class;
 
 static void nvme_ns_remove(struct nvme_ns *ns);
-static int nvme_revalidate_disk(struct gendisk *disk);
 static void nvme_put_subsystem(struct nvme_subsystem *subsys);
 static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 					   unsigned nsid);
+static u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
+			       u8 opcode);
+static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects);
 
 static void nvme_update_bdev_size(struct gendisk *disk)
 {
@@ -491,7 +493,7 @@ bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
 		case NVME_CTRL_CONNECTING:
 		case NVME_CTRL_SUSPENDED:
 			changed = true;
-			/* FALLTHRU */
+			fallthrough;
 		default:
 			break;
 		}
@@ -1169,14 +1171,7 @@ static u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 
 static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects)
 {
-	/*
-	 * Revalidate LBA changes prior to unfreezing. This is necessary to
-	 * prevent memory corruption if a logical block size was changed by
-	 * this command.
-	 */
-	if (effects & NVME_CMD_EFFECTS_LBCC)
-		nvme_update_formats(ctrl);
-	if (effects & (NVME_CMD_EFFECTS_LBCC | NVME_CMD_EFFECTS_CSE_MASK)) {
+	if (effects & NVME_CMD_EFFECTS_CSE_MASK) {
 		nvme_unfreeze(ctrl);
 		nvme_mpath_unfreeze(ctrl->subsys);
 		mutex_unlock(&ctrl->subsys->lock);
@@ -1681,38 +1676,6 @@ static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
 			metadata, meta_len, lower_32_bits(io.slba), NULL, 0);
 }
 
-static u32 nvme_known_admin_effects(u8 opcode)
-{
-	switch (opcode) {
-	case nvme_admin_format_nvm:
-		return NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC |
-					NVME_CMD_EFFECTS_CSE_MASK;
-	case nvme_admin_sanitize_nvm:
-		return NVME_CMD_EFFECTS_CSE_MASK;
-	default:
-		break;
-	}
-	return 0;
-}
-
-static void nvme_update_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
-{
-	nvme_mdev_ns_state_changed(ctrl, ns, false);
-}
-
-static void nvme_update_formats(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns;
-
-	down_read(&ctrl->namespaces_rwsem);
-	list_for_each_entry(ns, &ctrl->namespaces, list)
-		if (ns->disk && nvme_revalidate_disk(ns->disk))
-			nvme_set_queue_dying(ns);
-		else
-			nvme_update_ns(ctrl, ns);
-	up_read(&ctrl->namespaces_rwsem);
-}
-
 static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 			struct nvme_passthru_cmd __user *ucmd)
 {
@@ -2257,11 +2220,12 @@ static void nvme_set_chunk_sectors(struct nvme_ns *ns, struct nvme_id_ns *id)
 	struct nvme_ctrl *ctrl = ns->ctrl;
 	u32 iob;
 
+	ns->noiob = le16_to_cpu(id->noiob);
 	if ((ctrl->quirks & NVME_QUIRK_STRIPE_SIZE) &&
 	    is_power_of_2(ctrl->max_hw_sectors))
 		iob = ctrl->max_hw_sectors;
 	else
-		iob = nvme_lba_to_sect(ns, le16_to_cpu(id->noiob));
+		iob = nvme_lba_to_sect(ns, ns->noiob);
 
 	if (!iob)
 		return;
@@ -2858,7 +2822,7 @@ static void nvme_init_subnqn(struct nvme_subsystem *subsys, struct nvme_ctrl *ct
 	size_t nqnlen;
 	int off;
 
-	if (!(ctrl->quirks & NVME_QUIRK_IGNORE_DEV_SUBNQN)) {
+	if(!(ctrl->quirks & NVME_QUIRK_IGNORE_DEV_SUBNQN)) {
 		nqnlen = strnlen(id->subnqn, NVMF_NQN_SIZE);
 		if (nqnlen > 0 && nqnlen < NVMF_NQN_SIZE) {
 			strlcpy(subsys->subnqn, id->subnqn, NVMF_NQN_SIZE);
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 39b2edfb85a7..160e4c67b32a 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -461,6 +461,7 @@ struct nvme_ns {
 #define NVME_NS_REMOVING	0
 #define NVME_NS_DEAD     	1
 #define NVME_NS_ANA_PENDING	2
+	u16 noiob;
 
 	struct nvme_fault_inject fault_inject;
 
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index e8a3d5c72bc5..dd1321bdb6fe 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -1600,7 +1600,6 @@ static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 	memset((void *)nvmeq->cqes, 0, CQ_SIZE(nvmeq));
 	nvme_dbbuf_init(dev, nvmeq, qid);
 	dev->online_queues++;
-
 	wmb(); /* ensure the first interrupt sees the initialization */
 
 	if (test_bit(NVMEQ_EXTERNAL, &nvmeq->flags))
@@ -2188,9 +2187,9 @@ static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 	 * but some Apple controllers require all queues to use the first
 	 * vector.
 	 */
-	irq_queues = 1;
-	if (!(dev->ctrl.quirks & NVME_QUIRK_SINGLE_VECTOR))
-		irq_queues += (nr_io_queues - this_p_queues);
+	if (dev->ctrl.quirks & NVME_QUIRK_SINGLE_VECTOR)
+		irq_queues = 1;
+
 	return pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues,
 			      PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
 }
@@ -3069,7 +3068,7 @@ static int nvme_ext_queue_poll(struct nvme_ctrl *ctrl, u16 qid,
 		u16 tag = results[j].tag & 0xFFFF;
 		struct nvme_ext_iod *iod = &nvmeq->ext.iods[tag];
 
-		if (WARN_ON(tag >= nvmeq->q_depth || iod->nprps == -1))
+		if (WARN_ON_ONCE(tag >= nvmeq->q_depth || iod->nprps == -1))
 			continue;
 
 		results[j].tag = iod->user_tag;
@@ -3078,7 +3077,7 @@ static int nvme_ext_queue_poll(struct nvme_ctrl *ctrl, u16 qid,
 		nvmeq->ext.inflight--;
 	}
 
-	WARN_ON(nvmeq->ext.inflight < 0);
+	WARN_ON_ONCE(nvmeq->ext.inflight < 0);
 	return i;
 }
 
diff --git a/drivers/nvme/mdev/Kconfig b/drivers/nvme/mdev/Kconfig
index 1ace298a364d..3335d1df4739 100644
--- a/drivers/nvme/mdev/Kconfig
+++ b/drivers/nvme/mdev/Kconfig
@@ -18,7 +18,28 @@ config NVME_MDEV_VFIO
 config NVME_MDEV_VFIO_GENERIC_IO
 	bool "Use generic block layer IO"
 	depends on NVME_MDEV_VFIO
+	default n
 	help
 	  Send the IO through the block layer using polled IO queues,
 	  instead of dedicated mdev queues
 	  If unsure, say N.
+
+config NVME_MDEV_BPF
+	bool "NVMe Mediated device BPF support"
+	depends on NVME_MDEV_VFIO
+	depends on BPF_SYSCALL
+	help
+	  This provides support for attaching eBPF programs to
+	  virtual NVMe queues on a mediated NVMe device.
+
+config NMBPF_DEBUG
+	bool "NVMe-MDev BPF state machine debug"
+	depends on NVME_MDEV_BPF
+	default n
+	help
+	  Print state machine information at each BPF hook transition.
+
+config NMBPF_POLL_ONCE
+	bool "Poll once per vctrl"
+	depends on NVME_MDEV_VFIO
+	default n
diff --git a/drivers/nvme/mdev/Makefile b/drivers/nvme/mdev/Makefile
index 114016c48476..5d09b1b0d31c 100644
--- a/drivers/nvme/mdev/Makefile
+++ b/drivers/nvme/mdev/Makefile
@@ -1,5 +1,7 @@
 
 obj-$(CONFIG_NVME_MDEV_VFIO) 	+=	nvme-mdev.o
+obj-$(CONFIG_NVME_MDEV_BPF)	+=	bpf.o
 
-nvme-mdev-y += adm.o events.o instance.o host.o io.o irq.o \
+nvme-mdev-y += adm.o events.o instance.o host.o io.o iothreads.o irq.o \
 	       udata.o viommu.o vns.o vsq.o vcq.o vctrl.o mmio.o pci.o
+nvme-mdev-$(CONFIG_NVME_MDEV_BPF) += notifyfd.o bpf_sm.o
diff --git a/drivers/nvme/mdev/adm.c b/drivers/nvme/mdev/adm.c
index 39a7ad252c69..7ff63b78df7c 100644
--- a/drivers/nvme/mdev/adm.c
+++ b/drivers/nvme/mdev/adm.c
@@ -17,27 +17,17 @@ struct adm_ctx {
 	unsigned int datalen;
 };
 
-/*Identify Controller */
-static int nvme_mdev_adm_handle_id_cntrl(struct adm_ctx *ctx)
+void __nvme_mdev_adm_id_vctrl(struct nvme_mdev_vctrl *vctrl,
+			      struct nvme_mdev_hctrl *hctrl,
+			      struct nvme_id_ctrl *id)
 {
-	int ret;
-	const struct nvme_identify *in = &ctx->in->identify;
-	struct nvme_id_ctrl *id;
-
-	if (in->nsid != 0)
-		return DNR(NVME_SC_INVALID_FIELD);
-
-	id =  kzalloc(sizeof(*id), GFP_KERNEL);
-	if (!id)
-		return NVME_SC_INTERNAL;
-
 	/** Controller Capabilities and Features ************************/
 	// PCI vendor ID
 	store_le16(&id->vid, NVME_MDEV_PCI_VENDOR_ID);
 	// PCI Subsystem Vendor ID
 	store_le16(&id->ssvid, NVME_MDEV_PCI_SUBVENDOR_ID);
 	// Serial Number
-	store_strsp(id->sn, ctx->vctrl->serial);
+	store_strsp(id->sn, vctrl->serial);
 	// Model Number
 	store_strsp(id->mn, "NVMe MDEV virtual device");
 	// Firmware Revision
@@ -49,7 +39,7 @@ static int nvme_mdev_adm_handle_id_cntrl(struct adm_ctx *ctx)
 	// Controller Multi-Path I/O and Namespace Sharing Capabilities
 	id->cmic = 0;
 	// Maximum Data Transfer Size (power of two, in page size units)
-	id->mdts = ctx->hctrl->mdts;
+	id->mdts = hctrl->mdts;
 	// controller ID
 	id->cntlid = 0;
 	// NVME supported version
@@ -65,11 +55,11 @@ static int nvme_mdev_adm_handle_id_cntrl(struct adm_ctx *ctx)
 
 	/*Admin Command Set Attributes & Optional Controller Capabilities */
 	// Optional Admin Command Support
-	id->oacs = ctx->vctrl->mmio.shadow_db_supported ?
-			NVME_CTRL_OACS_DBBUF_SUPP : 0;
+	id->oacs =
+		vctrl->mmio.shadow_db_supported ? NVME_CTRL_OACS_DBBUF_SUPP : 0;
 	// Abort Command Limit (dummy, zero based)
 	id->acl = 3;
-	 // Asynchronous Event Request Limit (zero based)
+	// Asynchronous Event Request Limit (zero based)
 	id->aerl = MAX_AER_COMMANDS - 1;
 	// Firmware Updates (dummy)
 	id->frmw = 3;
@@ -130,7 +120,7 @@ static int nvme_mdev_adm_handle_id_cntrl(struct adm_ctx *ctx)
 	id->nn = MAX_VIRTUAL_NAMESPACES;
 	// Optional NVM Command Support
 	// (we add dsm and write zeros if host supports them)
-	id->oncs = ctx->hctrl->oncs;
+	id->oncs = hctrl->oncs;
 	// TODOLATER: IO: Fused Operation Support
 	id->fuses = 0;
 	// Format NVM Attributes (don't support)
@@ -148,12 +138,85 @@ static int nvme_mdev_adm_handle_id_cntrl(struct adm_ctx *ctx)
 	// SGL Support
 	id->sgls = 0;
 	// NVM Subsystem NVMe Qualified Name
-	strncpy(id->subnqn, ctx->vctrl->subnqn, sizeof(id->subnqn));
+	strncpy(id->subnqn, vctrl->subnqn, sizeof(id->subnqn));
 
 	/******************Power state descriptors ***********************/
 	store_le16(&id->psd[0].max_power, 0x9c4); // dummy
 	store_le32(&id->psd[0].entry_lat, 0x10);
 	store_le32(&id->psd[0].exit_lat, 0x4);
+}
+
+void __nvme_mdev_adm_id_vns(struct nvme_mdev_vctrl *vctrl,
+			    struct nvme_mdev_vns *ns, struct nvme_id_ns *idns)
+{
+	//Namespace Size
+	store_le64(&idns->nsze, ns->ns_size);
+	// Namespace Capacity
+	store_le64(&idns->ncap, ns->ns_size);
+	// Namespace Utilization
+	store_le64(&idns->nuse, ns->ns_size);
+	// Namespace Features (nothing to set here yet)
+	idns->nsfeat = 0;
+	// Number of LBA Formats (dummy, zero based)
+	idns->nlbaf = 0;
+	// Formatted LBA Size (current LBA format in use)
+	// + external metadata bit
+	idns->flbas = 0;
+	// Metadata Capabilities
+	idns->mc = 0;
+	// End-to-end Data Protection Capabilities
+	idns->dpc = 0;
+	// End-to-end Data Protection Type Settings
+	idns->dps = 0;
+	// Namespace Multi-path I/O and Namespace Sharing Capabilities
+	idns->nmic = 0;
+	// Reservation Capabilities
+	idns->rescap = 0;
+	// Format Progress Indicator (dummy)
+	idns->fpi = 0;
+	// Namespace Atomic Write Unit Normal
+	idns->nawun = 0;
+	// Namespace Atomic Write Unit Power Fail
+	idns->nawupf = 0;
+	// Namespace Atomic Compare & Write Unit
+	idns->nacwu = 0;
+	// Namespace Atomic Boundary Size Normal
+	idns->nabsn = 0;
+	// Namespace Atomic Boundary Offset
+	idns->nabo = 0;
+	// Namespace Atomic Boundary Size Power Fail
+	idns->nabspf = 0;
+	// Namespace Optimal IO Boundary
+	idns->noiob = ns->noiob;
+	// NVM Capacity (another capacity but in bytes)
+	idns->nvmcap[0] = 0;
+
+	// TODOLATER: NS: support NGUID/EUI64
+	idns->nguid[0] = 0;
+	idns->eui64[0] = 0;
+	// format 0 metadata size
+	idns->lbaf[0].ms = 0;
+	// format 0 block size (in power of two)
+	idns->lbaf[0].ds = ns->blksize_shift;
+	// format 0 relative performance
+	idns->lbaf[0].rp = 0;
+}
+
+/*Identify Controller */
+static int nvme_mdev_adm_handle_id_cntrl(struct adm_ctx *ctx)
+{
+	int ret;
+	const struct nvme_identify *in = &ctx->in->identify;
+	struct nvme_id_ctrl *id;
+
+	if (in->nsid != 0)
+		return DNR(NVME_SC_INVALID_FIELD);
+
+	id = kzalloc(sizeof(*id), GFP_KERNEL);
+	if (!id)
+		return NVME_SC_INTERNAL;
+
+	__nvme_mdev_adm_id_vctrl(ctx->vctrl, ctx->hctrl, id);
 
 	ret = nvme_mdev_write_to_udata(&ctx->udatait, id, sizeof(*id));
 	kfree(id);
@@ -171,62 +234,12 @@ static int nvme_mdev_adm_handle_id_ns(struct adm_ctx *ctx)
 		return DNR(NVME_SC_INVALID_NS);
 
 	/* Allocate return structure*/
-	idns =  kzalloc(NVME_IDENTIFY_DATA_SIZE, GFP_KERNEL);
+	idns = kzalloc(NVME_IDENTIFY_DATA_SIZE, GFP_KERNEL);
 	if (!idns)
 		return NVME_SC_INTERNAL;
 
 	if (ctx->ns) {
-		//Namespace Size
-		store_le64(&idns->nsze, ctx->ns->ns_size);
-		// Namespace Capacity
-		store_le64(&idns->ncap, ctx->ns->ns_size);
-		// Namespace Utilization
-		store_le64(&idns->nuse, ctx->ns->ns_size);
-		// Namespace Features (nothing to set here yet)
-		idns->nsfeat = 0;
-		// Number of LBA Formats (dummy, zero based)
-		idns->nlbaf = 0;
-		// Formatted LBA Size (current LBA format in use)
-		// + external metadata bit
-		idns->flbas = 0;
-		// Metadata Capabilities
-		idns->mc = 0;
-		// End-to-end Data Protection Capabilities
-		idns->dpc = 0;
-		// End-to-end Data Protection Type Settings
-		idns->dps = 0;
-		// Namespace Multi-path I/O and Namespace Sharing Capabilities
-		idns->nmic = 0;
-		// Reservation Capabilities
-		idns->rescap = 0;
-		// Format Progress Indicator (dummy)
-		idns->fpi = 0;
-		// Namespace Atomic Write Unit Normal
-		idns->nawun = 0;
-		// Namespace Atomic Write Unit Power Fail
-		idns->nawupf = 0;
-		// Namespace Atomic Compare & Write Unit
-		idns->nacwu = 0;
-		// Namespace Atomic Boundary Size Normal
-		idns->nabsn = 0;
-		// Namespace Atomic Boundary Offset
-		idns->nabo = 0;
-		// Namespace Atomic Boundary Size Power Fail
-		idns->nabspf = 0;
-		// Namespace Optimal IO Boundary
-		idns->noiob = ctx->ns->noiob;
-		// NVM Capacity (another capacity but in bytes)
-		idns->nvmcap[0]  = 0;
-
-		// TODOLATER: NS: support NGUID/EUI64
-		idns->nguid[0] = 0;
-		idns->eui64[0] = 0;
-		// format 0 metadata size
-		idns->lbaf[0].ms = 0;
-		// format 0 block size (in power of two)
-		idns->lbaf[0].ds = ctx->ns->blksize_shift;
-		// format 0 relative performance
-		idns->lbaf[0].rp = 0;
+		__nvme_mdev_adm_id_vns(ctx->vctrl, ctx->ns, idns);
 	}
 
 	ret = nvme_mdev_write_to_udata(&ctx->udatait, idns,
@@ -288,13 +301,12 @@ static int nvme_mdev_adm_handle_id_active_ns_list(struct adm_ctx *ctx)
 }
 
 /* Handle Identify command*/
-static int nvme_mdev_adm_handle_id(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_id(struct adm_ctx *ctx)
 {
 	const struct nvme_identify *in = &ctx->in->identify;
 
-	int ret = nvme_mdev_udata_iter_set_dptr(&ctx->udatait,
-						&ctx->in->common.dptr,
-						NVME_IDENTIFY_DATA_SIZE);
+	int ret = nvme_mdev_udata_iter_set_dptr(
+		&ctx->udatait, &ctx->in->common.dptr, NVME_IDENTIFY_DATA_SIZE);
 
 	u32 nsid = le32_to_cpu(in->nsid);
 
@@ -329,7 +341,7 @@ static int nvme_mdev_adm_handle_id(struct adm_ctx *ctx)
 }
 
 /* Error log for AER */
-static int nvme_mdev_adm_handle_get_log_page_err(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_get_log_page_err(struct adm_ctx *ctx)
 {
 	struct nvme_err_log_entry dummy_entry;
 	int ret;
@@ -337,8 +349,7 @@ static int nvme_mdev_adm_handle_get_log_page_err(struct adm_ctx *ctx)
 	// write one dummy entry with 0 error count
 	memset(&dummy_entry, 0, sizeof(dummy_entry));
 
-	ret = nvme_mdev_write_to_udata(&ctx->udatait,
-				       &dummy_entry,
+	ret = nvme_mdev_write_to_udata(&ctx->udatait, &dummy_entry,
 				       min((unsigned int)sizeof(dummy_entry),
 					   ctx->datalen));
 
@@ -346,22 +357,22 @@ static int nvme_mdev_adm_handle_get_log_page_err(struct adm_ctx *ctx)
 }
 
 /* This log page allows to tell user about connected/disconnected namespaces */
-static int nvme_mdev_adm_handle_get_log_page_changed_ns(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_get_log_page_changed_ns(struct adm_ctx *ctx)
 {
 	unsigned int datasize = min(ctx->vctrl->ns_log_size * 4, ctx->datalen);
 
-	int ret = nvme_mdev_write_to_udata(&ctx->udatait,
-					   &ctx->vctrl->ns_log, datasize);
+	int ret = nvme_mdev_write_to_udata(&ctx->udatait, &ctx->vctrl->ns_log,
+					   datasize);
 
 	nvme_mdev_vns_log_reset(ctx->vctrl);
 	return nvme_mdev_translate_error(ret);
 }
 
 /* S.M.A.R.T. log*/
-static int nvme_mdev_adm_handle_get_log_page_smart(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_get_log_page_smart(struct adm_ctx *ctx)
 {
-	unsigned int datasize = min_t(unsigned int,
-			sizeof(struct nvme_smart_log), ctx->datalen);
+	unsigned int datasize = min_t(
+		unsigned int, sizeof(struct nvme_smart_log), ctx->datalen);
 	int ret;
 	struct nvme_smart_log *log = kzalloc(sizeof(*log), GFP_KERNEL);
 
@@ -379,11 +390,11 @@ static int nvme_mdev_adm_handle_get_log_page_smart(struct adm_ctx *ctx)
 }
 
 /* FW slot log - useless */
-static int nvme_mdev_adm_handle_get_log_page_fw_slot(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_get_log_page_fw_slot(struct adm_ctx *ctx)
 {
-	unsigned int datasize = min_t(unsigned int,
-				      sizeof(struct nvme_fw_slot_info_log),
-				      ctx->datalen);
+	unsigned int datasize =
+		min_t(unsigned int, sizeof(struct nvme_fw_slot_info_log),
+		      ctx->datalen);
 	int ret;
 	struct nvme_fw_slot_info_log *log = kzalloc(sizeof(*log), GFP_KERNEL);
 
@@ -396,7 +407,7 @@ static int nvme_mdev_adm_handle_get_log_page_fw_slot(struct adm_ctx *ctx)
 }
 
 /* Response to GET LOG PAGE command */
-static int nvme_mdev_adm_handle_get_log_page(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_get_log_page(struct adm_ctx *ctx)
 {
 	const struct nvme_get_log_page_command *in = &ctx->in->get_log_page;
 	u8 log_page_id = ctx->in->get_log_page.lid;
@@ -413,27 +424,27 @@ static int nvme_mdev_adm_handle_get_log_page(struct adm_ctx *ctx)
 
 	/* ACK the AER */
 	if ((in->lsp & 0x80) == 0)
-		nvme_mdev_event_process_ack(ctx->vctrl, log_page_id);
+		__nvme_mdev_event_process_ack(ctx->vctrl, log_page_id);
 
 	/* map data pointer */
-	ret = nvme_mdev_udata_iter_set_dptr(&ctx->udatait,
-					    &in->dptr, ctx->datalen);
+	ret = nvme_mdev_udata_iter_set_dptr(&ctx->udatait, &in->dptr,
+					    ctx->datalen);
 	if (ret)
 		return nvme_mdev_translate_error(ret);
 
 	switch (log_page_id) {
 	case NVME_LOG_ERROR:
 		_DBG(ctx->vctrl, "ADMINQ: GET_LOG_PAGE : ERRLOG\n");
-		return nvme_mdev_adm_handle_get_log_page_err(ctx);
+		return __nvme_mdev_adm_handle_get_log_page_err(ctx);
 	case NVME_LOG_CHANGED_NS:
 		_DBG(ctx->vctrl, "ADMINQ: GET_LOG_PAGE : CHANGED_NS\n");
-		return nvme_mdev_adm_handle_get_log_page_changed_ns(ctx);
+		return __nvme_mdev_adm_handle_get_log_page_changed_ns(ctx);
 	case NVME_LOG_SMART:
 		_DBG(ctx->vctrl, "ADMINQ: GET_LOG_PAGE : SMART\n");
-		return nvme_mdev_adm_handle_get_log_page_smart(ctx);
+		return __nvme_mdev_adm_handle_get_log_page_smart(ctx);
 	case NVME_LOG_FW_SLOT:
 		_DBG(ctx->vctrl, "ADMINQ: GET_LOG_PAGE : FWSLOT\n");
-		return nvme_mdev_adm_handle_get_log_page_fw_slot(ctx);
+		return __nvme_mdev_adm_handle_get_log_page_fw_slot(ctx);
 	default:
 		_DBG(ctx->vctrl, "ADMINQ: GET_LOG_PAGE : log page 0x%02x\n",
 		     log_page_id);
@@ -442,7 +453,7 @@ static int nvme_mdev_adm_handle_get_log_page(struct adm_ctx *ctx)
 }
 
 /* Response to CREATE CQ command */
-static int nvme_mdev_adm_handle_create_cq(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_create_cq(struct adm_ctx *ctx)
 {
 	int irq = -1, ret;
 	struct nvme_mdev_vctrl *vctrl = ctx->vctrl;
@@ -453,12 +464,12 @@ static int nvme_mdev_adm_handle_create_cq(struct adm_ctx *ctx)
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
 				   RSRV_NSID | RSRV_DW23 | RSRV_DPTR_PRP2 |
-				   RSRV_MPTR | RSRV_DW12_15))
+					   RSRV_MPTR | RSRV_DW12_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	/* QID checks*/
-	if (!cqid ||
-	    cqid >= MAX_VIRTUAL_QUEUES || test_bit(cqid, vctrl->vcq_en))
+	if (!cqid || cqid >= MAX_VIRTUAL_QUEUES ||
+	    rcu_access_pointer(vctrl->vcqs[cqid]))
 		return DNR(NVME_SC_QID_INVALID);
 
 	/* Queue size checks*/
@@ -475,39 +486,47 @@ static int nvme_mdev_adm_handle_create_cq(struct adm_ctx *ctx)
 			return DNR(NVME_SC_INVALID_VECTOR);
 	}
 
-	ret = nvme_mdev_vcq_init(ctx->vctrl, cqid,
-				 le64_to_cpu(in->prp1),
-				 cq_flags & NVME_QUEUE_PHYS_CONTIG,
-				 qsize + 1, irq);
+	ret = __nvme_mdev_vcq_init(ctx->vctrl, cqid, le64_to_cpu(in->prp1),
+				   cq_flags & NVME_QUEUE_PHYS_CONTIG, qsize + 1,
+				   irq);
 
 	return nvme_mdev_translate_error(ret);
 }
 
 /* Response to DELETE CQ command */
-static int nvme_mdev_adm_handle_delete_cq(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_delete_cq(struct adm_ctx *ctx)
 {
 	struct nvme_mdev_vctrl *vctrl = ctx->vctrl;
-	const struct nvme_delete_queue *in =  &ctx->in->delete_queue;
+	const struct nvme_delete_queue *in = &ctx->in->delete_queue;
+	struct nvme_vcq *vcq;
 	u16 qid = le16_to_cpu(in->qid), sqid;
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
 				   RSRV_NSID | RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW11_15))
+					   RSRV_MPTR | RSRV_DW11_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
-	if (!qid || qid >= MAX_VIRTUAL_QUEUES || !test_bit(qid, vctrl->vcq_en))
+	if (!qid || qid >= MAX_VIRTUAL_QUEUES)
+		return DNR(NVME_SC_QID_INVALID);
+
+	vcq = rcu_dereference_protected(vctrl->vcqs[qid],
+					lockdep_is_held(&vctrl->lock));
+	if (!vcq)
 		return DNR(NVME_SC_QID_INVALID);
 
-	for_each_set_bit(sqid, vctrl->vsq_en, MAX_VIRTUAL_QUEUES)
-		if (vctrl->vsqs[sqid].vcq == &vctrl->vcqs[qid])
+	for (sqid = 0; sqid < MAX_VIRTUAL_QUEUES; sqid++) {
+		struct nvme_vsq *vsq = rcu_dereference_protected(
+			vctrl->vsqs[sqid], lockdep_is_held(&vctrl->lock));
+		if (vsq && vsq->vcq == vcq)
 			return DNR(NVME_SC_INVALID_QUEUE);
+	}
 
-	nvme_mdev_vcq_delete(vctrl, qid);
+	__nvme_mdev_vcq_delete(vctrl, qid);
 	return NVME_SC_SUCCESS;
 }
 
 /* Response to CREATE SQ command */
-static int nvme_mdev_adm_handle_create_sq(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_create_sq(struct adm_ctx *ctx)
 {
 	const struct nvme_create_sq *in = &ctx->in->create_sq;
 	struct nvme_mdev_vctrl *vctrl = ctx->vctrl;
@@ -520,17 +539,17 @@ static int nvme_mdev_adm_handle_create_sq(struct adm_ctx *ctx)
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
 				   RSRV_NSID | RSRV_DW23 | RSRV_DPTR_PRP2 |
-				   RSRV_MPTR | RSRV_DW12_15))
+					   RSRV_MPTR | RSRV_DW12_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
-	if (!sqid ||
-	    sqid >= MAX_VIRTUAL_QUEUES || test_bit(sqid, vctrl->vsq_en))
+	if (!sqid || sqid >= MAX_VIRTUAL_QUEUES ||
+	    rcu_access_pointer(vctrl->vsqs[sqid]))
 		return DNR(NVME_SC_QID_INVALID);
 
-	if (!cqid || cqid  >= MAX_VIRTUAL_QUEUES)
+	if (!cqid || cqid >= MAX_VIRTUAL_QUEUES)
 		return DNR(NVME_SC_QID_INVALID);
 
-	if (!test_bit(cqid, vctrl->vcq_en))
+	if (!rcu_access_pointer(vctrl->vcqs[cqid]))
 		return DNR(NVME_SC_CQ_INVALID);
 
 	/* Queue size checks */
@@ -541,10 +560,9 @@ static int nvme_mdev_adm_handle_create_sq(struct adm_ctx *ctx)
 	if (sq_flags & ~(NVME_QUEUE_PHYS_CONTIG | NVME_SQ_PRIO_MASK))
 		return DNR(NVME_SC_INVALID_FIELD);
 
-	ret = nvme_mdev_vsq_init(ctx->vctrl, sqid,
-				 le64_to_cpu(in->prp1),
-				 sq_flags & NVME_QUEUE_PHYS_CONTIG,
-				 qsize + 1, cqid);
+	ret = __nvme_mdev_vsq_init(ctx->vctrl, sqid, le64_to_cpu(in->prp1),
+				   sq_flags & NVME_QUEUE_PHYS_CONTIG, qsize + 1,
+				   cqid);
 	if (ret)
 		goto error;
 
@@ -554,26 +572,27 @@ static int nvme_mdev_adm_handle_create_sq(struct adm_ctx *ctx)
 }
 
 /* Response to DELETE SQ command */
-static int nvme_mdev_adm_handle_delete_sq(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_delete_sq(struct adm_ctx *ctx)
 {
 	struct nvme_mdev_vctrl *vctrl = ctx->vctrl;
-	const struct nvme_delete_queue *in =  &ctx->in->delete_queue;
+	const struct nvme_delete_queue *in = &ctx->in->delete_queue;
 	u16 qid = le16_to_cpu(in->qid);
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
 				   RSRV_NSID | RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW11_15))
+					   RSRV_MPTR | RSRV_DW11_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
-	if (!qid || qid >= MAX_VIRTUAL_QUEUES || !test_bit(qid, vctrl->vsq_en))
+	if (!qid || qid >= MAX_VIRTUAL_QUEUES ||
+	    !rcu_access_pointer(vctrl->vsqs[qid]))
 		return DNR(NVME_SC_QID_INVALID);
 
-	nvme_mdev_vsq_delete(ctx->vctrl, qid);
+	__nvme_mdev_vsq_delete(ctx->vctrl, qid);
 	return NVME_SC_SUCCESS;
 }
 
 /* Set the shadow doorbell */
-static int nvme_mdev_adm_handle_dbbuf(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_dbbuf(struct adm_ctx *ctx)
 {
 	const struct nvme_dbbuf *in = &ctx->in->dbbuf;
 	int ret;
@@ -590,8 +609,8 @@ static int nvme_mdev_adm_handle_dbbuf(struct adm_ctx *ctx)
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
-				   RSRV_NSID | RSRV_DW23 |
-				   RSRV_MPTR | RSRV_DW10_15))
+				   RSRV_NSID | RSRV_DW23 | RSRV_MPTR |
+					   RSRV_DW10_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	/* check input buffers */
@@ -599,12 +618,13 @@ static int nvme_mdev_adm_handle_dbbuf(struct adm_ctx *ctx)
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	/* switch to the new doorbell buffer */
-	ret = nvme_mdev_mmio_enable_dbs_shadow(ctx->vctrl, sdb_iova, eidx_iova);
+	ret = __nvme_mdev_mmio_enable_dbs_shadow(ctx->vctrl, sdb_iova,
+						 eidx_iova);
 	return nvme_mdev_translate_error(ret);
 }
 
 /* Response to GET_FEATURES command */
-static int nvme_mdev_adm_handle_get_features(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_get_features(struct adm_ctx *ctx)
 {
 	u32 value = 0;
 	u32 irq;
@@ -619,8 +639,8 @@ static int nvme_mdev_adm_handle_get_features(struct adm_ctx *ctx)
 
 	/* common reserved bits*/
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
-				   RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW12_15))
+				   RSRV_DW23 | RSRV_DPTR | RSRV_MPTR |
+					   RSRV_DW12_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	/* reserved bits in dword10*/
@@ -689,12 +709,12 @@ static int nvme_mdev_adm_handle_get_features(struct adm_ctx *ctx)
 		return DNR(NVME_SC_INVALID_FIELD);
 	}
 out:
-	nvme_mdev_vsq_cmd_done_adm(ctx->vctrl, value, cid, NVME_SC_SUCCESS);
+	__nvme_mdev_vsq_cmd_done_adm(ctx->vctrl, value, cid, NVME_SC_SUCCESS);
 	return -1;
 }
 
 /* Response to SET_FEATURES command */
-static int nvme_mdev_adm_handle_set_features(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_set_features(struct adm_ctx *ctx)
 {
 	const struct nvme_features *in = &ctx->in->features;
 	struct nvme_mdev_vctrl *vctrl = ctx->vctrl;
@@ -710,8 +730,8 @@ static int nvme_mdev_adm_handle_set_features(struct adm_ctx *ctx)
 		return DNR(NVME_SC_FEATURE_NOT_PER_NS);
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
-				   RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW12_15))
+				   RSRV_DW23 | RSRV_DPTR | RSRV_MPTR |
+					   RSRV_DW12_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	switch (fid) {
@@ -720,12 +740,13 @@ static int nvme_mdev_adm_handle_set_features(struct adm_ctx *ctx)
 		value = (MAX_VIRTUAL_QUEUES - 1) |
 			((MAX_VIRTUAL_QUEUES - 1) << 16);
 
-		nvme_mdev_vsq_cmd_done_adm(ctx->vctrl, value,
-					   cid, NVME_SC_SUCCESS);
+		__nvme_mdev_vsq_cmd_done_adm(ctx->vctrl, value, cid,
+					     NVME_SC_SUCCESS);
 		return -1;
 
 	case NVME_FEAT_ARBITRATION:
 		vctrl->arb_burst_shift = value & 0x7;
+		vctrl->arb_burst = 1 << vctrl->arb_burst_shift;
 		return NVME_SC_SUCCESS;
 
 	case NVME_FEAT_IRQ_COALESCE:
@@ -744,11 +765,11 @@ static int nvme_mdev_adm_handle_set_features(struct adm_ctx *ctx)
 	}
 	case NVME_FEAT_VOLATILE_WC:
 		return (value != 0x1) ? DNR(NVME_SC_FEATURE_NOT_CHANGEABLE) :
-							NVME_SC_SUCCESS;
+					      NVME_SC_SUCCESS;
 
 	case NVME_FEAT_ERR_RECOVERY:
 		return (value != 0) ? DNR(NVME_SC_FEATURE_NOT_CHANGEABLE) :
-							NVME_SC_SUCCESS;
+					    NVME_SC_SUCCESS;
 	case NVME_FEAT_POWER_MGMT:
 		if (value & 0xFFFFFF0F)
 			return DNR(NVME_SC_INVALID_FIELD);
@@ -759,7 +780,7 @@ static int nvme_mdev_adm_handle_set_features(struct adm_ctx *ctx)
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	case NVME_FEAT_ASYNC_EVENT:
-		nvme_mdev_event_set_aen_config(vctrl, value);
+		__nvme_mdev_event_set_aen_config(vctrl, value);
 		return NVME_SC_SUCCESS;
 	default:
 		return DNR(NVME_SC_INVALID_FIELD);
@@ -767,31 +788,31 @@ static int nvme_mdev_adm_handle_set_features(struct adm_ctx *ctx)
 }
 
 /* Response to AER command */
-static int nvme_mdev_adm_handle_async_event(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_async_event(struct adm_ctx *ctx)
 {
 	u16 cid = le16_to_cpu(ctx->in->common.command_id);
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
 				   RSRV_NSID | RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW10_15))
+					   RSRV_MPTR | RSRV_DW10_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
-	return nvme_mdev_event_request_receive(ctx->vctrl, cid);
+	return __nvme_mdev_event_request_receive(ctx->vctrl, cid);
 }
 
 /* (Dummy) response to ABORT command*/
-static int nvme_mdev_adm_handle_abort(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_abort(struct adm_ctx *ctx)
 {
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
 				   RSRV_NSID | RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW10_15))
+					   RSRV_MPTR | RSRV_DW10_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	return DNR(NVME_SC_ABORT_MISSING);
 }
 
 /* Process one new command in the admin queue*/
-static int nvme_mdev_adm_handle_cmd(struct adm_ctx *ctx)
+static int __nvme_mdev_adm_handle_cmd(struct adm_ctx *ctx)
 {
 	u8 optcode = ctx->in->common.opcode;
 
@@ -803,34 +824,34 @@ static int nvme_mdev_adm_handle_cmd(struct adm_ctx *ctx)
 
 	switch (optcode) {
 	case nvme_admin_identify:
-		return nvme_mdev_adm_handle_id(ctx);
+		return __nvme_mdev_adm_handle_id(ctx);
 	case nvme_admin_create_cq:
 		_DBG(ctx->vctrl, "ADMINQ: CREATE_CQ\n");
-		return nvme_mdev_adm_handle_create_cq(ctx);
+		return __nvme_mdev_adm_handle_create_cq(ctx);
 	case nvme_admin_create_sq:
 		_DBG(ctx->vctrl, "ADMINQ: CREATE_SQ\n");
-		return nvme_mdev_adm_handle_create_sq(ctx);
+		return __nvme_mdev_adm_handle_create_sq(ctx);
 	case nvme_admin_delete_sq:
 		_DBG(ctx->vctrl, "ADMINQ: DELETE_SQ\n");
-		return nvme_mdev_adm_handle_delete_sq(ctx);
+		return __nvme_mdev_adm_handle_delete_sq(ctx);
 	case nvme_admin_delete_cq:
 		_DBG(ctx->vctrl, "ADMINQ: DELETE_CQ\n");
-		return nvme_mdev_adm_handle_delete_cq(ctx);
+		return __nvme_mdev_adm_handle_delete_cq(ctx);
 	case nvme_admin_dbbuf:
 		_DBG(ctx->vctrl, "ADMINQ: DBBUF_CONFIG\n");
-		return nvme_mdev_adm_handle_dbbuf(ctx);
+		return __nvme_mdev_adm_handle_dbbuf(ctx);
 	case nvme_admin_get_log_page:
-		return nvme_mdev_adm_handle_get_log_page(ctx);
+		return __nvme_mdev_adm_handle_get_log_page(ctx);
 	case nvme_admin_get_features:
-		return nvme_mdev_adm_handle_get_features(ctx);
+		return __nvme_mdev_adm_handle_get_features(ctx);
 	case nvme_admin_set_features:
-		return nvme_mdev_adm_handle_set_features(ctx);
+		return __nvme_mdev_adm_handle_set_features(ctx);
 	case nvme_admin_async_event:
 		_DBG(ctx->vctrl, "ADMINQ: ASYNC_EVENT_REQ\n");
-		return nvme_mdev_adm_handle_async_event(ctx);
+		return __nvme_mdev_adm_handle_async_event(ctx);
 	case nvme_admin_abort_cmd:
 		_DBG(ctx->vctrl, "ADMINQ: ABORT\n");
-		return nvme_mdev_adm_handle_abort(ctx);
+		return __nvme_mdev_adm_handle_abort(ctx);
 	default:
 		_DBG(ctx->vctrl, "ADMINQ: optcode 0x%04x\n", optcode);
 		return DNR(NVME_SC_INVALID_OPCODE);
@@ -838,7 +859,7 @@ static int nvme_mdev_adm_handle_cmd(struct adm_ctx *ctx)
 }
 
 /* Process all pending admin commands */
-void nvme_mdev_adm_process_sq(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_adm_process_sq(struct nvme_mdev_vctrl *vctrl)
 {
 	struct adm_ctx ctx;
 
@@ -848,18 +869,21 @@ void nvme_mdev_adm_process_sq(struct nvme_mdev_vctrl *vctrl)
 	ctx.hctrl = vctrl->hctrl;
 	nvme_mdev_udata_iter_setup(&vctrl->viommu, &ctx.udatait);
 
-	nvme_mdev_io_pause(ctx.vctrl);
+	__nvme_mdev_io_pause(ctx.vctrl);
 
 	while (!(nvme_mdev_vctrl_is_dead(vctrl))) {
 		int ret;
 		u16 cid;
+		struct nvme_vsq *adm_sq = rcu_dereference_protected(
+			vctrl->vsqs[0], lockdep_is_held(&vctrl->lock));
+		BUG_ON(!adm_sq);
 
-		ctx.in = nvme_mdev_vsq_get_cmd(vctrl, &vctrl->vsqs[0]);
+		ctx.in = nvme_mdev_vsq_get_cmd(vctrl, adm_sq);
 		if (!ctx.in)
 			break;
 
 		cid = le16_to_cpu(ctx.in->common.command_id);
-		ret = nvme_mdev_adm_handle_cmd(&ctx);
+		ret = __nvme_mdev_adm_handle_cmd(&ctx);
 
 		if (ret == -1)
 			continue;
@@ -867,7 +891,7 @@ void nvme_mdev_adm_process_sq(struct nvme_mdev_vctrl *vctrl)
 		if (ret != 0)
 			_DBG(vctrl, "ADMINQ: CID 0x%x FAILED: status 0x%x\n",
 			     cid, ret);
-		nvme_mdev_vsq_cmd_done_adm(vctrl, 0, cid, ret);
+		__nvme_mdev_vsq_cmd_done_adm(vctrl, 0, cid, ret);
 	}
-	nvme_mdev_io_resume(ctx.vctrl);
+	__nvme_mdev_io_resume(ctx.vctrl);
 }
diff --git a/drivers/nvme/mdev/bpf.c b/drivers/nvme/mdev/bpf.c
new file mode 100644
index 000000000000..dd936f729a84
--- /dev/null
+++ b/drivers/nvme/mdev/bpf.c
@@ -0,0 +1,67 @@
+#include <linux/bpf.h>
+#include <linux/filter.h>
+#include <linux/bpf_nvme_mdev.h>
+#include "priv.h"
+
+static bool is_valid_access(int off, int size, enum bpf_access_type type,
+			    const struct bpf_prog *prog,
+			    struct bpf_insn_access_aux *info)
+{
+	if (off < 0 || off >= sizeof(struct bpf_io_ctx) || off % size)
+		return false;
+
+	switch (off) {
+	case offsetof(struct bpf_io_ctx, sqid):
+	case offsetof(struct bpf_io_ctx, current_hook):
+	case offsetof(struct bpf_io_ctx, iostate):
+	case offsetof(struct bpf_io_ctx, data):
+		return type == BPF_READ && size == sizeof(__u32);
+	case bpf_ctx_range(struct bpf_io_ctx, cmd):
+		return off + size <= offsetofend(struct bpf_io_ctx, cmd);
+	}
+
+	return false;
+}
+
+/*
+static u32 convert_ctx_access(enum bpf_access_type type,
+			      const struct bpf_insn *si,
+			      struct bpf_insn *insn_buf, struct bpf_prog *prog,
+			      u32 *target_size)
+{
+	struct bpf_insn *insn = insn_buf;
+	s16 coff;
+
+	switch (si->off) {
+	case offsetof(struct bpf_io_ctx, sqid):
+		coff = bpf_target_off(struct bpf_io_ctx, sqid, 4, target_size);
+		*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg, coff);
+		break;
+	case offsetof(struct bpf_io_ctx, current_hook):
+		coff = bpf_target_off(struct bpf_io_ctx, current_hook, 4,
+				      target_size);
+		*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg, coff);
+		break;
+	case bpf_ctx_range(struct bpf_io_ctx, cmd):
+		coff = si->off - offsetof(struct bpf_io_ctx, cmd);
+		*insn++ = BPF_LDX_MEM(BPF_SIZE(si->code), si->dst_reg,
+				      si->src_reg,
+				      offsetof(struct bpf_io_ctx, cmd) + coff);
+		break;
+	}
+
+	return insn - insn_buf;
+}
+*/
+
+static const struct bpf_func_proto *get_func_proto(enum bpf_func_id func_id,
+						   const struct bpf_prog *prog)
+{
+	return bpf_base_func_proto(func_id);
+}
+
+const struct bpf_prog_ops nvme_mdev_prog_ops = {};
+const struct bpf_verifier_ops nvme_mdev_verifier_ops = {
+	.is_valid_access = is_valid_access,
+	.get_func_proto = get_func_proto,
+};
diff --git a/drivers/nvme/mdev/bpf_sm.c b/drivers/nvme/mdev/bpf_sm.c
new file mode 100644
index 000000000000..a2a2312f426a
--- /dev/null
+++ b/drivers/nvme/mdev/bpf_sm.c
@@ -0,0 +1,127 @@
+#include <linux/bpf_nvme_mdev.h>
+#include "priv.h"
+#include "bpf_sm.h"
+#include "notifyfd.h"
+
+bool nvme_mdev_bpfsm_get(struct nvme_vsq *vsq, u16 cid, int *ios)
+{
+	atomic_t *pios = &vsq->ctx_data[cid].iostate;
+
+	*ios = atomic_read(pios);
+	return !!(*ios & NMBPF_COMPLETION_MASK);
+}
+
+bool nvme_mdev_bpfsm_runhook(struct bpf_io_ctx *ctx, struct nvme_vsq *vsq,
+			     u16 ucid, int current_hook, u32 data)
+{
+	struct bpf_prog *prog;
+	u32 bpf_ret;
+
+	/* skip cmd copy for vsq hook since that's already done in process_sq */
+	if (current_hook != NMBPF_HOOK_VSQ)
+		ctx->cmd = vsq->ctx_data[ucid].cmd;
+	ctx->sqid = vsq->qid;
+	ctx->current_hook = current_hook;
+	ctx->data = data;
+
+	if (current_hook == NMBPF_HOOK_VSQ)
+		ctx->iostate = 0;
+	else if (!nvme_mdev_bpfsm_get(vsq, ucid, &ctx->iostate) ||
+		 !(ctx->iostate & current_hook))
+		return false;
+
+#ifdef CONFIG_NMBPF_DEBUG
+	_INFO(vsq->vctrl, "IOQ: QID %d CID %d pre hook %#x %#x = %#x\n",
+	      vsq->qid, ucid, current_hook, data, ctx->iostate);
+#endif
+
+	if (current_hook == NMBPF_HOOK_HCQ)
+		ctx->iostate &= ~NMBPF_SEND_HQ;
+	else if (current_hook == NMBPF_HOOK_NFD_WRITE)
+		ctx->iostate &= ~NMBPF_SEND_FD;
+
+	if (ctx->iostate & NMBPF_SEND_MASK) {
+		_INFO(vsq->vctrl,
+		      "IOQ: QID %d CID %d FAILED: pending send, ios=%#x\n",
+		      vsq->qid, ucid, ctx->iostate);
+		return false;
+	}
+
+	prog = rcu_dereference(vsq->vctrl->prog);
+	if (prog) {
+		bpf_ret = BPF_PROG_RUN(prog, ctx);
+		ctx->iostate = nvme_mdev_bpfsm_set(vsq, ucid, bpf_ret);
+#ifdef CONFIG_NMBPF_DEBUG
+		_INFO(vsq->vctrl,
+		      "IOQ: QID %d CID %d post hook %#x %#x = %#x\n", vsq->qid,
+		      ucid, current_hook, data, ctx->iostate);
+#endif
+		return true;
+	}
+	return false;
+}
+
+int nvme_mdev_bpfsm_set(struct nvme_vsq *vsq, u16 cid, int bpf_ret)
+{
+	int cur, tmp, next;
+	atomic_t *pios = &vsq->ctx_data[cid].iostate;
+
+	do {
+		cur = atomic_read(pios);
+		if (cur & NMBPF_COMPLETION_MASK) {
+			next = (cur & NMBPF_STATUS_MASK) |
+			       (bpf_ret & (NMBPF_HOOK_MASK | NMBPF_SEND_MASK |
+					   NMBPF_COMPLETION_MASK));
+		} else
+			next = bpf_ret;
+
+		tmp = atomic_cmpxchg(pios, cur, next);
+	} while (cur != tmp);
+#ifdef CONFIG_NMBPF_DEBUG
+	_INFO(vsq->vctrl, "IOQ: QID %d CID %d set %#x -> %#x\n", vsq->qid, cid,
+	      cur, next);
+#endif
+	return next;
+}
+
+int nvme_mdev_bpfsm_update(struct nvme_vsq *vsq, u16 cid, u16 status,
+			   int cmpl_type)
+{
+	int cur, tmp, next;
+	int send_type = WILL_COMPLETE_TO_SEND(cmpl_type);
+	atomic_t *pios = &vsq->ctx_data[cid].iostate;
+	do {
+		cur = atomic_read(pios);
+		if (unlikely(!(cur & NMBPF_COMPLETION_MASK))) {
+			_INFO(vsq->vctrl,
+			      "IOC: QID %d CID %d FAILED: invalid iostate, expected %#x\n",
+			      vsq->qid, cid, send_type | cmpl_type | status);
+			return 0;
+		}
+
+		next = cur;
+		if (!(next & send_type)) {
+			_INFO(vsq->vctrl,
+			      "IOC: QID %d CID %d FAILED: unknown iostate type %#x, expected %#x\n",
+			      vsq->qid, cid, next,
+			      send_type | cmpl_type | status);
+			return next;
+		}
+
+		next &= ~send_type;
+		if ((next & NMBPF_COMPLETION_MASK) == cmpl_type)
+			next = (next & ~NMBPF_STATUS_MASK) | status;
+		if (!(next & NMBPF_SEND_MASK))
+			/* all completion sources have finished,
+			 * we can now erase
+			 */
+			next = (next & ~NMBPF_COMPLETION_MASK) | NMBPF_COMPLETE;
+
+		tmp = atomic_cmpxchg(pios, cur, next);
+	} while (cur != tmp);
+#ifdef CONFIG_NMBPF_DEBUG
+	_INFO(vsq->vctrl, "IOQ: QID %d CID %d update %#x -> %#x\n", vsq->qid,
+	      cid, cur, next);
+#endif
+	return next;
+}
diff --git a/drivers/nvme/mdev/bpf_sm.h b/drivers/nvme/mdev/bpf_sm.h
new file mode 100644
index 000000000000..59f1da5cfdfc
--- /dev/null
+++ b/drivers/nvme/mdev/bpf_sm.h
@@ -0,0 +1,19 @@
+#ifndef _NVME_MDEV_BPF_SM_H
+#define _NVME_MDEV_BPF_SM_H
+
+#include <linux/types.h>
+
+struct io_ctx;
+struct nvme_vsq;
+
+bool nvme_mdev_bpfsm_get(struct nvme_vsq *vsq, u16 cid, int *ios);
+
+bool nvme_mdev_bpfsm_runhook(struct bpf_io_ctx *ctx, struct nvme_vsq *vsq,
+			     u16 ucid, int current_hook, u32 data);
+
+int nvme_mdev_bpfsm_set(struct nvme_vsq *vsq, u16 cid, int bpf_ret);
+
+int nvme_mdev_bpfsm_update(struct nvme_vsq *vsq, u16 cid, u16 status,
+			   int cmpl_type);
+
+#endif
diff --git a/drivers/nvme/mdev/events.c b/drivers/nvme/mdev/events.c
index 9854c1cabdcb..5d424acd9cb1 100644
--- a/drivers/nvme/mdev/events.c
+++ b/drivers/nvme/mdev/events.c
@@ -12,8 +12,15 @@ static void nvme_mdev_event_complete(struct nvme_mdev_vctrl *vctrl)
 {
 	u16 lid, cid;
 	u32 dw0;
+	struct nvme_vsq *adm_sq;
 
-	for_each_set_bit(lid, vctrl->events.events_pending, MAX_LOG_PAGES) {
+	rcu_read_lock();
+	adm_sq = rcu_dereference(vctrl->vsqs[0]);
+	if (!adm_sq) {
+		rcu_read_unlock();
+		return;
+	}
+	for_each_set_bit (lid, vctrl->events.events_pending, MAX_LOG_PAGES) {
 		/* we have pending aer requests, but no requests*/
 		if (vctrl->events.aer_cid_count == 0)
 			break;
@@ -26,18 +33,22 @@ static void nvme_mdev_event_complete(struct nvme_mdev_vctrl *vctrl)
 		clear_bit(lid, vctrl->events.events_pending);
 
 		_DBG(vctrl,
-		     "AEN: replying to AER (CID=%d) with status 0x%08x\n",
-		     cid, dw0);
+		     "AEN: replying to AER (CID=%d) with status 0x%08x\n", cid,
+		     dw0);
 
-		nvme_mdev_vsq_cmd_done_adm(vctrl, dw0, cid, NVME_SC_SUCCESS);
+		nvme_mdev_vcq_write_adm(vctrl, adm_sq->vcq, dw0, adm_sq->head,
+					cid, NVME_SC_SUCCESS);
 	}
+	rcu_read_unlock();
 }
 
 /* deal with received async event request from the user*/
-int nvme_mdev_event_request_receive(struct nvme_mdev_vctrl *vctrl,
-				    u16 cid)
+int __nvme_mdev_event_request_receive(struct nvme_mdev_vctrl *vctrl, u16 cid)
 {
 	int cnt = vctrl->events.aer_cid_count;
+	struct nvme_vcq *adm_cq = rcu_dereference_protected(
+		vctrl->vcqs[0], lockdep_is_held(&vctrl->lock));
+	BUG_ON(!adm_cq);
 
 	if (cnt >= MAX_AER_COMMANDS)
 		return DNR(NVME_SC_ASYNC_LIMIT);
@@ -45,7 +56,7 @@ int nvme_mdev_event_request_receive(struct nvme_mdev_vctrl *vctrl,
 	/* don't allow AER to be pending if there is no space left in the
 	 * completion queue permanently
 	 */
-	if ((cnt + 1) >= vctrl->vcqs[0].size - 1)
+	if ((cnt + 1) >= adm_cq->size - 1)
 		return DNR(NVME_SC_ASYNC_LIMIT);
 
 	vctrl->events.aer_cids[cnt++] = cid;
@@ -100,7 +111,7 @@ u32 nvme_mdev_event_read_aen_config(struct nvme_mdev_vctrl *vctrl)
 	return value;
 }
 
-void nvme_mdev_event_set_aen_config(struct nvme_mdev_vctrl *vctrl, u32 value)
+void __nvme_mdev_event_set_aen_config(struct nvme_mdev_vctrl *vctrl, u32 value)
 {
 	_DBG(vctrl, "AEN: set config: 0x%04x\n", value);
 
@@ -113,7 +124,7 @@ void nvme_mdev_event_set_aen_config(struct nvme_mdev_vctrl *vctrl, u32 value)
 }
 
 /* called when user acks an log page which causes an AER event to be unmasked*/
-void nvme_mdev_event_process_ack(struct nvme_mdev_vctrl *vctrl, u8 log_page)
+void __nvme_mdev_event_process_ack(struct nvme_mdev_vctrl *vctrl, u8 log_page)
 {
 	lockdep_assert_held(&vctrl->lock);
 
@@ -139,4 +150,3 @@ void nvme_mdev_events_reset(struct nvme_mdev_vctrl *vctrl)
 {
 	memset(&vctrl->events, 0, sizeof(vctrl->events));
 }
-
diff --git a/drivers/nvme/mdev/host.c b/drivers/nvme/mdev/host.c
index 306571997349..2e1b8dfa80a8 100644
--- a/drivers/nvme/mdev/host.c
+++ b/drivers/nvme/mdev/host.c
@@ -10,9 +10,10 @@
 #include <linux/mdev.h>
 #include <linux/module.h>
 #include "priv.h"
+#include "iothreads.h"
 
-static LIST_HEAD(nvme_mdev_hctrl_list);
-static DEFINE_MUTEX(nvme_mdev_hctrl_list_mutex);
+LIST_HEAD(nvme_mdev_hctrl_list);
+DEFINE_MUTEX(nvme_mdev_hctrl_list_mutex);
 static struct nvme_mdev_inst_type **instance_types;
 
 unsigned int io_timeout_ms = 30000;
@@ -22,19 +23,33 @@ MODULE_PARM_DESC(io_timeout,
 
 unsigned int poll_timeout_ms = 500;
 module_param_named(poll_timeout, poll_timeout_ms, uint, 0644);
-MODULE_PARM_DESC(poll_timeout,
-		 "Maximum idle time to keep polling (in msec) (0 - poll forever)");
+MODULE_PARM_DESC(
+	poll_timeout,
+	"Maximum idle time to keep polling (in msec) (0 - poll forever)");
 
 unsigned int admin_poll_rate_ms = 100;
-module_param_named(admin_poll_rate, poll_timeout_ms, uint, 0644);
-MODULE_PARM_DESC(admin_poll_rate,
-		 "Admin queue polling rate (in msec) (used only when shadow doorbell is disabled)");
+module_param_named(admin_poll_rate, admin_poll_rate_ms, uint, 0644);
+MODULE_PARM_DESC(
+	admin_poll_rate,
+	"Admin queue polling rate (in msec) (used only when shadow doorbell is disabled)");
 
 bool use_shadow_doorbell = true;
 module_param(use_shadow_doorbell, bool, 0644);
 MODULE_PARM_DESC(use_shadow_doorbell,
 		 "Enable the shadow doorbell NVMe extension");
 
+#ifndef CONFIG_NMBPF_POLL_ONCE
+unsigned int vsq_poll_loops = 1;
+module_param(vsq_poll_loops, uint, 0644);
+MODULE_PARM_DESC(vsq_poll_loops, "Number of VSQ poll iterations");
+#endif
+
+#ifndef CONFIG_NVME_MDEV_VFIO_GENERIC_IO
+unsigned int oncs_mask = NVME_CTRL_ONCS_DSM | NVME_CTRL_ONCS_WRITE_ZEROES;
+module_param_named(oncs_mask, oncs_mask, uint, 0644);
+MODULE_PARM_DESC(oncs_mask, "ONCS mask to apply to vctrl");
+#endif
+
 /* Create a new host controller */
 static struct nvme_mdev_hctrl *nvme_mdev_hctrl_create(struct nvme_ctrl *ctrl)
 {
@@ -42,12 +57,6 @@ static struct nvme_mdev_hctrl *nvme_mdev_hctrl_create(struct nvme_ctrl *ctrl)
 	u32 max_lba_transfer;
 	unsigned int nr_host_queues;
 
-	/* TODOLATER: IO: support more page size configurations*/
-	if (ctrl->page_size != PAGE_SIZE) {
-		dev_info(ctrl->dev, "no support for mdev - page_size mismatch");
-		return NULL;
-	}
-
 	hctrl = kzalloc_node(sizeof(*hctrl), GFP_KERNEL,
 			     dev_to_node(ctrl->dev));
 	if (!hctrl)
@@ -58,14 +67,14 @@ static struct nvme_mdev_hctrl *nvme_mdev_hctrl_create(struct nvme_ctrl *ctrl)
 	max_lba_transfer = ctrl->max_hw_sectors >> (PAGE_SHIFT - 9);
 
 	if (nr_host_queues == 0) {
-		dev_info(ctrl->dev,
-			 "no support for mdev - no mdev reserved queues available");
+		dev_info(
+			ctrl->dev,
+			"no support for mdev - no mdev reserved queues available");
 		kfree(hctrl);
 		return NULL;
 	}
 
-	hctrl->oncs = ctrl->oncs &
-		(NVME_CTRL_ONCS_DSM | NVME_CTRL_ONCS_WRITE_ZEROES);
+	hctrl->oncs = ctrl->oncs & oncs_mask;
 #else
 	/* for now don't deal with bio chaining */
 	max_lba_transfer = BIO_MAX_PAGES;
@@ -80,7 +89,6 @@ static struct nvme_mdev_hctrl *nvme_mdev_hctrl_create(struct nvme_ctrl *ctrl)
 	hctrl->nvme_ctrl = ctrl;
 	nvme_get_ctrl(ctrl);
 
-
 	hctrl->id = ctrl->instance;
 	hctrl->node = dev_to_node(ctrl->dev);
 	hctrl->mdts = ilog2(__rounddown_pow_of_two(max_lba_transfer));
@@ -123,7 +131,7 @@ struct nvme_mdev_hctrl *nvme_mdev_hctrl_lookup_get(struct device *parent)
 	struct nvme_mdev_hctrl *hctrl = NULL, *tmp;
 
 	mutex_lock(&nvme_mdev_hctrl_list_mutex);
-	list_for_each_entry(tmp, &nvme_mdev_hctrl_list, link) {
+	list_for_each_entry (tmp, &nvme_mdev_hctrl_list, link) {
 		if (tmp->nvme_ctrl->dev == parent) {
 			hctrl = tmp;
 			kref_get(&hctrl->ref);
@@ -134,6 +142,22 @@ struct nvme_mdev_hctrl *nvme_mdev_hctrl_lookup_get(struct device *parent)
 	return hctrl;
 }
 
+struct nvme_mdev_hctrl *nvme_mdev_hctrl_lookup_get_byid(u32 ctrlid)
+{
+	struct nvme_mdev_hctrl *hctrl = NULL, *tmp;
+
+	mutex_lock(&nvme_mdev_hctrl_list_mutex);
+	list_for_each_entry (tmp, &nvme_mdev_hctrl_list, link) {
+		if (tmp->id == ctrlid) {
+			hctrl = tmp;
+			kref_get(&hctrl->ref);
+			break;
+		}
+	}
+	mutex_unlock(&nvme_mdev_hctrl_list_mutex);
+	return hctrl;
+}
+
 /* Release a held reference to a host controller*/
 void nvme_mdev_hctrl_put(struct nvme_mdev_hctrl *hctrl)
 {
@@ -160,14 +184,13 @@ int nvme_mdev_hctrl_hqs_available(struct nvme_mdev_hctrl *hctrl)
 	int ret;
 
 	mutex_lock(&hctrl->lock);
-	ret =  hctrl->nr_host_queues;
+	ret = hctrl->nr_host_queues;
 	mutex_unlock(&hctrl->lock);
 	return ret;
 }
 
 /* Reserve N host IO queues, for later allocation to a specific user*/
-bool nvme_mdev_hctrl_hqs_reserve(struct nvme_mdev_hctrl *hctrl,
-				 unsigned int n)
+bool nvme_mdev_hctrl_hqs_reserve(struct nvme_mdev_hctrl *hctrl, unsigned int n)
 {
 	mutex_lock(&hctrl->lock);
 
@@ -214,8 +237,8 @@ bool nvme_mdev_hctrl_hq_check_op(struct nvme_mdev_hctrl *hctrl, u8 optcode)
 int nvme_mdev_hctrl_hq_alloc(struct nvme_mdev_hctrl *hctrl)
 {
 	u16 qid = 0;
-	int ret = hctrl->nvme_ctrl->ops->ext_queue_alloc(hctrl->nvme_ctrl,
-			&qid);
+	int ret =
+		hctrl->nvme_ctrl->ops->ext_queue_alloc(hctrl->nvme_ctrl, &qid);
 
 	if (ret)
 		return ret;
@@ -234,12 +257,9 @@ bool nvme_mdev_hctrl_hq_can_submit(struct nvme_mdev_hctrl *hctrl, u16 qid)
 	return hctrl->nvme_ctrl->ops->ext_queue_full(hctrl->nvme_ctrl, qid);
 }
 
-
-
 /* Submit a IO passthrough command */
 int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
-			      struct nvme_mdev_vns *vns,
-			      u16 qid, u32 tag,
+			      struct nvme_mdev_vns *vns, u16 qid, u32 tag,
 			      struct nvme_command *cmd,
 			      struct nvme_ext_data_iter *datait)
 {
@@ -249,8 +269,7 @@ int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
 }
 
 /* Poll for completion of IO passthrough commands */
-int nvme_mdev_hctrl_hq_poll(struct nvme_mdev_hctrl *hctrl,
-			    u32 qid,
+int nvme_mdev_hctrl_hq_poll(struct nvme_mdev_hctrl *hctrl, u32 qid,
 			    struct nvme_ext_cmd_result *results,
 			    unsigned int max_len)
 {
@@ -267,7 +286,7 @@ int nvme_mdev_hctrl_hq_alloc(struct nvme_mdev_hctrl *hctrl)
 	int qid, ret;
 	struct hw_mbio_queue *hwq;
 
-	for (qid = 0 ; qid < MDEV_NVME_NUM_BIO_QUEUES ; qid++)
+	for (qid = 0; qid < MDEV_NVME_NUM_BIO_QUEUES; qid++)
 		if (!hctrl->hw_queues[qid])
 			break;
 
@@ -337,8 +356,7 @@ static void nvme_mdev_hctrl_bio_done(struct bio *bio)
 
 /* Submit a IO passthrough command */
 int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
-			      struct nvme_mdev_vns *vns,
-			      u16 qid, u32 tag,
+			      struct nvme_mdev_vns *vns, u16 qid, u32 tag,
 			      struct nvme_command *cmd,
 			      struct nvme_ext_data_iter *datait)
 {
@@ -357,8 +375,8 @@ int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
 
 	/* read/write buffer processing */
 	if (opcode == nvme_cmd_read || opcode == nvme_cmd_write) {
-		unsigned long datalength =
-			(le16_to_cpu(cmd->rw.length) + 1) << vns->blksize_shift;
+		unsigned long datalength = (le16_to_cpu(cmd->rw.length) + 1)
+					   << vns->blksize_shift;
 
 		if (opcode == nvme_cmd_read) {
 			op = REQ_OP_READ;
@@ -379,8 +397,8 @@ int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
 		mbio = container_of(bio, struct mbio, bio);
 
 		/* starting sector */
-		bio->bi_iter.bi_sector = le64_to_cpu(cmd->rw.slba) <<
-				(vns->blksize_shift - 9);
+		bio->bi_iter.bi_sector = le64_to_cpu(cmd->rw.slba)
+					 << (vns->blksize_shift - 9);
 
 		/* Data. Last page might be partial size*/
 		while (datait->count) {
@@ -392,8 +410,8 @@ int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
 			page = pfn_to_page(PHYS_PFN(datait->physical));
 			offset = OFFSET_IN_PAGE(datait->physical);
 
-			if (bio_add_page(&mbio->bio, page,
-					 chunk, offset) != chunk) {
+			if (bio_add_page(&mbio->bio, page, chunk, offset) !=
+			    chunk) {
 				WARN_ON(1);
 				retval = -ENOMEM;
 				goto error;
@@ -405,7 +423,7 @@ int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
 			datalength -= chunk;
 		}
 
-	/* flush request */
+		/* flush request */
 	} else if (opcode == nvme_cmd_flush) {
 		op = REQ_OP_WRITE;
 		op_flags = REQ_PREFLUSH;
@@ -414,7 +432,7 @@ int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
 			return -ENOMEM;
 		mbio = container_of(bio, struct mbio, bio);
 	} else {
-		retval =  -EINVAL;
+		retval = -EINVAL;
 		goto error;
 	}
 
@@ -445,8 +463,7 @@ int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
 }
 
 /* Poll for completion of IO passthrough commands */
-int nvme_mdev_hctrl_hq_poll(struct nvme_mdev_hctrl *hctrl,
-			    u32 qid,
+int nvme_mdev_hctrl_hq_poll(struct nvme_mdev_hctrl *hctrl, u32 qid,
 			    struct nvme_ext_cmd_result *results,
 			    unsigned int max_len)
 {
@@ -458,7 +475,7 @@ int nvme_mdev_hctrl_hq_poll(struct nvme_mdev_hctrl *hctrl,
 	if (!hwq->inflight)
 		return -1;
 
-	list_for_each_entry_safe(mbio, tmp, &hwq->bios_in_flight, link) {
+	list_for_each_entry_safe (mbio, tmp, &hwq->bios_in_flight, link) {
 		if (mbio->status == NVME_STATUS_PENDING)
 			blk_poll(mbio->blk_queue, mbio->cookie, false);
 
@@ -484,7 +501,7 @@ void nvme_mdev_hctrl_destroy_all(void)
 {
 	struct nvme_mdev_hctrl *hctrl = NULL, *tmp;
 
-	list_for_each_entry_safe(hctrl, tmp, &nvme_mdev_hctrl_list, link) {
+	list_for_each_entry_safe (hctrl, tmp, &nvme_mdev_hctrl_list, link) {
 		list_del(&hctrl->link);
 		hctrl->removing = true;
 		mdev_unregister_device(hctrl->nvme_ctrl->dev);
@@ -515,20 +532,21 @@ static ssize_t name_show(struct kobject *kobj, struct device *dev, char *buf)
 static MDEV_TYPE_ATTR_RO(name);
 
 /* This shows description of the instance type */
-static ssize_t description_show(struct kobject *kobj,
-				struct device *dev, char *buf)
+static ssize_t description_show(struct kobject *kobj, struct device *dev,
+				char *buf)
 {
 	struct nvme_mdev_inst_type *type = nvme_mdev_inst_type_get(kobj->name);
 
-	return sprintf(buf,
-		       "MDEV nvme device, using maximum %d hw submission queues\n",
-		       type->max_hw_queues);
+	return sprintf(
+		buf,
+		"MDEV nvme device, using maximum %d hw submission queues\n",
+		type->max_hw_queues);
 }
 static MDEV_TYPE_ATTR_RO(description);
 
 /* This shows the device API of the instance type */
-static ssize_t device_api_show(struct kobject *kobj,
-			       struct device *dev, char *buf)
+static ssize_t device_api_show(struct kobject *kobj, struct device *dev,
+			       char *buf)
 {
 	return sprintf(buf, "%s\n", VFIO_DEVICE_API_PCI_STRING);
 }
@@ -587,14 +605,16 @@ static int nvme_mdev_instance_types_init(struct mdev_parent_ops *ops)
 	struct nvme_mdev_inst_type *type;
 	struct attribute_group *attrgroup;
 
-	ops->supported_type_groups = kzalloc(sizeof(struct attribute_group *)
-			* (MAX_HOST_QUEUES + 1), GFP_KERNEL);
+	ops->supported_type_groups = kzalloc(sizeof(struct attribute_group *) *
+						     (MAX_HOST_QUEUES + 1),
+					     GFP_KERNEL);
 
 	if (!ops->supported_type_groups)
 		return -ENOMEM;
 
-	instance_types = kzalloc(sizeof(struct nvme_mdev_inst_type *)
-			* MAX_HOST_QUEUES + 1, GFP_KERNEL);
+	instance_types = kzalloc(
+		sizeof(struct nvme_mdev_inst_type *) * MAX_HOST_QUEUES + 1,
+		GFP_KERNEL);
 
 	if (!instance_types) {
 		kfree(ops->supported_type_groups);
@@ -643,10 +663,10 @@ static void nvme_mdev_nvme_ctrl_state_changed(struct nvme_ctrl *ctrl)
 		if (!hctrl) {
 			hctrl = nvme_mdev_hctrl_create(ctrl);
 			return;
-		/* a controller is live again after reset/reconnect/suspend*/
+			/* a controller is live again after reset/reconnect/suspend*/
 		} else {
 			mutex_lock(&nvme_mdev_vctrl_list_mutex);
-			list_for_each_entry(vctrl, &nvme_mdev_vctrl_list, link)
+			list_for_each_entry (vctrl, &nvme_mdev_vctrl_list, link)
 				if (vctrl->hctrl == hctrl)
 					nvme_mdev_vctrl_resume(vctrl);
 			mutex_unlock(&nvme_mdev_vctrl_list_mutex);
@@ -661,13 +681,14 @@ static void nvme_mdev_nvme_ctrl_state_changed(struct nvme_ctrl *ctrl)
 			return;
 
 		mutex_lock(&nvme_mdev_vctrl_list_mutex);
-		list_for_each_entry(vctrl, &nvme_mdev_vctrl_list, link)
+		list_for_each_entry (vctrl, &nvme_mdev_vctrl_list, link)
 			if (vctrl->hctrl == hctrl)
 				nvme_mdev_vctrl_pause(vctrl);
 		mutex_unlock(&nvme_mdev_vctrl_list_mutex);
 		break;
 
 	case NVME_CTRL_DELETING:
+	case NVME_CTRL_DELETING_NOIO:
 	case NVME_CTRL_DEAD:
 		/* host nvme controller is dead, remove it*/
 		if (!hctrl)
@@ -681,8 +702,8 @@ static void nvme_mdev_nvme_ctrl_state_changed(struct nvme_ctrl *ctrl)
 }
 
 /* A host namespace might have its properties changed/removed.*/
-static void nvme_mdev_nvme_ctrl_ns_updated(struct nvme_ctrl *ctrl,
-					   u32 nsid, bool removed)
+static void nvme_mdev_nvme_ctrl_ns_updated(struct nvme_ctrl *ctrl, u32 nsid,
+					   bool removed)
 {
 	struct nvme_mdev_vctrl *vctrl;
 	struct nvme_mdev_hctrl *hctrl = nvme_mdev_hctrl_lookup_get(ctrl->dev);
@@ -691,7 +712,7 @@ static void nvme_mdev_nvme_ctrl_ns_updated(struct nvme_ctrl *ctrl,
 		return;
 
 	mutex_lock(&nvme_mdev_vctrl_list_mutex);
-	list_for_each_entry(vctrl, &nvme_mdev_vctrl_list, link)
+	list_for_each_entry (vctrl, &nvme_mdev_vctrl_list, link)
 		if (vctrl->hctrl == hctrl)
 			nvme_mdev_vns_host_ns_update(vctrl, nsid, removed);
 	mutex_unlock(&nvme_mdev_vctrl_list_mutex);
@@ -710,10 +731,12 @@ static int __init nvme_mdev_init(void)
 
 	nvme_mdev_instance_types_init(&mdev_fops);
 	ret = nvme_core_register_mdev_driver(&nvme_mdev_driver);
-	if (ret) {
-		nvme_mdev_instance_types_fini(&mdev_fops);
-		return ret;
-	}
+	if (ret)
+		goto err_mdev;
+
+	ret = nvme_mdev_io_init();
+	if (ret)
+		goto err_iothreads;
 
 	pr_info("nvme_mdev " NVME_MDEV_FIRMWARE_VERSION " loaded\n");
 
@@ -721,10 +744,18 @@ static int __init nvme_mdev_init(void)
 	pr_info("nvme_mdev: using block layer polled IO\b");
 #endif
 	return 0;
+
+err_iothreads:
+	nvme_core_unregister_mdev_driver(&nvme_mdev_driver);
+
+err_mdev:
+	nvme_mdev_instance_types_fini(&mdev_fops);
+	return ret;
 }
 
 static void __exit nvme_mdev_exit(void)
 {
+	nvme_mdev_io_exit();
 	nvme_core_unregister_mdev_driver(&nvme_mdev_driver);
 	nvme_mdev_hctrl_destroy_all();
 	nvme_mdev_instance_types_fini(&mdev_fops);
@@ -735,6 +766,4 @@ MODULE_AUTHOR("Maxim Levitsky <mlevitsk@redhat.com>");
 MODULE_LICENSE("GPL");
 MODULE_VERSION(NVME_MDEV_FIRMWARE_VERSION);
 
-module_init(nvme_mdev_init)
-module_exit(nvme_mdev_exit)
-
+module_init(nvme_mdev_init) module_exit(nvme_mdev_exit)
diff --git a/drivers/nvme/mdev/instance.c b/drivers/nvme/mdev/instance.c
index d692b2bf2ef2..e3de3f33eb72 100644
--- a/drivers/nvme/mdev/instance.c
+++ b/drivers/nvme/mdev/instance.c
@@ -10,7 +10,13 @@
 #include <linux/vfio.h>
 #include <linux/sysfs.h>
 #include <linux/mdev.h>
+#include <linux/nvme_mdev.h>
 #include "priv.h"
+#include "iothreads.h"
+
+#ifdef CONFIG_NVME_MDEV_BPF
+#include "notifyfd.h"
+#endif
 
 #define OFFSET_TO_REGION(offset) ((offset) >> 20)
 #define REGION_TO_OFFSET(nr) (((u64)nr) << 20)
@@ -32,8 +38,8 @@ static int nvme_mdev_map_notifier(struct notifier_block *nb,
 	struct nvme_mdev_vctrl *vctrl =
 		container_of(nb, struct nvme_mdev_vctrl, vfio_map_notifier);
 
-	int ret = nvme_mdev_vctrl_viommu_map(vctrl, map->flags,
-			map->iova, map->size);
+	int ret = nvme_mdev_vctrl_viommu_map(vctrl, map->flags, map->iova,
+					     map->size);
 	return ret ? NOTIFY_OK : notifier_from_errno(ret);
 }
 
@@ -99,7 +105,7 @@ static int nvme_mdev_ops_open(struct mdev_device *mdev)
 	if (!vctrl)
 		return -ENODEV;
 
-	ret =  nvme_mdev_vctrl_open(vctrl);
+	ret = nvme_mdev_vctrl_open(vctrl);
 	if (ret)
 		return ret;
 
@@ -107,9 +113,8 @@ static int nvme_mdev_ops_open(struct mdev_device *mdev)
 	vctrl->vfio_unmap_notifier.notifier_call = nvme_mdev_unmap_notifier;
 	events = VFIO_IOMMU_NOTIFY_DMA_UNMAP;
 
-	ret = vfio_register_notifier(mdev_dev(vctrl->mdev),
-				     VFIO_IOMMU_NOTIFY, &events,
-				     &vctrl->vfio_unmap_notifier);
+	ret = vfio_register_notifier(mdev_dev(vctrl->mdev), VFIO_IOMMU_NOTIFY,
+				     &events, &vctrl->vfio_unmap_notifier);
 
 	if (ret != 0) {
 		nvme_mdev_vctrl_release(vctrl);
@@ -120,9 +125,8 @@ static int nvme_mdev_ops_open(struct mdev_device *mdev)
 	vctrl->vfio_map_notifier.notifier_call = nvme_mdev_map_notifier;
 	events = VFIO_IOMMU_NOTIFY_DMA_MAP;
 
-	ret = vfio_register_notifier(mdev_dev(vctrl->mdev),
-				     VFIO_IOMMU_NOTIFY, &events,
-				     &vctrl->vfio_map_notifier);
+	ret = vfio_register_notifier(mdev_dev(vctrl->mdev), VFIO_IOMMU_NOTIFY,
+				     &events, &vctrl->vfio_map_notifier);
 
 	if (ret != 0) {
 		vfio_unregister_notifier(mdev_dev(vctrl->mdev),
@@ -137,26 +141,12 @@ static int nvme_mdev_ops_open(struct mdev_device *mdev)
 /* Called when new mediated device is closed (last close of the user) */
 static void nvme_mdev_ops_release(struct mdev_device *mdev)
 {
-	struct nvme_mdev_vctrl *vctrl = mdev_to_vctrl(mdev);
-	int ret;
-
-	ret = vfio_unregister_notifier(mdev_dev(vctrl->mdev),
-				       VFIO_IOMMU_NOTIFY,
-				       &vctrl->vfio_unmap_notifier);
-	WARN_ON(ret);
-
-	ret = vfio_unregister_notifier(mdev_dev(vctrl->mdev),
-				       VFIO_IOMMU_NOTIFY,
-				       &vctrl->vfio_map_notifier);
-	WARN_ON(ret);
-
-	nvme_mdev_vctrl_release(vctrl);
+	nvme_mdev_vctrl_release(mdev_to_vctrl(mdev));
 }
 
 /* Helper function for bar/pci config read/write access */
-static ssize_t nvme_mdev_access(struct nvme_mdev_vctrl *vctrl,
-				char *buf, size_t count,
-				loff_t pos, bool is_write)
+static ssize_t nvme_mdev_access(struct nvme_mdev_vctrl *vctrl, char *buf,
+				size_t count, loff_t pos, bool is_write)
 {
 	int index = OFFSET_TO_REGION(pos);
 	int ret = -EINVAL;
@@ -191,8 +181,8 @@ static ssize_t nvme_mdev_ops_read(struct mdev_device *mdev, char __user *buf,
 		if (count >= 4 && !(*ppos % 4)) {
 			u32 val;
 
-			ret = nvme_mdev_access(vctrl, (char *)&val,
-					       sizeof(val), *ppos, false);
+			ret = nvme_mdev_access(vctrl, (char *)&val, sizeof(val),
+					       *ppos, false);
 			if (ret <= 0)
 				goto read_err;
 
@@ -202,8 +192,8 @@ static ssize_t nvme_mdev_ops_read(struct mdev_device *mdev, char __user *buf,
 		} else if (count >= 2 && !(*ppos % 2)) {
 			u16 val;
 
-			ret = nvme_mdev_access(vctrl, (char *)&val,
-					       sizeof(val), *ppos, false);
+			ret = nvme_mdev_access(vctrl, (char *)&val, sizeof(val),
+					       *ppos, false);
 			if (ret <= 0)
 				goto read_err;
 			if (copy_to_user(buf, &val, sizeof(val)))
@@ -212,8 +202,8 @@ static ssize_t nvme_mdev_ops_read(struct mdev_device *mdev, char __user *buf,
 		} else {
 			u8 val;
 
-			ret = nvme_mdev_access(vctrl, (char *)&val,
-					       sizeof(val), *ppos, false);
+			ret = nvme_mdev_access(vctrl, (char *)&val, sizeof(val),
+					       *ppos, false);
 			if (ret <= 0)
 				goto read_err;
 			if (copy_to_user(buf, &val, sizeof(val)))
@@ -233,8 +223,8 @@ static ssize_t nvme_mdev_ops_read(struct mdev_device *mdev, char __user *buf,
 
 /* Called when write() is done on the device */
 static ssize_t nvme_mdev_ops_write(struct mdev_device *mdev,
-				   const char __user *buf,
-				   size_t count, loff_t *ppos)
+				   const char __user *buf, size_t count,
+				   loff_t *ppos)
 {
 	unsigned int done = 0;
 	int ret;
@@ -251,8 +241,8 @@ static ssize_t nvme_mdev_ops_write(struct mdev_device *mdev,
 
 			if (copy_from_user(&val, buf, sizeof(val)))
 				goto write_err;
-			ret = nvme_mdev_access(vctrl, (char *)&val,
-					       sizeof(val), *ppos, true);
+			ret = nvme_mdev_access(vctrl, (char *)&val, sizeof(val),
+					       *ppos, true);
 			if (ret <= 0)
 				goto write_err;
 			filled = sizeof(val);
@@ -262,8 +252,8 @@ static ssize_t nvme_mdev_ops_write(struct mdev_device *mdev,
 			if (copy_from_user(&val, buf, sizeof(val)))
 				goto write_err;
 
-			ret = nvme_mdev_access(vctrl, (char *)&val,
-					       sizeof(val), *ppos, true);
+			ret = nvme_mdev_access(vctrl, (char *)&val, sizeof(val),
+					       *ppos, true);
 			if (ret <= 0)
 				goto write_err;
 			filled = sizeof(val);
@@ -272,8 +262,8 @@ static ssize_t nvme_mdev_ops_write(struct mdev_device *mdev,
 
 			if (copy_from_user(&val, buf, sizeof(val)))
 				goto write_err;
-			ret = nvme_mdev_access(vctrl, (char *)&val,
-					       sizeof(val), *ppos, true);
+			ret = nvme_mdev_access(vctrl, (char *)&val, sizeof(val),
+					       *ppos, true);
 			if (ret <= 0)
 				goto write_err;
 			filled = sizeof(val);
@@ -306,11 +296,9 @@ static int nvme_mdev_irq_counts(struct nvme_mdev_vctrl *vctrl,
 
 /* VFIO VFIO_IRQ_SET_ACTION_TRIGGER implementation */
 static int nvme_mdev_ioctl_set_irqs_trigger(struct nvme_mdev_vctrl *vctrl,
-					    u32 flags,
-					    unsigned int irq_type,
+					    u32 flags, unsigned int irq_type,
 					    unsigned int start,
-					    unsigned int count,
-					    void *data)
+					    unsigned int count, void *data)
 {
 	u32 data_type = flags & VFIO_IRQ_SET_DATA_TYPE_MASK;
 	u8 *bools = NULL;
@@ -345,7 +333,7 @@ static int nvme_mdev_ioctl_set_irqs_trigger(struct nvme_mdev_vctrl *vctrl,
 		if (irq_type == VFIO_PCI_REQ_IRQ_INDEX)
 			return -EINVAL;
 
-		for (i = 0 ; i < count ; i++) {
+		for (i = 0; i < count; i++) {
 			int index = start + i;
 
 			if (!bools || bools[i])
@@ -356,8 +344,8 @@ static int nvme_mdev_ioctl_set_irqs_trigger(struct nvme_mdev_vctrl *vctrl,
 	case VFIO_IRQ_SET_DATA_EVENTFD:
 		switch (irq_type) {
 		case VFIO_PCI_REQ_IRQ_INDEX:
-			return nvme_mdev_irqs_set_unplug_trigger(vctrl,
-							*(int32_t *)data);
+			return nvme_mdev_irqs_set_unplug_trigger(
+				vctrl, *(int32_t *)data);
 		case VFIO_PCI_INTX_IRQ_INDEX:
 			ret = nvme_mdev_irqs_enable(vctrl,
 						    NVME_MDEV_IMODE_INTX);
@@ -372,8 +360,8 @@ static int nvme_mdev_ioctl_set_irqs_trigger(struct nvme_mdev_vctrl *vctrl,
 		if (ret)
 			return ret;
 
-		return nvme_mdev_irqs_set_triggers(vctrl, start,
-						   count, (int32_t *)data);
+		return nvme_mdev_irqs_set_triggers(vctrl, start, count,
+						   (int32_t *)data);
 	default:
 		return -EINVAL;
 	}
@@ -411,7 +399,7 @@ static int nvme_mdev_ioctl_get_reg_info(struct nvme_mdev_vctrl *vctrl,
 
 	minsz = offsetofend(struct vfio_region_info, offset);
 	maxsz = sizeof(struct mdev_nvme_vfio_region_info) +
-				sizeof(struct vfio_region_sparse_mmap_area);
+		sizeof(struct vfio_region_sparse_mmap_area);
 
 	info = kzalloc(maxsz, GFP_KERNEL);
 	if (!info)
@@ -438,12 +426,12 @@ static int nvme_mdev_ioctl_get_reg_info(struct nvme_mdev_vctrl *vctrl,
 	info->base.argsz = maxsz;
 	info->base.size = region->size;
 
-	info->base.flags = VFIO_REGION_INFO_FLAG_READ |
-				VFIO_REGION_INFO_FLAG_WRITE;
+	info->base.flags =
+		VFIO_REGION_INFO_FLAG_READ | VFIO_REGION_INFO_FLAG_WRITE;
 
 	if (region->mmap_ops) {
 		info->base.flags |= (VFIO_REGION_INFO_FLAG_MMAP |
-						VFIO_REGION_INFO_FLAG_CAPS);
+				     VFIO_REGION_INFO_FLAG_CAPS);
 
 		info->base.cap_offset =
 			offsetof(struct mdev_nvme_vfio_region_info, mmap_cap);
@@ -500,10 +488,8 @@ static int nvme_mdev_ioctl_set_irqs(struct nvme_mdev_vctrl *vctrl,
 		return -EFAULT;
 
 	irqcount = nvme_mdev_irq_counts(vctrl, hdr.index);
-	ret = vfio_set_irqs_validate_and_prepare(&hdr,
-						 irqcount,
-						 VFIO_PCI_NUM_IRQS,
-						 &data_size);
+	ret = vfio_set_irqs_validate_and_prepare(&hdr, irqcount,
+						 VFIO_PCI_NUM_IRQS, &data_size);
 	if (ret)
 		return ret;
 
@@ -522,15 +508,14 @@ static int nvme_mdev_ioctl_set_irqs(struct nvme_mdev_vctrl *vctrl,
 		case VFIO_IRQ_SET_ACTION_MASK:
 		case VFIO_IRQ_SET_ACTION_UNMASK:
 			// pretend to support this (even with eventfd)
-			ret = hdr.index == VFIO_PCI_INTX_IRQ_INDEX ?
-					0 : -EINVAL;
+			ret = hdr.index == VFIO_PCI_INTX_IRQ_INDEX ? 0 :
+								     -EINVAL;
 			break;
 		case VFIO_IRQ_SET_ACTION_TRIGGER:
 			ret = nvme_mdev_ioctl_set_irqs_trigger(vctrl, hdr.flags,
 							       hdr.index,
 							       hdr.start,
-							       hdr.count,
-							       data);
+							       hdr.count, data);
 			break;
 		}
 		break;
@@ -540,6 +525,42 @@ static int nvme_mdev_ioctl_set_irqs(struct nvme_mdev_vctrl *vctrl,
 	return ret;
 }
 
+#ifdef CONFIG_NVME_MDEV_BPF
+static int nvme_mdev_ioctl_attach_bpf(struct nvme_mdev_vctrl *vctrl, int ufd)
+{
+	struct bpf_prog *prog = bpf_prog_get_type(ufd, BPF_PROG_TYPE_NVME_MDEV);
+	int err;
+
+	if (IS_ERR(prog))
+		return PTR_ERR(prog);
+
+	err = nvme_mdev_vctrl_attach_prog(vctrl, prog);
+	if (err < 0) {
+		if (prog)
+			bpf_prog_put(prog);
+		return err;
+	}
+
+	return 0;
+}
+
+static int nvme_mdev_ioctl_detach_bpf(struct nvme_mdev_vctrl *vctrl)
+{
+	return nvme_mdev_vctrl_attach_prog(vctrl, NULL);
+}
+
+static int nvme_mdev_ioctl_open_notifyfd(struct nvme_mdev_vctrl *vctrl,
+					 struct nmntfy_open_arg __user *arg)
+{
+	struct nmntfy_open_arg karg;
+	if (copy_from_user(&karg, arg, sizeof(karg)))
+		return -EFAULT;
+	if (!karg.sqid || karg.sqid >= MAX_VIRTUAL_QUEUES)
+		return -EINVAL;
+	return nvme_mdev_notifyfd_open(vctrl, karg.sqid);
+}
+#endif /* CONFIG_NVME_MDEV_BPF */
+
 /* ioctl() implementation */
 static long nvme_mdev_ops_ioctl(struct mdev_device *mdev, unsigned int cmd,
 				unsigned long arg)
@@ -561,6 +582,15 @@ static long nvme_mdev_ops_ioctl(struct mdev_device *mdev, unsigned int cmd,
 	case VFIO_DEVICE_RESET:
 		nvme_mdev_vctrl_reset(vctrl);
 		return 0;
+#ifdef CONFIG_NVME_MDEV_BPF
+	case NVME_MDEV_ATTACH_BPF:
+		return nvme_mdev_ioctl_attach_bpf(vctrl, (int)arg);
+	case NVME_MDEV_DETACH_BPF:
+		return nvme_mdev_ioctl_detach_bpf(vctrl);
+	case NVME_MDEV_OPEN_FD:
+		return nvme_mdev_ioctl_open_notifyfd(
+			vctrl, (struct nmntfy_open_arg __user *)arg);
+#endif
 	default:
 		return -ENOTTY;
 	}
@@ -676,7 +706,7 @@ static ssize_t remove_namespace_store(struct device *dev,
 	if (ret)
 		return ret;
 
-	ret =  nvme_mdev_vns_destroy(vctrl, user_nsid);
+	ret = nvme_mdev_vns_destroy(vctrl, user_nsid);
 	if (ret)
 		return ret;
 	return count;
@@ -695,35 +725,56 @@ static ssize_t namespaces_show(struct device *dev,
 }
 static DEVICE_ATTR_RO(namespaces);
 
-/* change the cpu binding of the IO threads*/
-static ssize_t iothread_cpu_store(struct device *dev,
+static ssize_t iothread_idx_store(struct device *dev,
 				  struct device_attribute *attr,
 				  const char *buf, size_t count)
 {
 	unsigned long val;
 	int ret;
 	struct nvme_mdev_vctrl *vctrl = dev_to_vctrl(dev);
+	bool old_enabled = false;
 
 	if (!vctrl)
 		return -ENODEV;
 	ret = kstrtoul(buf, 10, &val);
 	if (ret)
 		return ret;
-	nvme_mdev_vctrl_bind_iothread(vctrl, val);
+	if (val >= num_iothreads)
+		return -EINVAL;
+
+	mutex_lock(&vctrl->lock);
+	if (vctrl->binding && READ_ONCE(vctrl->binding->poll_enabled))
+		old_enabled = true;
+	mutex_unlock(&vctrl->lock);
+	nvme_mdev_io_unbind_all(vctrl);
+	ret = nvme_mdev_io_bind(vctrl, &iothreads[val], old_enabled);
+	if (ret)
+		return ret;
+
 	return count;
 }
 
-/* change the cpu binding of the IO threads*/
-static ssize_t
-iothread_cpu_show(struct device *dev, struct device_attribute *attr, char *buf)
+static ssize_t iothread_idx_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
 {
 	struct nvme_mdev_vctrl *vctrl = dev_to_vctrl(dev);
+	unsigned int ret = MAX_IOTHREADS, i;
 
 	if (!vctrl)
 		return -ENODEV;
-	return sprintf(buf, "%d\n", vctrl->iothread_cpu);
+
+	mutex_lock(&vctrl->lock);
+	for (i = 0; i < num_iothreads; i++)
+		if (vctrl->binding->it == &iothreads[i])
+			ret = i;
+	mutex_unlock(&vctrl->lock);
+
+	if (ret < MAX_IOTHREADS)
+		return sprintf(buf, "%u\n", ret);
+	else
+		return -ENODEV;
 }
-static DEVICE_ATTR_RW(iothread_cpu);
+static DEVICE_ATTR_RW(iothread_idx);
 
 /* change the cpu binding of the IO threads*/
 static ssize_t shadow_doorbell_store(struct device *dev,
@@ -759,21 +810,17 @@ static ssize_t shadow_doorbell_show(struct device *dev,
 static DEVICE_ATTR_RW(shadow_doorbell);
 
 static struct attribute *nvme_mdev_dev_ns_atttributes[] = {
-	&dev_attr_add_namespace.attr,
-	&dev_attr_remove_namespace.attr,
-	&dev_attr_namespaces.attr,
-	NULL
+	&dev_attr_add_namespace.attr, &dev_attr_remove_namespace.attr,
+	&dev_attr_namespaces.attr, NULL
 };
 
 static struct attribute *nvme_mdev_dev_settings_atttributes[] = {
-	&dev_attr_iothread_cpu.attr,
-	&dev_attr_shadow_doorbell.attr,
-	NULL
+	&dev_attr_iothread_idx.attr, &dev_attr_shadow_doorbell.attr, NULL
 };
 
 /* show perf stats */
-static ssize_t stats_show(struct device *dev,
-			  struct device_attribute *attr, char *buf)
+static ssize_t stats_show(struct device *dev, struct device_attribute *attr,
+			  char *buf)
 {
 	struct nvme_mdev_vctrl *vctrl = dev_to_vctrl(dev);
 	struct nvme_mdev_perf *perf;
@@ -783,24 +830,14 @@ static ssize_t stats_show(struct device *dev,
 
 	perf = &vctrl->perf;
 
-	return sprintf(buf,
-		"%u %llu %llu %llu %llu %llu %llu\n",
-
-		tsc_khz,
-
-		perf->cmds_started,
-		perf->cycles_send_to_hw,
-
-		perf->cmds_complete,
-		perf->cycles_receive_from_hw,
-
-		perf->interrupts_sent,
-		perf->cycles_irq_delivery);
+	return sprintf(buf, "%u %llu %llu %llu %llu %llu %llu\n", tsc_khz,
+		       perf->cmds_started, perf->cycles_send_to_hw,
+		       perf->cmds_complete, perf->cycles_receive_from_hw,
+		       perf->interrupts_sent, perf->cycles_irq_delivery);
 }
 
 /* clear the perf stats */
-static ssize_t stats_store(struct device *dev,
-			   struct device_attribute *attr,
+static ssize_t stats_store(struct device *dev, struct device_attribute *attr,
 			   const char *buf, size_t count)
 {
 	bool val;
@@ -823,8 +860,7 @@ static ssize_t stats_store(struct device *dev,
 static DEVICE_ATTR_RW(stats);
 
 static struct attribute *nvme_mdev_dev_debug_attributes[] = {
-	&dev_attr_stats.attr,
-	NULL
+	&dev_attr_stats.attr, NULL
 };
 
 static const struct attribute_group nvme_mdev_ns_attr_group = {
@@ -850,17 +886,16 @@ static const struct attribute_group *nvme_mdev_dev_attributte_groups[] = {
 };
 
 struct mdev_parent_ops mdev_fops = {
-	.owner			= THIS_MODULE,
-	.create			= nvme_mdev_ops_create,
-	.remove			= nvme_mdev_ops_remove,
-	.open			= nvme_mdev_ops_open,
-	.release		= nvme_mdev_ops_release,
-	.read			= nvme_mdev_ops_read,
-	.write			= nvme_mdev_ops_write,
-	.mmap			= nvme_mdev_ops_mmap,
-	.ioctl			= nvme_mdev_ops_ioctl,
-	.request		= nvme_mdev_ops_request,
-	.mdev_attr_groups	= nvme_mdev_dev_attributte_groups,
-	.dev_attr_groups	= NULL,
+	.owner = THIS_MODULE,
+	.create = nvme_mdev_ops_create,
+	.remove = nvme_mdev_ops_remove,
+	.open = nvme_mdev_ops_open,
+	.release = nvme_mdev_ops_release,
+	.read = nvme_mdev_ops_read,
+	.write = nvme_mdev_ops_write,
+	.mmap = nvme_mdev_ops_mmap,
+	.ioctl = nvme_mdev_ops_ioctl,
+	.request = nvme_mdev_ops_request,
+	.mdev_attr_groups = nvme_mdev_dev_attributte_groups,
+	.dev_attr_groups = NULL,
 };
-
diff --git a/drivers/nvme/mdev/io.c b/drivers/nvme/mdev/io.c
index d3c46de33b01..d53a2eeb8b41 100644
--- a/drivers/nvme/mdev/io.c
+++ b/drivers/nvme/mdev/io.c
@@ -9,39 +9,29 @@
 #include <linux/nvme.h>
 #include <linux/timekeeping.h>
 #include <linux/ktime.h>
+#include <linux/filter.h>
+#include <linux/bpf_nvme_mdev.h>
 #include <asm/msr.h>
 #include "priv.h"
-
-
-struct io_ctx {
-	struct nvme_mdev_hctrl *hctrl;
-	struct nvme_mdev_vctrl *vctrl;
-
-	const struct nvme_command *in;
-	struct nvme_mdev_vns *ns;
-	struct nvme_ext_data_iter udatait;
-
-	struct nvme_command out;
-	struct nvme_ext_data_iter *kdatait;
-
-	ktime_t last_io_t;
-	ktime_t last_admin_poll_time;
-	unsigned int idle_timeout_ms;
-	unsigned int admin_poll_rate_ms;
-	unsigned int arb_burst;
-};
+#include "notifyfd.h"
+#include "bpf_sm.h"
+#include "iothreads.h"
 
 /* Check if we need to read a command from the admin queue */
 static bool nvme_mdev_adm_needs_processing(struct io_ctx *ctx)
 {
-	if (!timeout(ctx->last_admin_poll_time,
-		     ctx->vctrl->now, ctx->admin_poll_rate_ms))
+	struct nvme_vsq *adm_sq;
+
+	if (!timeout(ctx->vctrl->binding->last_admin_poll_time, ctx->vctrl->now,
+		     admin_poll_rate_ms))
 		return false;
 
-	if (nvme_mdev_vsq_has_data(ctx->vctrl, &ctx->vctrl->vsqs[0]))
+	adm_sq = rcu_dereference(ctx->vctrl->vsqs[0]);
+	BUG_ON(!adm_sq);
+	if (nvme_mdev_vsq_has_data(ctx->vctrl, adm_sq))
 		return true;
 
-	ctx->last_admin_poll_time = ctx->vctrl->now;
+	ctx->vctrl->binding->last_admin_poll_time = ctx->vctrl->now;
 	return false;
 }
 
@@ -98,8 +88,8 @@ static int nvme_mdev_io_translate_flush(struct io_ctx *ctx)
 	_DBG(ctx->vctrl, "IOQ: FLUSH\n");
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
-				   RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW10_15))
+				   RSRV_DW23 | RSRV_DPTR | RSRV_MPTR |
+					   RSRV_DW10_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	if (ctx->ns->readonly)
@@ -119,8 +109,8 @@ static int nvme_mdev_io_translate_write_zeros(struct io_ctx *ctx)
 	_DBG(ctx->vctrl, "IOQ: WRITE_ZEROS\n");
 
 	if (!check_reserved_dwords(ctx->in->dwords, 16,
-				   RSRV_DW23 | RSRV_DPTR |
-				   RSRV_MPTR | RSRV_DW13_15))
+				   RSRV_DW23 | RSRV_DPTR | RSRV_MPTR |
+					   RSRV_DW13_15))
 		return DNR(NVME_SC_INVALID_FIELD);
 
 	if (!nvme_mdev_hctrl_hq_check_op(ctx->hctrl, in->opcode))
@@ -188,7 +178,7 @@ static int nvme_mdev_io_translate_dsm(struct io_ctx *ctx)
 
 	data_ptr = (struct nvme_dsm_range *)ctx->kdatait->kmem.data;
 
-	for (i = 0 ; i < nr; i++) {
+	for (i = 0; i < nr; i++) {
 		u64 slba = le64_to_cpu(data_ptr[i].slba);
 		/* looks like not zero based value*/
 		u32 nlb = le32_to_cpu(data_ptr[i].nlb);
@@ -216,6 +206,7 @@ static int nvme_mdev_io_translate_dsm(struct io_ctx *ctx)
 static int nvme_mdev_io_translate_cmd(struct io_ctx *ctx)
 {
 	memset(&ctx->out, 0, sizeof(ctx->out));
+
 	/* translate opcode */
 	ctx->out.common.opcode = ctx->in->common.opcode;
 
@@ -252,67 +243,141 @@ static int nvme_mdev_io_translate_cmd(struct io_ctx *ctx)
 	}
 }
 
+static int nvme_mdev_io_passthrough(struct io_ctx *ctx, struct nvme_vsq *vsq,
+				    u16 sqid, u16 ucid, u32 tag)
+{
+	/* translate the command */
+	int ret = nvme_mdev_io_translate_cmd(ctx);
+	if (ret != -1) {
+		_INFO(ctx->vctrl,
+		      "IOQ: QID %d CID %d FAILED: status 0x%x (translate)\n",
+		      sqid, ucid, ret);
+		nvme_mdev_vsq_cmd_done_io(ctx->vctrl, vsq, ucid, ret);
+		return ret;
+	}
+
+	/*passthrough*/
+	ret = nvme_mdev_hctrl_hq_submit(ctx->hctrl, ctx->ns, vsq->hsq, tag,
+					&ctx->out, ctx->kdatait);
+	if (ret) {
+		ret = nvme_mdev_translate_error(ret);
+
+		_INFO(ctx->vctrl,
+		      "IOQ: QID %d CID %d FAILED: status 0x%x (host submit)\n",
+		      sqid, ucid, ret);
+
+		nvme_mdev_vsq_cmd_done_io(ctx->vctrl, vsq, ucid, ret);
+		return ret;
+	}
+
+	return -1;
+}
+
+#ifdef CONFIG_NVME_MDEV_BPF
+int nvme_mdev_io_act(struct io_ctx *ctx, struct nvme_vsq *vsq, u16 ucid,
+		     bool needs_react)
+{
+	int ret = -1;
+	u32 tag = NVME_MDEV_MKTAG(vsq->qid, ucid);
+	int next_cmpl;
+
+	ctx->in = &ctx->bctx.cmd;
+
+	if (needs_react) {
+		if (ctx->bctx.iostate & NMBPF_SEND_HQ) {
+			ret = nvme_mdev_io_passthrough(ctx, vsq, vsq->qid, ucid,
+						       tag);
+			if (ret != -1)
+				ctx->bctx.iostate = nvme_mdev_bpfsm_update(
+					vsq, ucid, ret, NMBPF_WILL_COMPLETE_HQ);
+		}
+
+		if ((ctx->bctx.iostate & NMBPF_SEND_FD)) {
+			while (!nvme_mdev_notifyfd_send(vsq, ctx->in))
+				;
+		}
+	}
+
+	next_cmpl = ctx->bctx.iostate & NMBPF_COMPLETION_MASK;
+	if (next_cmpl == NMBPF_COMPLETE) {
+		if (nvme_mdev_bpfsm_runhook(&ctx->bctx, vsq, ucid,
+					    NMBPF_HOOK_PRE_VCQ, 0))
+			return nvme_mdev_io_act(ctx, vsq, ucid, true);
+		else {
+			u16 status =
+				(u16)(ctx->bctx.iostate & NMBPF_STATUS_MASK);
+			nvme_mdev_vsq_cmd_done_io(ctx->vctrl, vsq, ucid,
+						  status);
+		}
+	}
+
+	return ret;
+}
+#endif
+
 /* process a user submission queue */
-static bool nvme_mdev_io_process_sq(struct io_ctx *ctx, u16 sqid)
+static int nvme_mdev_io_process_sq(struct io_ctx *ctx, struct nvme_vsq *vsq)
 {
-	struct nvme_vsq *vsq = &ctx->vctrl->vsqs[sqid];
 	u16 ucid;
-	int ret;
 	unsigned long long c1, c2;
 
+	u32 tag;
+#ifdef CONFIG_NVME_MDEV_BPF
+	int ret, oldstate;
+#endif
+
 	c1 = rdtsc();
 
 	/* If host queue is full, we can't process a command
 	 * as a command will likely result in passthrough
 	 */
 	if (!nvme_mdev_hctrl_hq_can_submit(ctx->hctrl, vsq->hsq))
-		return false;
+		return -1;
 
 	/* read the command */
 	ctx->in = nvme_mdev_vsq_get_cmd(ctx->vctrl, vsq);
 	if (!ctx->in)
-		return false;
+		return -1;
 	ucid = le16_to_cpu(ctx->in->common.command_id);
-
-	/* translate the command */
-	ret = nvme_mdev_io_translate_cmd(ctx);
-	if (ret != -1) {
-		_DBG(ctx->vctrl,
-		     "IOQ: QID %d CID %d FAILED: status 0x%x (translate)\n",
-		     sqid, ucid, ret);
-		nvme_mdev_vsq_cmd_done_io(ctx->vctrl, sqid, ucid, ret);
-		return true;
+	tag = NVME_MDEV_MKTAG(vsq->qid, ucid);
+
+#ifdef CONFIG_NVME_MDEV_BPF
+	/* vsq->ctx_data[ucid].cmd = *ctx->in; */
+	ctx->bctx.cmd = *ctx->in;
+	oldstate = atomic_xchg(&vsq->ctx_data[ucid].iostate, 0);
+	if ((oldstate & NMBPF_SEND_MASK) ||
+	    (oldstate & NMBPF_COMPLETION_MASK)) {
+		_INFO(ctx->vctrl,
+		      "IOQ: QID %d CID %d unexpected oldstate: %#x\n", vsq->qid,
+		      ucid, oldstate);
 	}
 
-	/*passthrough*/
-	ret = nvme_mdev_hctrl_hq_submit(ctx->hctrl,
-					ctx->ns,
-					vsq->hsq,
-					(((u32)vsq->qid) << 16) | ((u32)ucid),
-					&ctx->out,
-					ctx->kdatait);
-	if (ret) {
-		ret = nvme_mdev_translate_error(ret);
+	if (!nvme_mdev_bpfsm_runhook(&ctx->bctx, vsq, ucid, NMBPF_HOOK_VSQ, 0))
+		ctx->bctx.iostate = nvme_mdev_bpfsm_set(
+			vsq, ucid, NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ);
 
-		_DBG(ctx->vctrl,
-		     "IOQ: QID %d CID %d FAILED: status 0x%x (host submit)\n",
-		     sqid, ucid, ret);
+	vsq->ctx_data[ucid].cmd = ctx->bctx.cmd;
 
-		nvme_mdev_vsq_cmd_done_io(ctx->vctrl, sqid, ucid, ret);
-	}
+	ret = nvme_mdev_io_act(ctx, vsq, ucid, true);
+	if (ret != -1)
+		_INFO(ctx->vctrl, "IOQ: QID %d CID %d bpf act failed: %#x\n",
+		      vsq->qid, ucid, ret);
+#else
+	nvme_mdev_io_passthrough(ctx, vsq, sqid, ucid, tag);
+#endif
 
 	c2 = rdtsc();
 
 	ctx->vctrl->perf.cmds_started++;
 	ctx->vctrl->perf.cycles_send_to_hw += (c2 - c1);
 
-	return true;
+	return 0;
 }
 
 /* process a user completion queue */
-static void nvme_mdev_io_process_cq(struct io_ctx *ctx, u16 cqid)
+static void nvme_mdev_io_process_cq(struct io_ctx *ctx, struct nvme_vcq *vcq)
 {
-	nvme_mdev_vcq_process(ctx->vctrl, cqid, true);
+	nvme_mdev_vcq_process(ctx->vctrl, vcq, true);
 }
 
 /* process hardware completion queue */
@@ -323,6 +388,10 @@ static int nvme_mdev_io_process_hwq(struct io_ctx *ctx, u16 hwq)
 
 	unsigned long long c1, c2;
 
+#ifdef CONFIG_NVME_MDEV_BPF
+	bool needs_react;
+#endif
+
 	c1 = rdtsc();
 
 	/* process the completions from the hardware */
@@ -330,18 +399,32 @@ static int nvme_mdev_io_process_hwq(struct io_ctx *ctx, u16 hwq)
 	if (n == -1)
 		return -1;
 
+	rcu_read_lock();
 	for (i = 0; i < n; i++) {
-		u16 qid = res[i].tag >> 16;
-		u16 cid = res[i].tag & 0xFFFF;
+		u16 qid = NVME_MDEV_TAG_SQID(res[i].tag);
+		u16 cid = NVME_MDEV_TAG_UCID(res[i].tag);
 		u16 status = res[i].status;
+		struct nvme_vsq *vsq = rcu_dereference(ctx->vctrl->vsqs[qid]);
+		if (!vsq)
+			continue;
 
 		if (status != 0)
 			_DBG(ctx->vctrl,
 			     "IOQ: QID %d CID %d FAILED: status 0x%x (host response)\n",
 			     qid, cid, status);
 
-		nvme_mdev_vsq_cmd_done_io(ctx->vctrl, qid, cid, status);
+#ifdef CONFIG_NVME_MDEV_BPF
+		needs_react = nvme_mdev_bpfsm_runhook(&ctx->bctx, vsq, cid,
+						      NMBPF_HOOK_HCQ, status);
+		if (!needs_react)
+			ctx->bctx.iostate = nvme_mdev_bpfsm_update(
+				vsq, cid, status, NMBPF_WILL_COMPLETE_HQ);
+		nvme_mdev_io_act(ctx, vsq, cid, needs_react);
+#else
+		nvme_mdev_vsq_cmd_done_io(ctx->vctrl, vsq, cid, status);
+#endif
 	}
+	rcu_read_unlock();
 
 	if (n > 0) {
 		c2 = rdtsc();
@@ -352,81 +435,149 @@ static int nvme_mdev_io_process_hwq(struct io_ctx *ctx, u16 hwq)
 	return n;
 }
 
+static void nvme_mdev_io_process_sq_burst(struct io_ctx *ctx,
+					  struct nvme_vsq *vsq)
+{
+	u16 i;
+#ifdef CONFIG_NVME_MDEV_BPF
+	struct nvme_mdev_notifier *notifier;
+
+	for (i = 0; i < vsq->vctrl->arb_burst; i++)
+		if (nvme_mdev_io_process_sq(ctx, vsq) < 0)
+			break;
+	notifier = rcu_dereference(vsq->vctrl->notifier[vsq->qid]);
+	if (notifier)
+		wake_up_interruptible(&notifier->wqh);
+#else
+	for (i = 0; i < vctrl->arb_burst; i++)
+		nvme_mdev_io_process_sq(ctx, vsq);
+#endif
+}
+
+void nvme_mdev_io_begin_poll(struct nvme_mdev_vctrl_iothread_binding *binding)
+{
+	struct nvme_mdev_vctrl *vctrl = binding->vctrl;
+	BUG_ON(binding->state != NVME_MDEV_BINDING_ZERO);
+	binding->ctx.vctrl = vctrl;
+	binding->ctx.hctrl = vctrl->hctrl;
+	nvme_mdev_udata_iter_setup(&vctrl->viommu, &binding->ctx.udatait);
+	binding->hsqcnt = nvme_mdev_vctrl_hqs_list(vctrl, binding->hsqs);
+	binding->idle = false;
+	/* can't stop polling when shadow db not enabled */
+	binding->idle_timeout_ms =
+		vctrl->mmio.shadow_db_en ? poll_timeout_ms : 0;
+	vctrl->now = ktime_get();
+	binding->last_admin_poll_time = binding->last_io_t = vctrl->now;
+	binding->state = NVME_MDEV_BINDING_BEGUN;
+}
 
 /* do polling till one of events stops it */
-static void nvme_mdev_io_polling_loop(struct io_ctx *ctx)
+int nvme_mdev_io_polling_loop(struct io_ctx *ctx)
 {
 	struct nvme_mdev_vctrl *vctrl = ctx->vctrl;
-	u16 i, cqid, sqid, hsqcnt;
-	u16 hsqs[MAX_HOST_QUEUES];
-	bool idle = false;
+	struct nvme_mdev_vctrl_iothread_binding *binding = vctrl->binding;
 
-	hsqcnt = nvme_mdev_vctrl_hqs_list(vctrl, hsqs);
-	ctx->arb_burst = 1 << ctx->vctrl->arb_burst_shift;
+	u16 i, cqid, sqid;
+	unsigned int iter;
 
-	/* can't stop polling when shadow db not enabled */
-	ctx->idle_timeout_ms = vctrl->mmio.shadow_db_en ? poll_timeout_ms : 0;
-	ctx->admin_poll_rate_ms = admin_poll_rate_ms;
-
-	vctrl->now = ktime_get();
-	ctx->last_admin_poll_time = vctrl->now;
-	ctx->last_io_t = vctrl->now;
+	BUG_ON(!binding || binding->state != NVME_MDEV_BINDING_BEGUN);
 
 	/* main loop */
-	while (!kthread_should_park()) {
+	for (iter = 0; iter < vsq_poll_loops; iter++) {
+		if (!READ_ONCE(binding->poll_enabled))
+			return 0;
+
 		vctrl->now = ktime_get();
 
+		rcu_read_lock();
 		/* check if we have to exit to support admin polling */
-		if (!vctrl->mmio.shadow_db_supported)
-			if (nvme_mdev_adm_needs_processing(ctx))
-				break;
+		if (!vctrl->mmio.shadow_db_supported) {
+			if (nvme_mdev_adm_needs_processing(ctx)) {
+				rcu_read_unlock();
+				return 0;
+			}
+		}
 
 		/* process the submission queues*/
-		sqid = 1;
-		for_each_set_bit_from(sqid, vctrl->vsq_en, MAX_VIRTUAL_QUEUES)
-			for (i = 0 ; i < ctx->arb_burst ; i++)
-				if (!nvme_mdev_io_process_sq(ctx, sqid))
-					break;
+		for (sqid = 1; sqid < MAX_VIRTUAL_QUEUES; sqid++) {
+			struct nvme_vsq *vsq =
+				rcu_dereference(vctrl->vsqs[sqid]);
+			if (vsq)
+				nvme_mdev_io_process_sq_burst(ctx, vsq);
+		}
 
 		/* process the completions from the guest*/
-		cqid = 1;
-		for_each_set_bit_from(cqid, vctrl->vcq_en, MAX_VIRTUAL_QUEUES)
-			nvme_mdev_io_process_cq(ctx, cqid);
+		for (cqid = 1; cqid < MAX_VIRTUAL_QUEUES; cqid++) {
+			struct nvme_vcq *vcq =
+				rcu_dereference(vctrl->vcqs[cqid]);
+			if (vcq)
+				nvme_mdev_io_process_cq(ctx, vcq);
+		}
+		rcu_read_unlock();
 
 		/* process the completions from the hardware*/
-		for (i = 0 ; i < hsqcnt ; i++)
-			if (nvme_mdev_io_process_hwq(ctx, hsqs[i]) > 0)
-				ctx->last_io_t = vctrl->now;
+		for (i = 0; i < binding->hsqcnt; i++)
+			if (nvme_mdev_io_process_hwq(ctx, binding->hsqs[i]) > 0)
+				binding->last_io_t = vctrl->now;
+
+		/* process the completions from the notifiers */
+		for (i = 0; i < MAX_VIRTUAL_QUEUES; i++)
+			if (nvme_mdev_notifyfd_process_ncq(ctx->vctrl, i) > 0)
+				binding->last_io_t = vctrl->now;
 
 		/* Check if we need to stop polling*/
-		if (ctx->idle_timeout_ms) {
-			if (timeout(ctx->last_io_t,
-				    vctrl->now, ctx->idle_timeout_ms)) {
-				idle = true;
-				break;
+		if (binding->idle_timeout_ms) {
+			if (timeout(binding->last_io_t, vctrl->now,
+				    binding->idle_timeout_ms)) {
+				binding->idle = true;
+				return 0;
 			}
 		}
+
 		cond_resched();
 	}
 
-	/* Drain the host IO */
-	for (;;) {
+	return -EAGAIN;
+}
+
+int nvme_mdev_io_end_poll(struct io_ctx *ctx)
+{
+	struct nvme_mdev_vctrl *vctrl = ctx->vctrl;
+	struct nvme_mdev_vctrl_iothread_binding *binding = vctrl->binding;
+
+	u16 i, cqid, sqid;
+	int ret = -EAGAIN;
+	unsigned int iter;
+
+	BUG_ON(!binding || binding->state == NVME_MDEV_BINDING_ZERO);
+
+	/* Drain the host/notifier IO */
+	for (iter = 0; iter < vsq_poll_loops; iter++) {
 		bool pending_io = false;
 
-		vctrl->now = ktime_get_coarse_boottime();
+		vctrl->now = ktime_get();
 
 		if (nvme_mdev_vctrl_is_dead(vctrl) || ctx->hctrl->removing) {
-			idle = false;
+			binding->idle = false;
 			break;
 		}
 
-		for (i = 0; i < hsqcnt; i++) {
-			int n = nvme_mdev_io_process_hwq(ctx, hsqs[i]);
+		for (i = 0; i < binding->hsqcnt; i++) {
+			int n = nvme_mdev_io_process_hwq(ctx, binding->hsqs[i]);
 
 			if (n != -1)
 				pending_io = true;
 			if (n > 0)
-				ctx->last_io_t = vctrl->now;
+				binding->last_io_t = vctrl->now;
+		}
+
+		for (i = 0; i < MAX_VIRTUAL_QUEUES; i++) {
+			int n = nvme_mdev_notifyfd_process_ncq(ctx->vctrl, i);
+
+			if (n != -1)
+				pending_io = true;
+			if (n > 0)
+				binding->last_io_t = vctrl->now;
 		}
 
 		if (!pending_io)
@@ -434,168 +585,103 @@ static void nvme_mdev_io_polling_loop(struct io_ctx *ctx)
 
 		cond_resched();
 
-		if (!timeout(ctx->last_io_t, vctrl->now, io_timeout_ms))
-			continue;
+		if (timeout(binding->last_io_t, vctrl->now, io_timeout_ms)) {
+			_WARN(ctx->vctrl,
+			      "IO: skipping flush - host IO timeout\n");
+			binding->idle = false;
+			ret = -ETIMEDOUT;
+			break;
+		}
+	}
 
-		_WARN(ctx->vctrl, "IO: skipping flush - host IO timeout\n");
-		idle = false;
-		break;
+	if (ret == -EAGAIN && iter == vsq_poll_loops) {
+		binding->state = NVME_MDEV_BINDING_ENDING;
+		return ret;
+	} else {
+		binding->state = NVME_MDEV_BINDING_ZERO;
 	}
 
 	/* Drain all the pending completion interrupts to the guest*/
-	cqid = 1;
-	for_each_set_bit_from(cqid, vctrl->vcq_en, MAX_VIRTUAL_QUEUES)
-		if (nvme_mdev_vcq_flush(vctrl, cqid))
-			idle = false;
+	rcu_read_lock();
+	for (cqid = 1; cqid < MAX_VIRTUAL_QUEUES; cqid++) {
+		struct nvme_vcq *vcq = rcu_dereference(vctrl->vcqs[cqid]);
+		if (vcq && nvme_mdev_vcq_flush(vctrl, vcq))
+			binding->idle = false;
+	}
+	rcu_read_unlock();
 
 	/* Park IO thread if IO is truly idle*/
-	if (idle) {
+	if (binding->idle) {
 		/* don't bother going idle if someone holds the vctrl
 		 * lock. It might try to park us, and thus
 		 * cause a deadlock
 		 */
 		if (!mutex_trylock(&vctrl->lock))
-			return;
-
-		sqid = 1;
-		for_each_set_bit_from(sqid, vctrl->vsq_en, MAX_VIRTUAL_QUEUES)
-			if (!nvme_mdev_vsq_suspend_io(vctrl, sqid)) {
-				idle = false;
+			return ret;
+
+		for (sqid = 1; sqid < MAX_VIRTUAL_QUEUES; sqid++) {
+			struct nvme_vsq *vsq = rcu_dereference_protected(
+				vctrl->vsqs[sqid],
+				lockdep_is_held(&vctrl->lock));
+			if (vsq && !__nvme_mdev_vsq_suspend_io(vctrl, vsq)) {
+				binding->idle = false;
 				break;
 			}
+		}
 
-		if (idle) {
+		if (binding->idle) {
 			_DBG(ctx->vctrl, "IO: self-parking\n");
 			vctrl->io_idle = true;
-			nvme_mdev_io_pause(vctrl);
+			__nvme_mdev_io_pause(vctrl);
 		}
 
+		ret = vctrl->io_idle ? 0 : -EAGAIN;
+
 		mutex_unlock(&vctrl->lock);
 	}
 
 	/* Admin poll for cases when shadow doorbell is not supported */
 	if (!vctrl->mmio.shadow_db_supported) {
 		if (mutex_trylock(&vctrl->lock)) {
-			nvme_mdev_vcq_process(vctrl, 0, false);
-			nvme_mdev_adm_process_sq(ctx->vctrl);
-			ctx->last_admin_poll_time = vctrl->now;
+			struct nvme_vcq *adm_cq =
+				rcu_dereference_protected(vctrl->vcqs[0], 1);
+			BUG_ON(!adm_cq);
+			nvme_mdev_vcq_process(vctrl, adm_cq, false);
+			__nvme_mdev_adm_process_sq(ctx->vctrl);
+			binding->last_admin_poll_time = vctrl->now;
 			mutex_unlock(&ctx->vctrl->lock);
 		}
 	}
-}
-
-/* the main IO thread */
-static int nvme_mdev_io_polling_thread(void *data)
-{
-	struct io_ctx ctx;
-
-	if (kthread_should_stop())
-		return 0;
-
-	memset(&ctx, 0, sizeof(struct io_ctx));
-	ctx.vctrl = (struct nvme_mdev_vctrl *)data;
-	ctx.hctrl = ctx.vctrl->hctrl;
-	nvme_mdev_udata_iter_setup(&ctx.vctrl->viommu, &ctx.udatait);
-
-	_DBG(ctx.vctrl, "IO: iothread started\n");
-
-	for (;;) {
-		if (kthread_should_park()) {
-			_DBG(ctx.vctrl, "IO: iothread parked\n");
-			kthread_parkme();
-		}
-
-		if (kthread_should_stop())
-			break;
-
-		nvme_mdev_io_polling_loop(&ctx);
-	}
 
-	_DBG(ctx.vctrl, "IO: iothread stopped\n");
-	return 0;
+	return ret;
 }
 
 /* Kick the IO thread into running state*/
-void nvme_mdev_io_resume(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_io_resume(struct nvme_mdev_vctrl *vctrl)
 {
 	lockdep_assert_held(&vctrl->lock);
 
-	if (!vctrl->iothread || !vctrl->iothread_parked)
-		return;
-	if (vctrl->io_idle || vctrl->vctrl_paused)
-		return;
+	BUG_ON(!vctrl->binding || !vctrl->binding->it);
+	WRITE_ONCE(vctrl->binding->poll_enabled, true);
 
-	vctrl->iothread_parked = false;
-	/* has memory barrier*/
-	kthread_unpark(vctrl->iothread);
+	kthread_unpark(vctrl->binding->it->task);
 }
 
 /* Pause the IO thread */
-void nvme_mdev_io_pause(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_io_pause(struct nvme_mdev_vctrl *vctrl)
 {
 	lockdep_assert_held(&vctrl->lock);
 
-	if (!vctrl->iothread || vctrl->iothread_parked)
-		return;
-
-	vctrl->iothread_parked = true;
-	kthread_park(vctrl->iothread);
-}
-
-/* setup the main IO thread */
-int nvme_mdev_io_create(struct nvme_mdev_vctrl *vctrl, unsigned int cpu)
-{
-	/*TODOLATER: IO: Better thread name*/
-	char name[TASK_COMM_LEN];
-
-	_DBG(vctrl, "IO: creating the polling iothread\n");
-
-	if (WARN_ON(vctrl->iothread))
-		return -EINVAL;
-
-	snprintf(name, sizeof(name), "nvme%d_poll_io", vctrl->hctrl->id);
-
-	vctrl->iothread_cpu = cpu;
-	vctrl->iothread_parked = false;
-	vctrl->io_idle = true;
-
-	vctrl->iothread = kthread_create_on_node(nvme_mdev_io_polling_thread,
-						 vctrl,
-						 vctrl->hctrl->node,
-						 name);
-	if (IS_ERR(vctrl->iothread)) {
-		vctrl->iothread = NULL;
-		return PTR_ERR(vctrl->iothread);
-	}
-
-	kthread_bind(vctrl->iothread, cpu);
-
-	if (vctrl->io_idle) {
-		vctrl->iothread_parked = true;
-		kthread_park(vctrl->iothread);
-		return 0;
+	if (vctrl->binding) {
+		bool old = xchg(&vctrl->binding->poll_enabled, false);
+		if (old && vctrl->binding->it->task != current) {
+			kthread_unpark(vctrl->binding->it->task);
+			wait_event(vctrl->quiescent_wqh, true);
+		}
 	}
-
-	wake_up_process(vctrl->iothread);
-	return 0;
-}
-
-/* End the  main IO thread */
-void nvme_mdev_io_free(struct nvme_mdev_vctrl *vctrl)
-{
-	int ret;
-
-	_DBG(vctrl, "IO: destroying the polling iothread\n");
-
-	lockdep_assert_held(&vctrl->lock);
-	nvme_mdev_io_pause(vctrl);
-	ret = kthread_stop(vctrl->iothread);
-	WARN_ON(ret);
-	vctrl->iothread = NULL;
 }
 
-void nvme_mdev_assert_io_not_running(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_assert_io_not_running(struct nvme_mdev_vctrl *vctrl)
 {
-	if (WARN_ON(vctrl->iothread && !vctrl->iothread_parked))
-		nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 }
diff --git a/drivers/nvme/mdev/iothreads.c b/drivers/nvme/mdev/iothreads.c
new file mode 100644
index 000000000000..2702d3cbf17d
--- /dev/null
+++ b/drivers/nvme/mdev/iothreads.c
@@ -0,0 +1,163 @@
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include "priv.h"
+#include "iothreads.h"
+
+unsigned int num_iothreads = MAX_IOTHREADS;
+module_param(num_iothreads, uint, 0444);
+MODULE_PARM_DESC(num_iothreads, "Number of assignable IO threads");
+
+struct nvme_mdev_iothread iothreads[MAX_IOTHREADS];
+
+static int nvme_mdev_io_thread(void *data)
+{
+	struct nvme_mdev_iothread *it = data;
+
+	while (!kthread_should_stop()) {
+		struct nvme_mdev_vctrl_iothread_binding *binding, *tmp;
+		int active = 0;
+
+		mutex_lock(&it->bindings_lock);
+		list_for_each_entry_safe (binding, tmp, &it->bindings, link) {
+			switch (binding->state) {
+			case NVME_MDEV_BINDING_ZERO:
+				if (!binding->poll_enabled)
+					continue;
+				nvme_mdev_io_begin_poll(binding);
+				fallthrough;
+			case NVME_MDEV_BINDING_BEGUN:
+				if (nvme_mdev_io_polling_loop(&binding->ctx)) {
+					active++;
+					break;
+				}
+				fallthrough;
+			case NVME_MDEV_BINDING_ENDING:
+				if (nvme_mdev_io_end_poll(&binding->ctx))
+					active++;
+				if (binding->state == NVME_MDEV_BINDING_ZERO)
+					wake_up_all(
+						&binding->vctrl->quiescent_wqh);
+				break;
+			}
+
+			cond_resched();
+		}
+		mutex_unlock(&it->bindings_lock);
+
+		if (!active) {
+			pr_debug("iothread %u parking", it->idx);
+			kthread_park(current);
+			kthread_parkme();
+		}
+	}
+
+	mutex_lock(&it->bindings_lock);
+	WARN_ON(!list_empty(&it->bindings));
+	mutex_unlock(&it->bindings_lock);
+
+	return 0;
+}
+
+int nvme_mdev_io_init(void)
+{
+	unsigned int i;
+
+	memset(iothreads, 0, MAX_IOTHREADS * sizeof(struct nvme_mdev_iothread));
+
+	num_iothreads = clamp(num_iothreads, 1u, MAX_IOTHREADS);
+	for (i = 0; i < num_iothreads; i++) {
+		iothreads[i].idx = i;
+		INIT_LIST_HEAD(&iothreads[i].bindings);
+		mutex_init(&iothreads[i].bindings_lock);
+		iothreads[i].task =
+			kthread_create(nvme_mdev_io_thread, &iothreads[i],
+				       "nvme_mdev_poll%u", i);
+		if (IS_ERR(iothreads[i].task)) {
+			unsigned int j;
+			for (j = 0; j < i; j++)
+				kthread_stop(iothreads[j].task);
+			return PTR_ERR(xchg(&iothreads[i].task, NULL));
+		}
+		kthread_bind(iothreads[i].task, i);
+		wake_up_process(iothreads[i].task);
+	}
+
+	return 0;
+}
+
+void nvme_mdev_io_exit(void)
+{
+	unsigned int i;
+	for (i = 0; i < num_iothreads; i++)
+		kthread_stop(iothreads[i].task);
+}
+
+int nvme_mdev_io_bind(struct nvme_mdev_vctrl *vctrl,
+		      struct nvme_mdev_iothread *it, bool poll_enabled)
+{
+	struct nvme_mdev_vctrl_iothread_binding *binding;
+	int err;
+
+	mutex_lock(&vctrl->lock);
+
+	if (vctrl->binding) {
+		err = -EEXIST;
+		goto fail;
+	}
+
+	binding = kcalloc(1, sizeof(*binding), GFP_KERNEL);
+	if (!binding) {
+		err = -ENOMEM;
+		goto fail;
+	}
+
+	binding->it = it;
+	binding->vctrl = vctrl;
+	binding->poll_enabled = poll_enabled;
+	vctrl->binding = binding;
+
+	mutex_unlock(&vctrl->lock);
+
+	mutex_lock(&it->bindings_lock);
+	list_add_tail(&binding->link, &it->bindings);
+	mutex_unlock(&it->bindings_lock);
+
+	return 0;
+
+fail:
+	mutex_unlock(&vctrl->lock);
+
+	return err;
+}
+
+void nvme_mdev_io_unbind_all(struct nvme_mdev_vctrl *vctrl)
+{
+	unsigned int i;
+	int succeeded = 0;
+
+	for (i = 0; i < num_iothreads; i++) {
+		struct nvme_mdev_iothread *it = &iothreads[i];
+		struct nvme_mdev_vctrl_iothread_binding *binding, *tmp;
+
+		mutex_lock(&vctrl->lock);
+		__nvme_mdev_io_pause(vctrl);
+		mutex_unlock(&vctrl->lock);
+
+		mutex_lock(&it->bindings_lock);
+		list_for_each_entry_safe (binding, tmp, &it->bindings, link) {
+			if (binding->vctrl == vctrl) {
+				list_del(&binding->link);
+				succeeded++;
+			}
+		}
+		mutex_unlock(&it->bindings_lock);
+
+		BUG_ON(succeeded > 1);
+		if (succeeded) {
+			mutex_lock(&vctrl->lock);
+			vctrl->binding = NULL;
+			kfree(vctrl->binding);
+			mutex_unlock(&vctrl->lock);
+		}
+	}
+}
diff --git a/drivers/nvme/mdev/iothreads.h b/drivers/nvme/mdev/iothreads.h
new file mode 100644
index 000000000000..5e4e566ba9ee
--- /dev/null
+++ b/drivers/nvme/mdev/iothreads.h
@@ -0,0 +1,57 @@
+#ifndef _MDEV_NVME_IOTHREADS_H
+#define _MDEV_NVME_IOTHREADS_H
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/wait.h>
+
+#define MAX_IOTHREADS 4u
+
+extern unsigned int num_iothreads;
+
+struct nvme_mdev_vctrl;
+
+struct nvme_mdev_iothread {
+	/* informational only */
+	unsigned int idx;
+	/* nvme_mdev_vctrl_iothread_binding->link */
+	struct list_head bindings;
+	struct mutex bindings_lock;
+	struct task_struct *task;
+};
+
+extern struct nvme_mdev_iothread iothreads[MAX_IOTHREADS];
+
+enum nvme_mdev_binding_state {
+	NVME_MDEV_BINDING_ZERO,
+	NVME_MDEV_BINDING_BEGUN,
+	NVME_MDEV_BINDING_ENDING
+};
+
+struct nvme_mdev_vctrl_iothread_binding {
+	struct list_head link;
+	struct nvme_mdev_iothread *it;
+	struct nvme_mdev_vctrl *vctrl;
+	bool poll_enabled;
+
+	/* for temporary use per poll cycle only */
+	enum nvme_mdev_binding_state state;
+	u16 hsqs[MAX_HOST_QUEUES];
+	u16 hsqcnt;
+	/* for calculation of idleness */
+	bool idle;
+	unsigned int idle_timeout_ms;
+	ktime_t last_io_t;
+	ktime_t last_admin_poll_time;
+
+	struct io_ctx ctx;
+};
+
+int nvme_mdev_io_init(void);
+void nvme_mdev_io_exit(void);
+int nvme_mdev_io_bind(struct nvme_mdev_vctrl *vctrl,
+		      struct nvme_mdev_iothread *it, bool poll_enabled);
+void nvme_mdev_io_unbind_all(struct nvme_mdev_vctrl *vctrl);
+
+#endif
diff --git a/drivers/nvme/mdev/irq.c b/drivers/nvme/mdev/irq.c
index b6010a69b584..8223a753a0f2 100644
--- a/drivers/nvme/mdev/irq.c
+++ b/drivers/nvme/mdev/irq.c
@@ -34,9 +34,9 @@ static int __nvme_mdev_irqs_enable(struct nvme_mdev_vctrl *vctrl,
 	else
 		WARN_ON(1);
 
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 	vctrl->irqs.mode = mode;
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_io_resume(vctrl);
 	return 0;
 }
 
@@ -69,7 +69,7 @@ static void __nvme_mdev_irqs_disable(struct nvme_mdev_vctrl *vctrl,
 	else
 		WARN_ON(1);
 
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 
 	for (i = 0; i < MAX_VIRTUAL_IRQS; i++) {
 		struct nvme_mdev_user_irq *vec = &vctrl->irqs.vecs[i];
@@ -82,7 +82,7 @@ static void __nvme_mdev_irqs_disable(struct nvme_mdev_vctrl *vctrl,
 		vec->irq_time = 0;
 	}
 	vctrl->irqs.mode = NVME_MDEV_IMODE_NONE;
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_io_resume(vctrl);
 }
 
 void nvme_mdev_irqs_disable(struct nvme_mdev_vctrl *vctrl,
@@ -94,13 +94,13 @@ void nvme_mdev_irqs_disable(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Set eventfd triggers for INTx or MSIx interrupts */
-int nvme_mdev_irqs_set_triggers(struct nvme_mdev_vctrl *vctrl,
-				int start, int count, int32_t *fds)
+int nvme_mdev_irqs_set_triggers(struct nvme_mdev_vctrl *vctrl, int start,
+				int count, int32_t *fds)
 {
 	unsigned int i;
 
 	mutex_lock(&vctrl->lock);
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 
 	for (i = 0; i < count; i++) {
 		int irqindex = start + i;
@@ -121,7 +121,7 @@ int nvme_mdev_irqs_set_triggers(struct nvme_mdev_vctrl *vctrl,
 
 		irq->trigger = trigger;
 	}
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_io_resume(vctrl);
 	mutex_unlock(&vctrl->lock);
 	return 0;
 }
@@ -151,8 +151,7 @@ static int __nvme_mdev_irqs_set_unplug_trigger(struct nvme_mdev_vctrl *vctrl,
 	return 0;
 }
 
-int nvme_mdev_irqs_set_unplug_trigger(struct nvme_mdev_vctrl *vctrl,
-				      int32_t fd)
+int nvme_mdev_irqs_set_unplug_trigger(struct nvme_mdev_vctrl *vctrl, int32_t fd)
 {
 	int retval;
 
@@ -163,7 +162,7 @@ int nvme_mdev_irqs_set_unplug_trigger(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Reset the interrupts subsystem */
-void nvme_mdev_irqs_reset(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_irqs_reset(struct nvme_mdev_vctrl *vctrl)
 {
 	int i;
 
@@ -208,15 +207,17 @@ void nvme_mdev_irq_raise_unplug_event(struct nvme_mdev_vctrl *vctrl,
 
 	if (vctrl->irqs.request_trigger) {
 		if (!(count % 10))
-			dev_notice_ratelimited(mdev_dev(vctrl->mdev),
-					       "Relaying device request to user (#%u)\n",
-					       count);
+			dev_notice_ratelimited(
+				mdev_dev(vctrl->mdev),
+				"Relaying device request to user (#%u)\n",
+				count);
 
 		eventfd_signal(vctrl->irqs.request_trigger, 1);
 
 	} else if (count == 0) {
-		dev_notice(mdev_dev(vctrl->mdev),
-			   "No device request channel registered, blocked until released by user\n");
+		dev_notice(
+			mdev_dev(vctrl->mdev),
+			"No device request channel registered, blocked until released by user\n");
 	}
 	mutex_unlock(&vctrl->lock);
 }
@@ -230,8 +231,7 @@ void nvme_mdev_irq_raise(struct nvme_mdev_vctrl *vctrl, unsigned int index)
 }
 
 /* Unraise an interrupt */
-void nvme_mdev_irq_clear(struct nvme_mdev_vctrl *vctrl,
-			 unsigned int index)
+void nvme_mdev_irq_clear(struct nvme_mdev_vctrl *vctrl, unsigned int index)
 {
 	struct nvme_mdev_user_irq *irq = &vctrl->irqs.vecs[index];
 
@@ -240,8 +240,7 @@ void nvme_mdev_irq_clear(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Directly trigger an interrupt without affecting irq coalescing settings */
-void nvme_mdev_irq_trigger(struct nvme_mdev_vctrl *vctrl,
-			   unsigned int index)
+void nvme_mdev_irq_trigger(struct nvme_mdev_vctrl *vctrl, unsigned int index)
 {
 	struct nvme_mdev_user_irq *irq = &vctrl->irqs.vecs[index];
 
diff --git a/drivers/nvme/mdev/mdev.h b/drivers/nvme/mdev/mdev.h
index d139e090520e..11b11cc0af83 100644
--- a/drivers/nvme/mdev/mdev.h
+++ b/drivers/nvme/mdev/mdev.h
@@ -20,14 +20,14 @@ struct page_map {
 struct user_prplist {
 	/* used by user data iterator*/
 	struct page_map page;
-	unsigned int index;	/* index of current entry */
+	unsigned int index; /* index of current entry */
 };
 
 struct kernel_data {
 	/* used by kernel data iterator*/
-	void		*data;
-	unsigned int	size;
-	dma_addr_t	dma_addr;
+	void *data;
+	unsigned int size;
+	dma_addr_t dma_addr;
 };
 
 struct nvme_ext_data_iter {
@@ -40,10 +40,10 @@ struct nvme_ext_data_iter {
 	};
 
 	/* user interface */
-	u64		count;	/* number of data pages, yet to be covered */
+	u64 count; /* number of data pages, yet to be covered */
 
-	phys_addr_t	physical; /* iterator physical address value*/
-	dma_addr_t	host_iova; /* iterator dma address value*/
+	phys_addr_t physical; /* iterator physical address value*/
+	dma_addr_t host_iova; /* iterator dma address value*/
 
 	/* moves iterator to the next item */
 	int (*next)(struct nvme_ext_data_iter *data_iter);
diff --git a/drivers/nvme/mdev/mmio.c b/drivers/nvme/mdev/mmio.c
index a80962bf4a3d..1e764e20acac 100644
--- a/drivers/nvme/mdev/mmio.c
+++ b/drivers/nvme/mdev/mmio.c
@@ -18,10 +18,10 @@ static void nvme_mdev_mmio_fatal_error(struct nvme_mdev_vctrl *vctrl)
 		return;
 
 	vctrl->mmio.csts |= NVME_CSTS_CFS;
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 
 	if (vctrl->mmio.csts & NVME_CSTS_RDY)
-		nvme_mdev_vctrl_disable(vctrl);
+		__nvme_mdev_vctrl_disable(vctrl);
 }
 
 /* This sends an generic error notification to the user */
@@ -51,8 +51,8 @@ static const struct vm_operations_struct nvme_mdev_mmio_dbs_vm_ops = {
 };
 
 /* check that user db write is valid and send an error if not*/
-bool nvme_mdev_mmio_db_check(struct nvme_mdev_vctrl *vctrl,
-			     u16 qid, u16 size, u16 db)
+bool nvme_mdev_mmio_db_check(struct nvme_mdev_vctrl *vctrl, u16 qid, u16 size,
+			     u16 db)
 {
 	if (db < size)
 		return true;
@@ -70,14 +70,14 @@ bool nvme_mdev_mmio_db_check(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* handle submission queue doorbell write */
-static void nvme_mdev_mmio_db_write_sq(struct nvme_mdev_vctrl *vctrl,
-				       u32 qid, u32 val)
+static void __nvme_mdev_mmio_db_write_sq(struct nvme_mdev_vctrl *vctrl, u32 qid,
+					 u32 val)
 {
 	_DBG(vctrl, "MMIO: doorbell SQID %d, DB write %d\n", qid, val);
 
 	lockdep_assert_held(&vctrl->lock);
 	/* check if the db belongs to a valid queue */
-	if (qid >= MAX_VIRTUAL_QUEUES || !test_bit(qid, vctrl->vsq_en))
+	if (qid >= MAX_VIRTUAL_QUEUES || !rcu_access_pointer(vctrl->vsqs[qid]))
 		goto err_db;
 
 	/* emulate the shadow doorbell functionality */
@@ -90,7 +90,7 @@ static void nvme_mdev_mmio_db_write_sq(struct nvme_mdev_vctrl *vctrl,
 	if (vctrl->vctrl_paused || !vctrl->mmio.shadow_db_supported)
 		return;
 
-	qid ? nvme_mdev_io_resume(vctrl) : nvme_mdev_adm_process_sq(vctrl);
+	qid ? __nvme_mdev_io_resume(vctrl) : __nvme_mdev_adm_process_sq(vctrl);
 	return;
 err_db:
 
@@ -101,14 +101,14 @@ static void nvme_mdev_mmio_db_write_sq(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* handle doorbell write */
-static void nvme_mdev_mmio_db_write_cq(struct nvme_mdev_vctrl *vctrl,
-				       u32 qid, u32 val)
+static void __nvme_mdev_mmio_db_write_cq(struct nvme_mdev_vctrl *vctrl, u32 qid,
+					 u32 val)
 {
 	_DBG(vctrl, "MMIO: doorbell CQID %d, DB write %d\n", qid, val);
 
 	lockdep_assert_held(&vctrl->lock);
 	/* check if the db belongs to a valid queue */
-	if (qid >= MAX_VIRTUAL_QUEUES || !test_bit(qid, vctrl->vcq_en))
+	if (qid >= MAX_VIRTUAL_QUEUES || !rcu_access_pointer(vctrl->vcqs[qid]))
 		goto err_db;
 
 	/* emulate the shadow doorbell functionality */
@@ -119,23 +119,25 @@ static void nvme_mdev_mmio_db_write_cq(struct nvme_mdev_vctrl *vctrl,
 		return;
 
 	if (qid == 0) {
-		nvme_mdev_vcq_process(vctrl, 0, false);
+		struct nvme_vcq *adm_cq =
+			rcu_dereference_protected(vctrl->vcqs[0], 1);
+		BUG_ON(!adm_cq);
+		nvme_mdev_vcq_process(vctrl, adm_cq, false);
 		// if completion queue was full prior to that, we
 		// might have some admin commands pending,
 		// and this is the last chance to process them
-		nvme_mdev_adm_process_sq(vctrl);
+		__nvme_mdev_adm_process_sq(vctrl);
 	}
 	return;
 err_db:
-	_DBG(vctrl,
-	     "MMIO: inactive/invalid CQ DB write qid=%d, value=%d\n",
+	_DBG(vctrl, "MMIO: inactive/invalid CQ DB write qid=%d, value=%d\n",
 	     qid, val);
 
 	nvme_mdev_mmio_error(vctrl, NVME_AER_ERROR_INVALID_DB_REG);
 }
 
 /* This is called when user enables the controller */
-static void nvme_mdev_mmio_cntrl_enable(struct nvme_mdev_vctrl *vctrl)
+static void __nvme_mdev_mmio_cntrl_enable(struct nvme_mdev_vctrl *vctrl)
 {
 	u64 acq, asq;
 
@@ -161,7 +163,7 @@ static void nvme_mdev_mmio_cntrl_enable(struct nvme_mdev_vctrl *vctrl)
 	acq = vctrl->mmio.acql | ((u64)vctrl->mmio.acqh << 32);
 	asq = vctrl->mmio.asql | ((u64)vctrl->mmio.asqh << 32);
 
-	if (!nvme_mdev_vctrl_enable(vctrl, acq, asq, vctrl->mmio.aqa))
+	if (!__nvme_mdev_vctrl_enable(vctrl, acq, asq, vctrl->mmio.aqa))
 		goto error;
 
 	/* Success! */
@@ -175,7 +177,7 @@ static void nvme_mdev_mmio_cntrl_enable(struct nvme_mdev_vctrl *vctrl)
 /* This is called when user sends a notification that controller is
  * about to be disabled
  */
-static void nvme_mdev_mmio_cntrl_shutdown(struct nvme_mdev_vctrl *vctrl)
+static void __nvme_mdev_mmio_cntrl_shutdown(struct nvme_mdev_vctrl *vctrl)
 {
 	lockdep_assert_held(&vctrl->lock);
 
@@ -190,19 +192,18 @@ static void nvme_mdev_mmio_cntrl_shutdown(struct nvme_mdev_vctrl *vctrl)
 	/* not enabled */
 	if (!(vctrl->mmio.csts & NVME_CSTS_RDY)) {
 		_DBG(vctrl, "MMIO: shutdown notification with CSTS.RDY==0\n");
-		nvme_mdev_assert_io_not_running(vctrl);
+		__nvme_mdev_assert_io_not_running(vctrl);
 		return;
 	}
 
-	nvme_mdev_io_pause(vctrl);
-	nvme_mdev_vctrl_disable(vctrl);
+	__nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_vctrl_disable(vctrl);
 	vctrl->mmio.csts |= NVME_CSTS_SHST_CMPLT;
 }
 
 /* MMIO BAR read/write */
-static int nvme_mdev_mmio_bar_access(struct nvme_mdev_vctrl *vctrl,
-				     u16 offset, char *buf,
-				     u32 count, bool is_write)
+static int nvme_mdev_mmio_bar_access(struct nvme_mdev_vctrl *vctrl, u16 offset,
+				     char *buf, u32 count, bool is_write)
 {
 	u32 val, oldval;
 
@@ -275,7 +276,7 @@ static int nvme_mdev_mmio_bar_access(struct nvme_mdev_vctrl *vctrl,
 
 		if ((vctrl->mmio.cc & NVME_CC_SHN_MASK) != NVME_CC_SHN_NONE) {
 			_DBG(vctrl, "MMIO: CC.SHN != 0 - shutdown\n");
-			nvme_mdev_mmio_cntrl_shutdown(vctrl);
+			__nvme_mdev_mmio_cntrl_shutdown(vctrl);
 		}
 
 		/* change in controller enabled state */
@@ -284,7 +285,7 @@ static int nvme_mdev_mmio_bar_access(struct nvme_mdev_vctrl *vctrl,
 
 		if (vctrl->mmio.cc & NVME_CC_ENABLE) {
 			_DBG(vctrl, "MMIO: CC.EN<=1 - enable the controller\n");
-			nvme_mdev_mmio_cntrl_enable(vctrl);
+			__nvme_mdev_mmio_cntrl_enable(vctrl);
 		} else {
 			_DBG(vctrl, "MMIO: CC.EN<=0 - reset controller\n");
 			__nvme_mdev_vctrl_reset(vctrl, false);
@@ -357,7 +358,7 @@ static int nvme_mdev_mmio_bar_access(struct nvme_mdev_vctrl *vctrl,
 		store_le32(buf, 0);
 		break;
 
-	case NVME_REG_DBS ... (NVME_REG_DBS + DB_AREA_SIZE - 1): {
+	case NVME_REG_DBS ...(NVME_REG_DBS + DB_AREA_SIZE - 1): {
 		/* completion and submission doorbells */
 		u16 db_offset = offset - NVME_REG_DBS;
 		u16 index = db_offset >> (DB_STRIDE_SHIFT + 2);
@@ -373,8 +374,8 @@ static int nvme_mdev_mmio_bar_access(struct nvme_mdev_vctrl *vctrl,
 		if (nvme_mdev_vctrl_is_dead(vctrl))
 			goto drop;
 
-		sq ? nvme_mdev_mmio_db_write_sq(vctrl, qid, val) :
-		     nvme_mdev_mmio_db_write_cq(vctrl, qid, val);
+		sq ? __nvme_mdev_mmio_db_write_sq(vctrl, qid, val) :
+			   __nvme_mdev_mmio_db_write_cq(vctrl, qid, val);
 		break;
 	}
 	default:
@@ -395,8 +396,8 @@ int nvme_mdev_mmio_create(struct nvme_mdev_vctrl *vctrl)
 	int ret;
 
 	/* BAR0 */
-	nvme_mdev_pci_setup_bar(vctrl, PCI_BASE_ADDRESS_0,
-				MMIO_BAR_SIZE, nvme_mdev_mmio_bar_access);
+	nvme_mdev_pci_setup_bar(vctrl, PCI_BASE_ADDRESS_0, MMIO_BAR_SIZE,
+				nvme_mdev_mmio_bar_access);
 
 	/* Spec allows for maximum depth of 0x10000, but we limit
 	 * it to 1 less to avoid various overflows
@@ -436,8 +437,8 @@ int nvme_mdev_mmio_create(struct nvme_mdev_vctrl *vctrl)
 		((u64)(PAGE_SHIFT - 12) << 52);
 
 	/* Create the (regular) doorbell buffers */
-	vctrl->mmio.dbs_page = alloc_pages_node(vctrl->hctrl->node,
-						__GFP_ZERO, 0);
+	vctrl->mmio.dbs_page =
+		alloc_pages_node(vctrl->hctrl->node, __GFP_ZERO, 0);
 
 	ret = -ENOMEM;
 
@@ -448,8 +449,8 @@ int nvme_mdev_mmio_create(struct nvme_mdev_vctrl *vctrl)
 	if (!vctrl->mmio.db_page_kmap)
 		goto error1;
 
-	vctrl->mmio.fake_eidx_page = alloc_pages_node(vctrl->hctrl->node,
-						      __GFP_ZERO, 0);
+	vctrl->mmio.fake_eidx_page =
+		alloc_pages_node(vctrl->hctrl->node, __GFP_ZERO, 0);
 	if (!vctrl->mmio.fake_eidx_page)
 		goto error2;
 
@@ -474,7 +475,7 @@ void nvme_mdev_mmio_reset(struct nvme_mdev_vctrl *vctrl, bool pci_reset)
 	vctrl->mmio.csts = 0;
 
 	if (pci_reset) {
-		vctrl->mmio.aqa  = 0;
+		vctrl->mmio.aqa = 0;
 		vctrl->mmio.asql = 0;
 		vctrl->mmio.asqh = 0;
 		vctrl->mmio.acql = 0;
@@ -496,12 +497,12 @@ void nvme_mdev_mmio_open(struct nvme_mdev_vctrl *vctrl)
 }
 
 /* Called when the virtual controller queues are enabled */
-int nvme_mdev_mmio_enable_dbs(struct nvme_mdev_vctrl *vctrl)
+int __nvme_mdev_mmio_enable_dbs(struct nvme_mdev_vctrl *vctrl)
 {
 	if (WARN_ON(vctrl->mmio.shadow_db_en))
 		return -EINVAL;
 
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
 	/* setup normal doorbells and reset them*/
 	vctrl->mmio.dbs = vctrl->mmio.db_page_kmap;
@@ -512,35 +513,34 @@ int nvme_mdev_mmio_enable_dbs(struct nvme_mdev_vctrl *vctrl)
 }
 
 /* Called when the virtual controller shadow doorbell is enabled */
-int nvme_mdev_mmio_enable_dbs_shadow(struct nvme_mdev_vctrl *vctrl,
-				     dma_addr_t sdb_iova,
-				     dma_addr_t eidx_iova)
+int __nvme_mdev_mmio_enable_dbs_shadow(struct nvme_mdev_vctrl *vctrl,
+				       dma_addr_t sdb_iova,
+				       dma_addr_t eidx_iova)
 {
 	int ret;
 
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
-	ret = nvme_mdev_viommu_create_kmap(&vctrl->viommu,
-					   sdb_iova, &vctrl->mmio.sdb_map);
+	ret = nvme_mdev_viommu_create_kmap(&vctrl->viommu, sdb_iova,
+					   &vctrl->mmio.sdb_map, true);
 	if (ret)
 		return ret;
 
-	ret = nvme_mdev_viommu_create_kmap(&vctrl->viommu,
-					   eidx_iova, &vctrl->mmio.seidx_map);
+	ret = nvme_mdev_viommu_create_kmap(&vctrl->viommu, eidx_iova,
+					   &vctrl->mmio.seidx_map, true);
 	if (ret) {
-		nvme_mdev_viommu_free_kmap(&vctrl->viommu,
-					   &vctrl->mmio.sdb_map);
+		nvme_mdev_viommu_free_kmap(&vctrl->viommu, &vctrl->mmio.sdb_map,
+					   true);
 		return ret;
 	}
 
 	vctrl->mmio.dbs = vctrl->mmio.sdb_map.kmap;
 	vctrl->mmio.eidxs = vctrl->mmio.seidx_map.kmap;
 
-	memcpy((void *)vctrl->mmio.dbs,
-	       vctrl->mmio.db_page_kmap, DB_AREA_SIZE);
+	memcpy((void *)vctrl->mmio.dbs, vctrl->mmio.db_page_kmap, DB_AREA_SIZE);
 
-	memcpy((void *)vctrl->mmio.eidxs,
-	       vctrl->mmio.db_page_kmap, DB_AREA_SIZE);
+	memcpy((void *)vctrl->mmio.eidxs, vctrl->mmio.db_page_kmap,
+	       DB_AREA_SIZE);
 
 	vctrl->mmio.shadow_db_en = true;
 	return 0;
@@ -549,27 +549,30 @@ int nvme_mdev_mmio_enable_dbs_shadow(struct nvme_mdev_vctrl *vctrl,
 /* Called on guest mapping update to
  * verify that our mappings are still intact
  */
-void nvme_mdev_mmio_viommu_update(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_mmio_viommu_update(struct nvme_mdev_vctrl *vctrl)
 {
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 	if (!vctrl->mmio.shadow_db_en)
 		return;
 
-	nvme_mdev_viommu_update_kmap(&vctrl->viommu, &vctrl->mmio.sdb_map);
-	nvme_mdev_viommu_update_kmap(&vctrl->viommu, &vctrl->mmio.seidx_map);
+	nvme_mdev_viommu_update_kmap(&vctrl->viommu, &vctrl->mmio.sdb_map,
+				     true);
+	nvme_mdev_viommu_update_kmap(&vctrl->viommu, &vctrl->mmio.seidx_map,
+				     true);
 
 	vctrl->mmio.dbs = vctrl->mmio.sdb_map.kmap;
 	vctrl->mmio.eidxs = vctrl->mmio.seidx_map.kmap;
 }
 
 /* Disable the doorbells */
-void nvme_mdev_mmio_disable_dbs(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_mmio_disable_dbs(struct nvme_mdev_vctrl *vctrl)
 {
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
 	/* Free the shadow doorbells */
-	nvme_mdev_viommu_free_kmap(&vctrl->viommu, &vctrl->mmio.sdb_map);
-	nvme_mdev_viommu_free_kmap(&vctrl->viommu, &vctrl->mmio.seidx_map);
+	nvme_mdev_viommu_free_kmap(&vctrl->viommu, &vctrl->mmio.sdb_map, true);
+	nvme_mdev_viommu_free_kmap(&vctrl->viommu, &vctrl->mmio.seidx_map,
+				   true);
 
 	/* Clear the doorbells */
 	vctrl->mmio.dbs = NULL;
@@ -580,7 +583,7 @@ void nvme_mdev_mmio_disable_dbs(struct nvme_mdev_vctrl *vctrl)
 /* Called when the virtual controller is about to be freed */
 void nvme_mdev_mmio_free(struct nvme_mdev_vctrl *vctrl)
 {
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 	kunmap(vctrl->mmio.dbs_page);
 	put_page(vctrl->mmio.dbs_page);
 	kunmap(vctrl->mmio.fake_eidx_page);
diff --git a/drivers/nvme/mdev/notifyfd.c b/drivers/nvme/mdev/notifyfd.c
new file mode 100644
index 000000000000..07c071ad75ee
--- /dev/null
+++ b/drivers/nvme/mdev/notifyfd.c
@@ -0,0 +1,429 @@
+#include <linux/file.h>
+#include <linux/nvme_mdev.h>
+#include <linux/anon_inodes.h>
+#include <linux/lockdep.h>
+#include <linux/bpf_nvme_mdev.h>
+#include "notifyfd.h"
+#include "bpf_sm.h"
+
+#define NOTIFYFD_BURST 16
+
+static bool __notifyfd_get(struct nvme_mdev_notifier *notifier)
+{
+	bool ret = refcount_inc_not_zero(&notifier->mapcnt);
+	return ret;
+}
+
+static bool notifyfd_get(struct nvme_mdev_notifier *notifier)
+{
+	bool ret;
+	mutex_lock(&notifier->map_lock);
+	ret = __notifyfd_get(notifier);
+	mutex_unlock(&notifier->map_lock);
+	return ret;
+}
+
+static bool notifyfd_put(struct nvme_mdev_notifier *notifier)
+{
+	bool ret;
+	mutex_lock(&notifier->map_lock);
+	ret = refcount_dec_and_test(&notifier->mapcnt);
+	mutex_unlock(&notifier->map_lock);
+	if (ret)
+		vfree(notifier);
+	return ret;
+}
+
+static int notifyfd_release(struct inode *inode, struct file *filp)
+{
+	struct nvme_mdev_notifier *notifier = filp->private_data, *old;
+	struct nvme_mdev_vctrl *vctrl = notifier->vctrl;
+
+	mutex_lock(&vctrl->lock);
+
+	old = rcu_replace_pointer(vctrl->notifier[notifier->sqid], NULL, 1);
+	BUG_ON(old != notifier);
+
+	if (atomic_dec_and_test(&vctrl->notifier_cnt)) {
+		_INFO(vctrl, "release: %d no more notifiers\n",
+		      atomic_read(&vctrl->notifier_cnt));
+		wake_up_var(&vctrl->notifier_cnt);
+	}
+	mutex_unlock(&vctrl->lock);
+
+	synchronize_rcu();
+	notifyfd_put(notifier);
+
+	return 0;
+}
+
+static ssize_t notifyfd_read(struct file *filp, char __user *buf, size_t size,
+			     loff_t *ppos)
+{
+	struct nvme_mdev_notifier *notifier = filp->private_data;
+	struct nvme_command __user *ucbuf = (struct nvme_command __user *)buf;
+	int head, tail;
+	ssize_t count = 0;
+	while (count < size / sizeof(*ucbuf)) {
+		head = smp_load_acquire(&notifier->sqh);
+		tail = notifier->sqt & (NMNTFY_SQD_COUNT - 1);
+		if (CIRC_CNT(head, tail, NMNTFY_SQD_COUNT) >= 1) {
+			if (copy_to_user(&ucbuf[count], &notifier->sqd[tail],
+					 sizeof(*ucbuf)))
+				return -EFAULT;
+			count++;
+			smp_store_release(&notifier->sqt,
+					  (tail + 1) & (NMNTFY_SQD_COUNT - 1));
+		} else {
+			break;
+		}
+	}
+	return count * sizeof(*ucbuf);
+}
+
+static __poll_t notifyfd_poll(struct file *filp, struct poll_table_struct *wait)
+{
+	struct nvme_mdev_notifier *notifier = filp->private_data;
+	int head, tail;
+	poll_wait(filp, &notifier->wqh, wait);
+	head = smp_load_acquire(&notifier->sqh);
+	tail = notifier->sqt & (NMNTFY_SQD_COUNT - 1);
+	if (CIRC_CNT(head, tail, NMNTFY_SQD_COUNT) >= 1)
+		return EPOLLIN | EPOLLRDNORM;
+	else
+		return 0;
+}
+
+static void notifyfd_runhook(struct nvme_vsq *vsq, u16 ucid, u16 ustatus)
+{
+	struct io_ctx ctx = {
+		/* ctx->in should be set up by nvme_mdev_io_act */
+		.hctrl = vsq->vctrl->hctrl,
+		.vctrl = vsq->vctrl,
+		.ns = NULL,
+		.kdatait = NULL,
+	};
+	bool needs_react;
+	nvme_mdev_udata_iter_setup(&vsq->vctrl->viommu, &ctx.udatait);
+
+	needs_react = nvme_mdev_bpfsm_runhook(&ctx.bctx, vsq, ucid,
+					      NMBPF_HOOK_NFD_WRITE, ustatus);
+	if (!needs_react)
+		ctx.bctx.iostate = nvme_mdev_bpfsm_update(
+			vsq, ucid, ustatus, NMBPF_WILL_COMPLETE_FD);
+	nvme_mdev_io_act(&ctx, vsq, ucid, needs_react);
+}
+
+static void notifyfd_vmopen(struct vm_area_struct *vma)
+{
+	struct nvme_mdev_notifier *notifier = vma->vm_private_data;
+	notifyfd_get(notifier);
+}
+
+static void notifyfd_vmclose(struct vm_area_struct *vma)
+{
+	struct nvme_mdev_notifier *notifier = vma->vm_private_data;
+	notifyfd_put(notifier);
+}
+
+static const struct vm_operations_struct notifyfd_vmops = {
+	.open = notifyfd_vmopen,
+	.close = notifyfd_vmclose,
+};
+
+static bool notifyfd_mmap_clamp(struct vm_area_struct *vma,
+				unsigned long max_pages,
+				unsigned long allowed_flags)
+{
+	unsigned long size = vma->vm_end - vma->vm_start;
+	unsigned long forbidden_flags = VM_ACCESS_FLAGS ^ allowed_flags;
+	if (!IS_ALIGNED(size, PAGE_SIZE))
+		return false;
+	if (size < PAGE_SIZE || size > (max_pages << PAGE_SHIFT))
+		return false;
+	if (vma->vm_flags & forbidden_flags)
+		return false;
+	/* VM_MAYXXX */
+	vma->vm_flags &= ~(forbidden_flags << 4);
+	return true;
+}
+
+static int notifyfd_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct nvme_mdev_notifier *notifier = filp->private_data;
+	unsigned long size = vma->vm_end - vma->vm_start;
+	unsigned long off;
+	int ret;
+
+	mutex_lock(&notifier->map_lock);
+
+	if (!(vma->vm_flags & VM_SHARED)) {
+		ret = -EINVAL;
+		goto out;
+	}
+	vma->vm_private_data = notifier;
+	vma->vm_ops = &notifyfd_vmops;
+	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+
+	switch (vma->vm_pgoff) {
+	case NMNTFY_SQ_DATA_OFFSET:
+		if (!notifyfd_mmap_clamp(vma, NMNTFY_SQ_DATA_NR_PAGES,
+					 VM_READ)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		for (off = 0; off < size; off += PAGE_SIZE) {
+			u8 *pstart = ((u8 *)notifier->sqd) + off;
+			ret = vm_insert_page(vma, vma->vm_start + off,
+					     vmalloc_to_page(pstart));
+			WARN_ONCE(ret, "notifier: insert sqd failed");
+		}
+		break;
+	case NMNTFY_SQH_OFFSET:
+		if (!notifyfd_mmap_clamp(vma, 1, VM_READ)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = vm_insert_page(vma, vma->vm_start,
+				     vmalloc_to_page(&notifier->sqh));
+		WARN_ONCE(ret, "notifier: insert sqh failed");
+		break;
+	case NMNTFY_SQT_OFFSET:
+		/* allow writes to sqt (sqe consumer) */
+		if (!notifyfd_mmap_clamp(vma, 1, VM_READ | VM_WRITE)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = vm_insert_page(vma, vma->vm_start,
+				     vmalloc_to_page(&notifier->sqt));
+		WARN_ONCE(ret, "notifier: insert sqt failed");
+		break;
+	case NMNTFY_CQ_DATA_OFFSET:
+		/* allow writes to cqd (cqe producer) */
+		if (!notifyfd_mmap_clamp(vma, NMNTFY_CQ_DATA_NR_PAGES,
+					 VM_READ | VM_WRITE)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		for (off = 0; off < size; off += PAGE_SIZE) {
+			u8 *pstart = ((u8 *)notifier->cqd) + off;
+			ret = vm_insert_page(vma, vma->vm_start + off,
+					     vmalloc_to_page(pstart));
+			WARN_ONCE(ret, "notifier: insert cqd failed");
+		}
+		break;
+	case NMNTFY_CQH_OFFSET:
+		/* allow writes to cqh (cqe producer) */
+		if (!notifyfd_mmap_clamp(vma, 1, VM_READ | VM_WRITE)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = vm_insert_page(vma, vma->vm_start,
+				     vmalloc_to_page(&notifier->cqh));
+		WARN_ONCE(ret, "notifier: insert cqh failed");
+		break;
+	case NMNTFY_CQT_OFFSET:
+		if (!notifyfd_mmap_clamp(vma, 1, VM_READ)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = vm_insert_page(vma, vma->vm_start,
+				     vmalloc_to_page(&notifier->cqt));
+		WARN_ONCE(ret, "notifier: insert cqt failed");
+		break;
+	default:
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (!ret)
+		__notifyfd_get(notifier);
+
+out:
+	mutex_unlock(&notifier->map_lock);
+	return ret;
+}
+
+static int nvme_mdev_ioctl_id_vctrl(struct nvme_mdev_vctrl *vctrl,
+				    struct nvme_mdev_id_vctrl __user *arg)
+{
+	int ret = 0;
+	struct nvme_id_ctrl *id;
+
+	id = kzalloc(sizeof(struct nvme_id_ctrl), GFP_KERNEL);
+	if (!id)
+		return -ENOMEM;
+
+	mutex_lock(&vctrl->lock);
+	__nvme_mdev_adm_id_vctrl(vctrl, vctrl->hctrl, id);
+	mutex_unlock(&vctrl->lock);
+
+	if (copy_to_user(arg->data, id, sizeof(struct nvme_id_ctrl)))
+		ret = -EFAULT;
+
+	kfree(id);
+	return ret;
+}
+
+static int nvme_mdev_ioctl_id_vns(struct nvme_mdev_vctrl *vctrl,
+				  struct nvme_mdev_id_vns __user *arg)
+{
+	int ret = 0;
+	struct nvme_id_ns *idns;
+	u32 nsid;
+
+	if (copy_from_user(&nsid, &arg->nsid, sizeof(u32)))
+		return -EFAULT;
+
+	if (nsid == 0xffffffff || nsid == 0 || nsid > MAX_VIRTUAL_NAMESPACES)
+		return -EINVAL;
+
+	idns = kzalloc(sizeof(struct nvme_id_ns), GFP_KERNEL);
+	if (!idns)
+		return -ENOMEM;
+
+	mutex_lock(&vctrl->lock);
+	__nvme_mdev_adm_id_vns(vctrl, vctrl->namespaces[nsid - 1], idns);
+	mutex_unlock(&vctrl->lock);
+
+	if (copy_to_user(arg->data, idns, sizeof(struct nvme_id_ns)))
+		ret = -EFAULT;
+
+	kfree(idns);
+	return ret;
+}
+
+static long notifyfd_ioctl(struct file *filp, unsigned int cmd,
+			   unsigned long arg)
+{
+	struct nvme_mdev_notifier *notifier = filp->private_data;
+	struct nvme_mdev_vctrl *vctrl = notifier->vctrl;
+
+	switch (cmd) {
+	case NVME_MDEV_NOTIFYFD_ID_VCTRL:
+		return nvme_mdev_ioctl_id_vctrl(
+			vctrl, (struct nvme_mdev_id_vctrl __user *)arg);
+	case NVME_MDEV_NOTIFYFD_ID_VNS:
+		return nvme_mdev_ioctl_id_vns(
+			vctrl, (struct nvme_mdev_id_vns __user *)arg);
+	default:
+		return -ENOTTY;
+	}
+}
+
+static const struct file_operations notifyfd_fops = {
+	.owner = THIS_MODULE,
+	.release = notifyfd_release,
+	.read = notifyfd_read,
+	.poll = notifyfd_poll,
+	.mmap = notifyfd_mmap,
+	.unlocked_ioctl = notifyfd_ioctl,
+	.llseek = no_llseek,
+};
+
+bool nvme_mdev_notifyfd_send(struct nvme_vsq *vsq,
+			     const struct nvme_command *cmd)
+{
+	struct nvme_mdev_notifier *notifier;
+	int head, tail;
+	bool ret;
+
+	rcu_read_lock();
+	notifier = rcu_dereference(vsq->vctrl->notifier[vsq->qid]);
+	if (!notifier) {
+		rcu_read_unlock();
+		return false;
+	}
+
+	head = notifier->sqh;
+	tail = READ_ONCE(notifier->sqt) & (NMNTFY_SQD_COUNT - 1);
+	if (CIRC_SPACE(head, tail, NMNTFY_SQD_COUNT) >= 1) {
+		atomic_inc(&notifier->inflight);
+		notifier->sqd[head] = *cmd;
+		smp_store_release(&notifier->sqh,
+				  (head + 1) & (NMNTFY_SQD_COUNT - 1));
+		ret = true;
+	} else {
+		ret = false;
+	}
+
+	rcu_read_unlock();
+	return ret;
+}
+
+int nvme_mdev_notifyfd_open(struct nvme_mdev_vctrl *vctrl, u16 sqid)
+{
+	int ret;
+	struct nvme_mdev_notifier *notifier;
+
+	if (!sqid || sqid >= MAX_VIRTUAL_QUEUES)
+		return -EINVAL;
+
+	notifier = vmalloc_user(sizeof(*notifier));
+	if (!notifier)
+		return -ENOMEM;
+	notifier->vctrl = vctrl;
+	notifier->sqid = sqid;
+	mutex_init(&notifier->map_lock);
+	init_waitqueue_head(&notifier->wqh);
+	refcount_set(&notifier->mapcnt, 1);
+
+	mutex_lock(&vctrl->lock);
+
+	if (rcu_access_pointer(vctrl->notifier[sqid])) {
+		ret = -EBUSY;
+		goto fail;
+	}
+
+	ret = anon_inode_getfd("[nvme-mdev-notify]", &notifyfd_fops, notifier,
+			       O_RDWR);
+	if (ret < 0)
+		goto fail;
+
+	atomic_inc(&vctrl->notifier_cnt);
+	rcu_assign_pointer(vctrl->notifier[sqid], notifier);
+
+	mutex_unlock(&vctrl->lock);
+	return ret;
+
+fail:
+	mutex_unlock(&vctrl->lock);
+	vfree(notifier);
+	return ret;
+}
+
+int nvme_mdev_notifyfd_process_ncq(struct nvme_mdev_vctrl *vctrl, u16 sqid)
+{
+	struct nvme_vsq *vsq;
+	struct nvme_mdev_notifier *notifier;
+	int head, tail, count, i, inflight;
+
+	rcu_read_lock();
+	vsq = rcu_dereference(vctrl->vsqs[sqid]);
+	if (!vsq) {
+		rcu_read_unlock();
+		return -1;
+	}
+	notifier = rcu_dereference(vctrl->notifier[sqid]);
+	if (!notifier) {
+		rcu_read_unlock();
+		return -1;
+	}
+
+	head = smp_load_acquire(&notifier->cqh) & (NMNTFY_CQD_COUNT - 1);
+	tail = notifier->cqt;
+	count = CIRC_CNT(head, tail, NMNTFY_CQD_COUNT);
+	if (count >= 1) {
+		for (i = 0; i < count; i++) {
+			struct nmntfy_response *resp = &notifier->cqd[tail];
+			notifyfd_runhook(vsq, resp->ucid, resp->status);
+			tail = (tail + 1) & (NMNTFY_CQD_COUNT - 1);
+		}
+		smp_store_release(&notifier->cqt, tail);
+	}
+	inflight = atomic_sub_return(count, &notifier->inflight);
+	WARN_ON(inflight < 0);
+
+	rcu_read_unlock();
+	return (count == 0 && inflight == 0) ? -1 : count;
+}
diff --git a/drivers/nvme/mdev/notifyfd.h b/drivers/nvme/mdev/notifyfd.h
new file mode 100644
index 000000000000..79fa52497163
--- /dev/null
+++ b/drivers/nvme/mdev/notifyfd.h
@@ -0,0 +1,44 @@
+#ifndef _NVME_MDEV_NOTIFYFD_H
+#define _NVME_MDEV_NOTIFYFD_H
+
+#include <linux/circ_buf.h>
+#include <linux/nvme_mdev.h>
+#include "priv.h"
+
+#define NMNTFY_SQD_COUNT                                                       \
+	((NMNTFY_SQ_DATA_NR_PAGES << PAGE_SHIFT) / sizeof(struct nvme_command))
+#define NMNTFY_CQD_COUNT                                                       \
+	((NMNTFY_CQ_DATA_NR_PAGES << PAGE_SHIFT) /                             \
+	 sizeof(struct nmntfy_response))
+
+struct nvme_mdev_notifier {
+	/* mappable shared region */
+	struct nvme_command sqd[NMNTFY_SQD_COUNT] __aligned(PAGE_SIZE);
+	int sqh __aligned(PAGE_SIZE);
+	int sqt __aligned(PAGE_SIZE);
+	struct nmntfy_response cqd[NMNTFY_CQD_COUNT] __aligned(PAGE_SIZE);
+	int cqh __aligned(PAGE_SIZE);
+	int cqt __aligned(PAGE_SIZE);
+	/* private region */
+	/* map_lock must also be page-aligned to make sure tail has its own page */
+	struct mutex map_lock __aligned(PAGE_SIZE);
+	/* unlocked */
+	wait_queue_head_t wqh;
+	/* static */
+	struct nvme_mdev_vctrl *vctrl;
+	/* protected by map_lock */
+	refcount_t mapcnt;
+	/* unlocked */
+	atomic_t inflight;
+	/* static */
+	u16 sqid;
+};
+
+int nvme_mdev_notifyfd_open(struct nvme_mdev_vctrl *vctrl, u16 sqid);
+
+bool nvme_mdev_notifyfd_send(struct nvme_vsq *vsq,
+			     const struct nvme_command *cmd);
+
+int nvme_mdev_notifyfd_process_ncq(struct nvme_mdev_vctrl *vctrl, u16 sqid);
+
+#endif /* _NVME_MDEV_NOTIFYFD_H */
diff --git a/drivers/nvme/mdev/pci.c b/drivers/nvme/mdev/pci.c
index b7cdeaaf9c2e..e2a3c97d716e 100644
--- a/drivers/nvme/mdev/pci.c
+++ b/drivers/nvme/mdev/pci.c
@@ -8,25 +8,23 @@
 #include "priv.h"
 
 /* setup a 64 bit PCI bar */
-void nvme_mdev_pci_setup_bar(struct nvme_mdev_vctrl *vctrl,
-			     u8 bar,
-			     unsigned int size,
-			     region_access_fn access_fn)
+void nvme_mdev_pci_setup_bar(struct nvme_mdev_vctrl *vctrl, u8 bar,
+			     unsigned int size, region_access_fn access_fn)
 {
 	nvme_mdev_vctrl_add_region(vctrl,
 				   VFIO_PCI_BAR0_REGION_INDEX +
-				   ((bar - PCI_BASE_ADDRESS_0) >> 2),
+					   ((bar - PCI_BASE_ADDRESS_0) >> 2),
 				   size, access_fn);
 
 	store_le32(vctrl->pcicfg.wmask + bar, ~((u64)size - 1));
 	store_le32(vctrl->pcicfg.values + bar,
 		   PCI_BASE_ADDRESS_SPACE_MEMORY |
-		   PCI_BASE_ADDRESS_MEM_TYPE_64);
+			   PCI_BASE_ADDRESS_MEM_TYPE_64);
 }
 
 /* Allocate a pci capability*/
-static u8 nvme_mdev_pci_allocate_cap(struct nvme_mdev_vctrl *vctrl,
-				     u8 id, u8 size)
+static u8 nvme_mdev_pci_allocate_cap(struct nvme_mdev_vctrl *vctrl, u8 id,
+				     u8 size)
 {
 	u8 *cfg = vctrl->pcicfg.values;
 	u8 newcap = vctrl->pcicfg.end;
@@ -61,11 +59,11 @@ static u8 nvme_mdev_pci_allocate_cap(struct nvme_mdev_vctrl *vctrl,
 
 static void nvme_mdev_pci_setup_pm_cap(struct nvme_mdev_vctrl *vctrl)
 {
-	u8 *cfg  =  vctrl->pcicfg.values;
-	u8 *cfgm =  vctrl->pcicfg.wmask;
+	u8 *cfg = vctrl->pcicfg.values;
+	u8 *cfgm = vctrl->pcicfg.wmask;
 
-	u8 cap = nvme_mdev_pci_allocate_cap(vctrl,
-					    PCI_CAP_ID_PM, PCI_PM_SIZEOF);
+	u8 cap =
+		nvme_mdev_pci_allocate_cap(vctrl, PCI_CAP_ID_PM, PCI_PM_SIZEOF);
 
 	store_le16(cfg + cap + PCI_PM_PMC, 0x3);
 	store_le16(cfg + cap + PCI_PM_CTRL, PCI_PM_CTRL_NO_SOFT_RESET);
@@ -75,15 +73,14 @@ static void nvme_mdev_pci_setup_pm_cap(struct nvme_mdev_vctrl *vctrl)
 
 static void nvme_mdev_pci_setup_msix_cap(struct nvme_mdev_vctrl *vctrl)
 {
-	u8 *cfg  =  vctrl->pcicfg.values;
-	u8 *cfgm =  vctrl->pcicfg.wmask;
-	u8  cap = nvme_mdev_pci_allocate_cap(vctrl,
-					     PCI_CAP_ID_MSIX,
-					     PCI_CAP_MSIX_SIZEOF);
+	u8 *cfg = vctrl->pcicfg.values;
+	u8 *cfgm = vctrl->pcicfg.wmask;
+	u8 cap = nvme_mdev_pci_allocate_cap(vctrl, PCI_CAP_ID_MSIX,
+					    PCI_CAP_MSIX_SIZEOF);
 
 	int MSIX_TBL_SIZE = roundup(MAX_VIRTUAL_IRQS * 16, PAGE_SIZE);
-	int MSIX_PBA_SIZE = roundup(DIV_ROUND_UP(MAX_VIRTUAL_IRQS, 8),
-				    PAGE_SIZE);
+	int MSIX_PBA_SIZE =
+		roundup(DIV_ROUND_UP(MAX_VIRTUAL_IRQS, 8), PAGE_SIZE);
 
 	store_le16(cfg + cap + PCI_MSIX_FLAGS, MAX_VIRTUAL_IRQS - 1);
 	store_le16(cfgm + cap + PCI_MSIX_FLAGS,
@@ -92,21 +89,20 @@ static void nvme_mdev_pci_setup_msix_cap(struct nvme_mdev_vctrl *vctrl)
 	store_le32(cfg + cap + PCI_MSIX_TABLE, 0x2);
 	store_le32(cfg + cap + PCI_MSIX_PBA, MSIX_TBL_SIZE | 0x2);
 
-	nvme_mdev_pci_setup_bar(vctrl, PCI_BASE_ADDRESS_2,
-				__roundup_pow_of_two(MSIX_TBL_SIZE +
-						MSIX_PBA_SIZE), NULL);
+	nvme_mdev_pci_setup_bar(
+		vctrl, PCI_BASE_ADDRESS_2,
+		__roundup_pow_of_two(MSIX_TBL_SIZE + MSIX_PBA_SIZE), NULL);
 	vctrl->pcicfg.msixcap = cap;
 }
 
 static void nvme_mdev_pci_setup_pcie_cap(struct nvme_mdev_vctrl *vctrl)
 {
 	u8 *cfg = vctrl->pcicfg.values;
-	u8 cap = nvme_mdev_pci_allocate_cap(vctrl,
-					    PCI_CAP_ID_EXP,
+	u8 cap = nvme_mdev_pci_allocate_cap(vctrl, PCI_CAP_ID_EXP,
 					    PCI_CAP_EXP_ENDPOINT_SIZEOF_V2);
 
-	store_le16(cfg + cap + PCI_EXP_FLAGS, 0x02 |
-		   (PCI_EXP_TYPE_ENDPOINT << 4));
+	store_le16(cfg + cap + PCI_EXP_FLAGS,
+		   0x02 | (PCI_EXP_TYPE_ENDPOINT << 4));
 
 	store_le32(cfg + cap + PCI_EXP_DEVCAP,
 		   PCI_EXP_DEVCAP_RBER | PCI_EXP_DEVCAP_FLR);
@@ -121,9 +117,8 @@ static void nvme_mdev_pci_setup_pcie_cap(struct nvme_mdev_vctrl *vctrl)
 }
 
 /* This is called on PCI config read/write */
-static int nvme_mdev_pci_cfg_access(struct nvme_mdev_vctrl *vctrl,
-				    u16 offset, char *buf,
-				    u32 count, bool is_write)
+static int nvme_mdev_pci_cfg_access(struct nvme_mdev_vctrl *vctrl, u16 offset,
+				    char *buf, u32 count, bool is_write)
 {
 	unsigned int i;
 
@@ -188,20 +183,17 @@ int nvme_mdev_pci_create(struct nvme_mdev_vctrl *vctrl)
 	cfg = vctrl->pcicfg.values;
 	cfgm = vctrl->pcicfg.wmask;
 
-	nvme_mdev_vctrl_add_region(vctrl,
-				   VFIO_PCI_CONFIG_REGION_INDEX,
-				   PCI_CFG_SIZE,
-				   nvme_mdev_pci_cfg_access);
+	nvme_mdev_vctrl_add_region(vctrl, VFIO_PCI_CONFIG_REGION_INDEX,
+				   PCI_CFG_SIZE, nvme_mdev_pci_cfg_access);
 
 	/* vendor information */
 	store_le16(cfg + PCI_VENDOR_ID, NVME_MDEV_PCI_VENDOR_ID);
 	store_le16(cfg + PCI_DEVICE_ID, NVME_MDEV_PCI_DEVICE_ID);
 
 	/* pci command register */
-	store_le16(cfgm + PCI_COMMAND,
-		   PCI_COMMAND_INTX_DISABLE |
-		   PCI_COMMAND_MEMORY |
-		   PCI_COMMAND_MASTER);
+	store_le16(cfgm + PCI_COMMAND, PCI_COMMAND_INTX_DISABLE |
+					       PCI_COMMAND_MEMORY |
+					       PCI_COMMAND_MASTER);
 
 	/* pci status register */
 	store_le16(cfg + PCI_STATUS, PCI_STATUS_CAP_LIST);
diff --git a/drivers/nvme/mdev/priv.h b/drivers/nvme/mdev/priv.h
index 1dd5fce0bfa6..d93a7c0b9408 100644
--- a/drivers/nvme/mdev/priv.h
+++ b/drivers/nvme/mdev/priv.h
@@ -16,17 +16,32 @@
 #include <linux/pci.h>
 #include <linux/eventfd.h>
 #include <linux/byteorder/generic.h>
+#include <linux/bpf.h>
+#include <linux/filter.h>
+#include <linux/wait.h>
 #include "../host/nvme.h"
 #include "mdev.h"
 
-#define NVME_MDEV_NVME_VER  NVME_VS(0x01, 0x03, 0x00)
+#ifdef CONFIG_NVME_MDEV_BPF
+#include <linux/nvme_mdev.h>
+#include <linux/bpf_nvme_mdev.h>
+#endif
+
+extern struct list_head nvme_mdev_hctrl_list;
+extern struct mutex nvme_mdev_hctrl_list_mutex;
+
+#define NVME_MDEV_MKTAG(sqid, ucid) (((u32)(sqid) << 16) | ((u32)(ucid)))
+#define NVME_MDEV_TAG_SQID(tag) ((tag) >> 16)
+#define NVME_MDEV_TAG_UCID(tag) ((tag)&0xffff)
+
+#define NVME_MDEV_NVME_VER NVME_VS(0x01, 0x03, 0x00)
 #define NVME_MDEV_FIRMWARE_VERSION "1.0"
 
-#define NVME_MDEV_PCI_VENDOR_ID		PCI_VENDOR_ID_REDHAT_QUMRANET
-#define NVME_MDEV_PCI_DEVICE_ID		0x1234
-#define NVME_MDEV_PCI_SUBVENDOR_ID	PCI_SUBVENDOR_ID_REDHAT_QUMRANET
-#define NVME_MDEV_PCI_SUBDEVICE_ID	0
-#define NVME_MDEV_PCI_REVISION		0x0
+#define NVME_MDEV_PCI_VENDOR_ID PCI_VENDOR_ID_REDHAT_QUMRANET
+#define NVME_MDEV_PCI_DEVICE_ID 0x1234
+#define NVME_MDEV_PCI_SUBVENDOR_ID PCI_SUBVENDOR_ID_REDHAT_QUMRANET
+#define NVME_MDEV_PCI_SUBDEVICE_ID 0
+#define NVME_MDEV_PCI_REVISION 0x0
 
 #define DB_STRIDE_SHIFT 4 /*4 = 1 cacheline */
 #define MAX_VIRTUAL_QUEUES 16
@@ -47,16 +62,36 @@ extern bool use_shadow_doorbell;
 extern unsigned int io_timeout_ms;
 extern unsigned int poll_timeout_ms;
 extern unsigned int admin_poll_rate_ms;
+#ifdef CONFIG_NMBPF_POLL_ONCE
+static const unsigned int vsq_poll_loops = 1;
+#else
+extern unsigned int vsq_poll_loops;
+#endif
+
+struct nvme_mdev_notifier;
+struct nvme_mdev_vctrl_iothread_binding;
+
+#ifdef CONFIG_NVME_MDEV_BPF
+struct nvme_cmd_ctx {
+	struct nvme_command cmd;
+	atomic_t iostate ____cacheline_aligned_in_smp;
+};
+#endif
 
 /* virtual submission queue*/
-struct  nvme_vsq {
+struct nvme_vsq {
 	u16 qid;
 	u16 size;
-	u16 head;	/*next item to read */
+	u16 head; /*next item to read */
 
+	struct nvme_mdev_vctrl *vctrl;
 	struct nvme_command *data; /*the queue*/
 	struct nvme_vcq *vcq; /* completion queue*/
 
+#ifdef CONFIG_NVME_MDEV_BPF
+	struct nvme_cmd_ctx *ctx_data;
+#endif
+
 	dma_addr_t iova;
 	bool cont;
 
@@ -72,6 +107,7 @@ struct nvme_vcq {
 	u16 tail;
 	bool phase; /* current queue phase */
 
+	struct nvme_mdev_vctrl *vctrl;
 	volatile struct nvme_completion *data;
 
 	/* number of items pending*/
@@ -140,9 +176,9 @@ struct doorbell {
 
 /* MMIO state */
 struct nvme_mdev_user_ctrl_mmio {
-	u32 cc;		/* controller configuration */
-	u32 csts;	/* controller status */
-	u64 cap		/* controller capabilities*/;
+	u32 cc; /* controller configuration */
+	u32 csts; /* controller status */
+	u64 cap /* controller capabilities*/;
 
 	/* admin queue location & size */
 	u32 aqa;
@@ -186,7 +222,7 @@ struct nvme_mdev_user_irq {
 	struct eventfd_ctx *trigger;
 	/* IRQ coalescing */
 	bool irq_coalesc_en;
-	time_t irq_time;
+	ktime_t irq_time;
 	unsigned int irq_pending_cnt;
 };
 
@@ -221,7 +257,7 @@ struct nvme_mdev_user_events {
 	unsigned long events_masked[BITS_TO_LONGS(MAX_LOG_PAGES)];
 
 	/* events that are pending to be sent when user gives us an AER*/
-	unsigned long  events_pending[BITS_TO_LONGS(MAX_LOG_PAGES)];
+	unsigned long events_pending[BITS_TO_LONGS(MAX_LOG_PAGES)];
 	u32 event_values[MAX_LOG_PAGES];
 };
 
@@ -234,9 +270,8 @@ struct nvme_mdev_hq {
 
 /* IO region abstraction (BARs, the PCI config space */
 struct nvme_mdev_vctrl;
-typedef int (*region_access_fn) (struct nvme_mdev_vctrl *vctrl,
-				 u16 offset, char *buf,
-				 u32 size, bool is_write);
+typedef int (*region_access_fn)(struct nvme_mdev_vctrl *vctrl, u16 offset,
+				char *buf, u32 size, bool is_write);
 
 struct nvme_mdev_io_region {
 	unsigned int size;
@@ -263,13 +298,13 @@ struct nvme_mdev_perf {
 
 /*Virtual NVME controller state */
 struct nvme_mdev_vctrl {
-	struct kref ref;
 	struct mutex lock;
 	struct list_head link;
 
 	struct mdev_device *mdev;
 	struct nvme_mdev_hctrl *hctrl;
-	bool inuse;
+	struct nvme_mdev_vctrl_iothread_binding *binding;
+	atomic_t inuse;
 
 	struct nvme_mdev_io_region regions[VFIO_PCI_NUM_REGIONS];
 
@@ -285,12 +320,10 @@ struct nvme_mdev_vctrl {
 	unsigned int ns_log_size;
 
 	/* emulated submission queues*/
-	struct nvme_vsq vsqs[MAX_VIRTUAL_QUEUES];
-	unsigned long vsq_en[BITS_TO_LONGS(MAX_VIRTUAL_QUEUES)];
+	struct nvme_vsq __rcu *vsqs[MAX_VIRTUAL_QUEUES];
 
 	/* emulated completion queues*/
-	unsigned long vcq_en[BITS_TO_LONGS(MAX_VIRTUAL_QUEUES)];
-	struct nvme_vcq vcqs[MAX_VIRTUAL_QUEUES];
+	struct nvme_vcq __rcu *vcqs[MAX_VIRTUAL_QUEUES];
 
 	/* Host IO queues*/
 	int max_host_hw_queues;
@@ -302,15 +335,14 @@ struct nvme_mdev_vctrl {
 	struct nvme_mdev_viommu viommu;
 
 	/* the IO thread */
-	struct task_struct *iothread;
-	bool iothread_parked;
 	bool io_idle;
 	ktime_t now;
+	wait_queue_head_t quiescent_wqh;
 
 	/* Settings */
 	unsigned int arb_burst_shift;
+	unsigned int arb_burst;
 	u8 worload_hint;
-	unsigned int iothread_cpu;
 
 	/* Identification*/
 	char subnqn[256];
@@ -319,6 +351,13 @@ struct nvme_mdev_vctrl {
 	bool vctrl_paused; /* true when the host device paused our IO */
 
 	struct nvme_mdev_perf perf;
+
+#ifdef CONFIG_NVME_MDEV_BPF
+	struct bpf_prog __rcu *prog;
+	u32 nl_enabled;
+	struct nvme_mdev_notifier __rcu *notifier[MAX_VIRTUAL_QUEUES];
+	atomic_t notifier_cnt;
+#endif
 };
 
 /* mdev instance type*/
@@ -389,6 +428,7 @@ struct nvme_mdev_hctrl {
 };
 
 /* vctrl.c*/
+struct nvme_mdev_vctrl *nvme_mdev_vctrl_lookup(struct nvme_mdev_hctrl *hctrl);
 struct nvme_mdev_vctrl *nvme_mdev_vctrl_create(struct mdev_device *mdev,
 					       struct nvme_mdev_hctrl *hctrl,
 					       unsigned int max_host_queues);
@@ -398,13 +438,16 @@ int nvme_mdev_vctrl_destroy(struct nvme_mdev_vctrl *vctrl);
 int nvme_mdev_vctrl_open(struct nvme_mdev_vctrl *vctrl);
 void nvme_mdev_vctrl_release(struct nvme_mdev_vctrl *vctrl);
 
+void __nvme_mdev_vctrl_get(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_vctrl_put(struct nvme_mdev_vctrl *vctrl);
+
 void nvme_mdev_vctrl_pause(struct nvme_mdev_vctrl *vctrl);
 void nvme_mdev_vctrl_resume(struct nvme_mdev_vctrl *vctrl);
 
-bool nvme_mdev_vctrl_enable(struct nvme_mdev_vctrl *vctrl,
-			    dma_addr_t cqiova, dma_addr_t sqiova, u32 sizes);
+bool __nvme_mdev_vctrl_enable(struct nvme_mdev_vctrl *vctrl, dma_addr_t cqiova,
+			      dma_addr_t sqiova, u32 sizes);
 
-void nvme_mdev_vctrl_disable(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_vctrl_disable(struct nvme_mdev_vctrl *vctrl);
 
 void nvme_mdev_vctrl_reset(struct nvme_mdev_vctrl *vctrl);
 void __nvme_mdev_vctrl_reset(struct nvme_mdev_vctrl *vctrl, bool pci_reset);
@@ -414,8 +457,7 @@ void nvme_mdev_vctrl_add_region(struct nvme_mdev_vctrl *vctrl,
 				region_access_fn access_fn);
 
 void nvme_mdev_vctrl_region_set_mmap(struct nvme_mdev_vctrl *vctrl,
-				     unsigned int index,
-				     unsigned int offset,
+				     unsigned int index, unsigned int offset,
 				     unsigned int size,
 				     const struct vm_operations_struct *ops);
 
@@ -428,26 +470,37 @@ void nvme_mdev_vctrl_bind_iothread(struct nvme_mdev_vctrl *vctrl,
 int nvme_mdev_vctrl_set_shadow_doorbell_supported(struct nvme_mdev_vctrl *vctrl,
 						  bool enable);
 
-int nvme_mdev_vctrl_hq_alloc(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_vctrl_hq_free(struct nvme_mdev_vctrl *vctrl, u16 qid);
+int __nvme_mdev_vctrl_hq_alloc(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_vctrl_hq_free(struct nvme_mdev_vctrl *vctrl, u16 qid);
 unsigned int nvme_mdev_vctrl_hqs_list(struct nvme_mdev_vctrl *vctrl, u16 *out);
 bool nvme_mdev_vctrl_is_dead(struct nvme_mdev_vctrl *vctrl);
 
 int nvme_mdev_vctrl_viommu_map(struct nvme_mdev_vctrl *vctrl, u32 flags,
 			       dma_addr_t iova, u64 size);
 
-int nvme_mdev_vctrl_viommu_unmap(struct nvme_mdev_vctrl *vctrl,
-				 dma_addr_t iova, u64 size);
+int nvme_mdev_vctrl_viommu_unmap(struct nvme_mdev_vctrl *vctrl, dma_addr_t iova,
+				 u64 size);
+
+void __nvme_mdev_adm_id_vctrl(struct nvme_mdev_vctrl *vctrl,
+			      struct nvme_mdev_hctrl *hctrl,
+			      struct nvme_id_ctrl *id);
+void __nvme_mdev_adm_id_vns(struct nvme_mdev_vctrl *vctrl,
+			    struct nvme_mdev_vns *ns, struct nvme_id_ns *idns);
+
+#ifdef CONFIG_NVME_MDEV_BPF
+int nvme_mdev_vctrl_attach_prog(struct nvme_mdev_vctrl *vctrl,
+				struct bpf_prog *prog);
+#endif
 
 /* hctrl.c*/
 struct nvme_mdev_inst_type *nvme_mdev_inst_type_get(const char *name);
 struct nvme_mdev_hctrl *nvme_mdev_hctrl_lookup_get(struct device *parent);
+struct nvme_mdev_hctrl *nvme_mdev_hctrl_lookup_get_byid(u32 ctrlid);
 void nvme_mdev_hctrl_put(struct nvme_mdev_hctrl *hctrl);
 
 int nvme_mdev_hctrl_hqs_available(struct nvme_mdev_hctrl *hctrl);
 
-bool nvme_mdev_hctrl_hqs_reserve(struct nvme_mdev_hctrl *hctrl,
-				 unsigned int n);
+bool nvme_mdev_hctrl_hqs_reserve(struct nvme_mdev_hctrl *hctrl, unsigned int n);
 void nvme_mdev_hctrl_hqs_unreserve(struct nvme_mdev_hctrl *hctrl,
 				   unsigned int n);
 
@@ -457,24 +510,20 @@ bool nvme_mdev_hctrl_hq_can_submit(struct nvme_mdev_hctrl *hctrl, u16 qid);
 bool nvme_mdev_hctrl_hq_check_op(struct nvme_mdev_hctrl *hctrl, u8 optcode);
 
 int nvme_mdev_hctrl_hq_submit(struct nvme_mdev_hctrl *hctrl,
-			      struct nvme_mdev_vns *vns,
-			      u16 qid, u32 tag,
+			      struct nvme_mdev_vns *vns, u16 qid, u32 tag,
 			      struct nvme_command *cmd,
 			      struct nvme_ext_data_iter *datait);
 
-int nvme_mdev_hctrl_hq_poll(struct nvme_mdev_hctrl *hctrl,
-			    u32 qid,
+int nvme_mdev_hctrl_hq_poll(struct nvme_mdev_hctrl *hctrl, u32 qid,
 			    struct nvme_ext_cmd_result *results,
 			    unsigned int max_len);
 
 void nvme_mdev_hctrl_destroy_all(void);
 
 /* io.c */
-int nvme_mdev_io_create(struct nvme_mdev_vctrl *vctrl, unsigned int cpu);
-void nvme_mdev_io_free(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_io_pause(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_io_resume(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_assert_io_not_running(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_io_pause(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_io_resume(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_assert_io_not_running(struct nvme_mdev_vctrl *vctrl);
 
 /* mmio.c*/
 int nvme_mdev_mmio_create(struct nvme_mdev_vctrl *vctrl);
@@ -482,121 +531,115 @@ void nvme_mdev_mmio_open(struct nvme_mdev_vctrl *vctrl);
 void nvme_mdev_mmio_reset(struct nvme_mdev_vctrl *vctrl, bool pci_reset);
 void nvme_mdev_mmio_free(struct nvme_mdev_vctrl *vctrl);
 
-int nvme_mdev_mmio_enable_dbs(struct nvme_mdev_vctrl *vctrl);
-int nvme_mdev_mmio_enable_dbs_shadow(struct nvme_mdev_vctrl *vctrl,
-				     dma_addr_t sdb_iova, dma_addr_t eidx_iova);
+int __nvme_mdev_mmio_enable_dbs(struct nvme_mdev_vctrl *vctrl);
+int __nvme_mdev_mmio_enable_dbs_shadow(struct nvme_mdev_vctrl *vctrl,
+				       dma_addr_t sdb_iova,
+				       dma_addr_t eidx_iova);
 
-void nvme_mdev_mmio_viommu_update(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_mmio_disable_dbs(struct nvme_mdev_vctrl *vctrl);
-bool nvme_mdev_mmio_db_check(struct nvme_mdev_vctrl *vctrl,
-			     u16 qid, u16 size, u16 db);
+void __nvme_mdev_mmio_viommu_update(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_mmio_disable_dbs(struct nvme_mdev_vctrl *vctrl);
+bool nvme_mdev_mmio_db_check(struct nvme_mdev_vctrl *vctrl, u16 qid, u16 size,
+			     u16 db);
 
 /* pci.c*/
 int nvme_mdev_pci_create(struct nvme_mdev_vctrl *vctrl);
 void nvme_mdev_pci_free(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_pci_setup_bar(struct nvme_mdev_vctrl *vctrl,
-			     u8 bar, unsigned int size,
-			     region_access_fn access_fn);
+void nvme_mdev_pci_setup_bar(struct nvme_mdev_vctrl *vctrl, u8 bar,
+			     unsigned int size, region_access_fn access_fn);
 /* adm.c*/
-void nvme_mdev_adm_process_sq(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_adm_process_sq(struct nvme_mdev_vctrl *vctrl);
 
 /* events.c */
 void nvme_mdev_events_init(struct nvme_mdev_vctrl *vctrl);
 void nvme_mdev_events_reset(struct nvme_mdev_vctrl *vctrl);
 
-int nvme_mdev_event_request_receive(struct nvme_mdev_vctrl *vctrl, u16 cid);
-void nvme_mdev_event_process_ack(struct nvme_mdev_vctrl *vctrl, u8 log_page);
+int __nvme_mdev_event_request_receive(struct nvme_mdev_vctrl *vctrl, u16 cid);
+void __nvme_mdev_event_process_ack(struct nvme_mdev_vctrl *vctrl, u8 log_page);
 
 void nvme_mdev_event_send(struct nvme_mdev_vctrl *vctrl,
 			  enum nvme_async_event_type type,
 			  enum nvme_async_event info);
 
 u32 nvme_mdev_event_read_aen_config(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_event_set_aen_config(struct nvme_mdev_vctrl *vctrl, u32 value);
+void __nvme_mdev_event_set_aen_config(struct nvme_mdev_vctrl *vctrl, u32 value);
 
 /* irq.c*/
 void nvme_mdev_irqs_setup(struct nvme_mdev_vctrl *vctrl);
-void nvme_mdev_irqs_reset(struct nvme_mdev_vctrl *vctrl);
+void __nvme_mdev_irqs_reset(struct nvme_mdev_vctrl *vctrl);
 
 int nvme_mdev_irqs_enable(struct nvme_mdev_vctrl *vctrl,
 			  enum nvme_mdev_irq_mode mode);
 void nvme_mdev_irqs_disable(struct nvme_mdev_vctrl *vctrl,
 			    enum nvme_mdev_irq_mode mode);
 
-int nvme_mdev_irqs_set_triggers(struct nvme_mdev_vctrl *vctrl,
-				int start, int count, int32_t *fds);
+int nvme_mdev_irqs_set_triggers(struct nvme_mdev_vctrl *vctrl, int start,
+				int count, int32_t *fds);
 int nvme_mdev_irqs_set_unplug_trigger(struct nvme_mdev_vctrl *vctrl,
 				      int32_t fd);
 
 void nvme_mdev_irq_raise_unplug_event(struct nvme_mdev_vctrl *vctrl,
 				      unsigned int count);
-void nvme_mdev_irq_raise(struct nvme_mdev_vctrl *vctrl,
-			 unsigned int index);
-void nvme_mdev_irq_trigger(struct nvme_mdev_vctrl *vctrl,
-			   unsigned int index);
+void nvme_mdev_irq_raise(struct nvme_mdev_vctrl *vctrl, unsigned int index);
+void nvme_mdev_irq_trigger(struct nvme_mdev_vctrl *vctrl, unsigned int index);
 void nvme_mdev_irq_cond_trigger(struct nvme_mdev_vctrl *vctrl,
 				unsigned int index);
-void nvme_mdev_irq_clear(struct nvme_mdev_vctrl *vctrl,
-			 unsigned int index);
+void nvme_mdev_irq_clear(struct nvme_mdev_vctrl *vctrl, unsigned int index);
 
 /* ns.c*/
-int nvme_mdev_vns_open(struct nvme_mdev_vctrl *vctrl,
-		       u32 host_nsid, unsigned int host_partid);
-int nvme_mdev_vns_destroy(struct nvme_mdev_vctrl *vctrl,
-			  u32 user_nsid);
-void nvme_mdev_vns_destroy_all(struct nvme_mdev_vctrl *vctrl);
+int nvme_mdev_vns_open(struct nvme_mdev_vctrl *vctrl, u32 host_nsid,
+		       unsigned int host_partid);
+int nvme_mdev_vns_destroy(struct nvme_mdev_vctrl *vctrl, u32 user_nsid);
+void __nvme_mdev_vns_destroy_all(struct nvme_mdev_vctrl *vctrl);
 
 struct nvme_mdev_vns *nvme_mdev_vns_from_vnsid(struct nvme_mdev_vctrl *vctrl,
 					       u32 user_ns_id);
 
-int nvme_mdev_vns_print_description(struct nvme_mdev_vctrl *vctrl,
-				    char *buf, unsigned int size);
-void nvme_mdev_vns_host_ns_update(struct nvme_mdev_vctrl *vctrl,
-				  u32 host_nsid, bool removed);
+int nvme_mdev_vns_print_description(struct nvme_mdev_vctrl *vctrl, char *buf,
+				    unsigned int size);
+void nvme_mdev_vns_host_ns_update(struct nvme_mdev_vctrl *vctrl, u32 host_nsid,
+				  bool removed);
 
 void nvme_mdev_vns_log_reset(struct nvme_mdev_vctrl *vctrl);
 
 /* vcq.c */
-int nvme_mdev_vcq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
-		       dma_addr_t iova, bool cont, u16 size, int irq);
+int __nvme_mdev_vcq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
+			 dma_addr_t iova, bool cont, u16 size, int irq);
 
-int nvme_mdev_vcq_viommu_update(struct nvme_mdev_viommu *viommu,
-				struct nvme_vcq *q);
+int __nvme_mdev_vcq_viommu_update(struct nvme_mdev_viommu *viommu,
+				  struct nvme_vcq *q);
 
-void nvme_mdev_vcq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid);
-void nvme_mdev_vcq_process(struct nvme_mdev_vctrl *vctrl, u16 qid,
+void __nvme_mdev_vcq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid);
+void nvme_mdev_vcq_process(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q,
 			   bool trigger_irqs);
 
-bool nvme_mdev_vcq_flush(struct nvme_mdev_vctrl *vctrl, u16 qid);
+bool nvme_mdev_vcq_flush(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q);
 bool nvme_mdev_vcq_reserve_space(struct nvme_vcq *q);
 
-void nvme_mdev_vcq_write_io(struct nvme_mdev_vctrl *vctrl,
-			    struct nvme_vcq *q, u16 sq_head,
-			    u16 sqid, u16 cid, u16 status);
+void nvme_mdev_vcq_write_io(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q,
+			    u16 sq_head, u16 sqid, u16 cid, u16 status);
 
-void nvme_mdev_vcq_write_adm(struct nvme_mdev_vctrl *vctrl,
-			     struct nvme_vcq *q, u32 dw0,
-			     u16 sq_head, u16 cid, u16 status);
+void nvme_mdev_vcq_write_adm(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q,
+			     u32 dw0, u16 sq_head, u16 cid, u16 status);
 /* vsq.c*/
-int nvme_mdev_vsq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
-		       dma_addr_t iova, bool cont, u16 size, u16 cqid);
+int __nvme_mdev_vsq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
+			 dma_addr_t iova, bool cont, u16 size, u16 cqid);
 
-int nvme_mdev_vsq_viommu_update(struct nvme_mdev_viommu *viommu,
-				struct nvme_vsq *q);
+int __nvme_mdev_vsq_viommu_update(struct nvme_mdev_viommu *viommu,
+				  struct nvme_vsq *q);
 
-void nvme_mdev_vsq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid);
+void __nvme_mdev_vsq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid);
 
-bool nvme_mdev_vsq_has_data(struct nvme_mdev_vctrl *vctrl,
-			    struct nvme_vsq *q);
+bool nvme_mdev_vsq_has_data(struct nvme_mdev_vctrl *vctrl, struct nvme_vsq *q);
 
 const struct nvme_command *nvme_mdev_vsq_get_cmd(struct nvme_mdev_vctrl *vctrl,
 						 struct nvme_vsq *q);
 
 void nvme_mdev_vsq_cmd_done_io(struct nvme_mdev_vctrl *vctrl,
-			       u16 sqid, u16 cid, u16 status);
-void nvme_mdev_vsq_cmd_done_adm(struct nvme_mdev_vctrl *vctrl,
-				u32 dw0, u16 cid, u16 status);
-bool nvme_mdev_vsq_suspend_io(struct nvme_mdev_vctrl *vctrl, u16 sqid);
+			       struct nvme_vsq *q, u16 cid, u16 status);
+void __nvme_mdev_vsq_cmd_done_adm(struct nvme_mdev_vctrl *vctrl, u32 dw0,
+				  u16 cid, u16 status);
+bool __nvme_mdev_vsq_suspend_io(struct nvme_mdev_vctrl *vctrl,
+				struct nvme_vsq *q);
 
 /* udata.c*/
 void nvme_mdev_udata_iter_setup(struct nvme_mdev_viommu *viommu,
@@ -615,32 +658,29 @@ int nvme_mdev_write_to_udata(struct nvme_ext_data_iter *dstit, void *src,
 			     u64 size);
 
 void *nvme_mdev_udata_queue_vmap(struct nvme_mdev_viommu *viommu,
-				 dma_addr_t iova,
-				 unsigned int size, bool cont);
+				 dma_addr_t iova, unsigned int size, bool cont);
 /* viommu.c */
 void nvme_mdev_viommu_init(struct nvme_mdev_viommu *viommu,
-			   struct device *sw_dev,
-			   struct device *hw_dev);
+			   struct device *sw_dev, struct device *hw_dev);
 
 int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu, u32 flags,
 			 dma_addr_t iova, u64 size);
 
-int nvme_mdev_viommu_remove(struct nvme_mdev_viommu *viommu,
-			    dma_addr_t iova, u64 size);
+int nvme_mdev_viommu_remove(struct nvme_mdev_viommu *viommu, dma_addr_t iova,
+			    u64 size);
 
-int nvme_mdev_viommu_translate(struct nvme_mdev_viommu *viommu,
-			       dma_addr_t iova,
-			       dma_addr_t *physical,
-			       dma_addr_t *host_iova);
+int nvme_mdev_viommu_translate(struct nvme_mdev_viommu *viommu, dma_addr_t iova,
+			       dma_addr_t *physical, dma_addr_t *host_iova);
 
 int nvme_mdev_viommu_create_kmap(struct nvme_mdev_viommu *viommu,
-				 dma_addr_t iova, struct page_map *page);
+				 dma_addr_t iova, struct page_map *page,
+				 bool global);
 
 void nvme_mdev_viommu_free_kmap(struct nvme_mdev_viommu *viommu,
-				struct page_map *page);
+				struct page_map *page, bool global);
 
 void nvme_mdev_viommu_update_kmap(struct nvme_mdev_viommu *viommu,
-				  struct page_map *page);
+				  struct page_map *page, bool global);
 
 void nvme_mdev_viommu_reset(struct nvme_mdev_viommu *viommu);
 
@@ -649,33 +689,34 @@ void nvme_mdev_viommu_reset(struct nvme_mdev_viommu *viommu);
 #define store_le64(address, value) (*((__le64 *)(address)) = cpu_to_le64(value))
 #define store_le32(address, value) (*((__le32 *)(address)) = cpu_to_le32(value))
 #define store_le16(address, value) (*((__le16 *)(address)) = cpu_to_le16(value))
-#define store_le8(address, value)  (*((u8 *)(address)) = (value))
+#define store_le8(address, value) (*((u8 *)(address)) = (value))
 
 #define load_le16(address) le16_to_cpu(*(__le16 *)(address))
 #define load_le32(address) le32_to_cpu(*(__le32 *)(address))
 
-#define store_strsp(dst, src) \
+#define store_strsp(dst, src)                                                  \
 	memcpy_and_pad(dst, sizeof(dst), src, sizeof(src) - 1, ' ')
 
 #define DNR(e) ((e) | NVME_SC_DNR)
 
-#define PAGE_ADDRESS(address) ((address) & PAGE_MASK)
+#define PAGE_ADDRESS(address) ((address)&PAGE_MASK)
 #define OFFSET_IN_PAGE(address) ((address) & ~(PAGE_MASK))
 
-#define _DBG(vctrl, fmt, ...) \
+#define _DBG(vctrl, fmt, ...)                                                  \
 	dev_dbg(mdev_dev((vctrl)->mdev), fmt, ##__VA_ARGS__)
 
-#define _INFO(vctrl, fmt, ...) \
+#define _INFO(vctrl, fmt, ...)                                                 \
 	dev_info(mdev_dev((vctrl)->mdev), fmt, ##__VA_ARGS__)
 
-#define _WARN(vctrl, fmt, ...) \
+#define _WARN(vctrl, fmt, ...)                                                 \
 	dev_warn(mdev_dev((vctrl)->mdev), fmt, ##__VA_ARGS__)
 
-#define mdev_to_vctrl(mdev) \
-	((struct nvme_mdev_vctrl *)mdev_get_drvdata(mdev))
+#define _ERR(vctrl, fmt, ...)                                                  \
+	dev_err(mdev_dev((vctrl)->mdev), fmt, ##__VA_ARGS__)
 
-#define dev_to_vctrl(mdev) \
-	mdev_to_vctrl(mdev_from_dev(dev))
+#define mdev_to_vctrl(mdev) ((struct nvme_mdev_vctrl *)mdev_get_drvdata(mdev))
+
+#define dev_to_vctrl(mdev) mdev_to_vctrl(mdev_from_dev(dev))
 
 #define RSRV_NSID (BIT(1))
 #define RSRV_DW23 (BIT(2) | BIT(3))
@@ -690,15 +731,15 @@ void nvme_mdev_viommu_reset(struct nvme_mdev_viommu *viommu);
 #define RSRV_DW13_15 (BIT(13) | BIT(14) | BIT(15))
 #define RSRV_DW14_15 (BIT(14) | BIT(15))
 
-static inline bool check_reserved_dwords(const u32 *dwords,
-					 int count, unsigned long bitmask)
+static inline bool check_reserved_dwords(const u32 *dwords, int count,
+					 unsigned long bitmask)
 {
 	int bit;
 
 	if (WARN_ON(count > BITS_PER_TYPE(long)))
 		return false;
 
-	for_each_set_bit(bit, &bitmask, count)
+	for_each_set_bit (bit, &bitmask, count)
 		if (dwords[bit])
 			return false;
 	return true;
@@ -771,4 +812,31 @@ extern struct mdev_parent_ops mdev_fops;
 extern struct list_head nvme_mdev_vctrl_list;
 extern struct mutex nvme_mdev_vctrl_list_mutex;
 
+struct io_ctx {
+	const struct nvme_command *in;
+	struct nvme_command out;
+	struct bpf_io_ctx bctx;
+
+	struct nvme_mdev_hctrl *hctrl;
+	struct nvme_mdev_vctrl *vctrl;
+
+	/* members below must not be used by io_passthrough/io_act
+	 * to allow creating a fake io_ctx in notifyfd_runhook
+	 */
+
+	struct nvme_mdev_vns *ns;
+	struct nvme_ext_data_iter udatait;
+	struct nvme_ext_data_iter *kdatait;
+};
+
+void nvme_mdev_io_begin_poll(struct nvme_mdev_vctrl_iothread_binding *binding);
+/* -EAGAIN means poll should still be running */
+int nvme_mdev_io_polling_loop(struct io_ctx *ctx);
+int nvme_mdev_io_end_poll(struct io_ctx *ctx);
+
+#ifdef CONFIG_NVME_MDEV_BPF
+int nvme_mdev_io_act(struct io_ctx *ctx, struct nvme_vsq *vsq, u16 ucid,
+		     bool needs_react);
+#endif
+
 #endif // _MDEV_NVME_H
diff --git a/drivers/nvme/mdev/udata.c b/drivers/nvme/mdev/udata.c
index 7af6b3f6d6aa..1ba45315400a 100644
--- a/drivers/nvme/mdev/udata.c
+++ b/drivers/nvme/mdev/udata.c
@@ -28,14 +28,13 @@ void nvme_mdev_udata_iter_setup(struct nvme_mdev_viommu *viommu,
 static int nvme_mdev_udata_iter_load_prplist(struct nvme_ext_data_iter *iter,
 					     dma_addr_t iova)
 {
-	dma_addr_t  data_iova;
+	dma_addr_t data_iova;
 	int ret;
 	__le64 *map;
 
 	/* map the prp list*/
-	ret = nvme_mdev_viommu_create_kmap(iter->viommu,
-					   PAGE_ADDRESS(iova),
-					   &iter->uprp.page);
+	ret = nvme_mdev_viommu_create_kmap(iter->viommu, PAGE_ADDRESS(iova),
+					   &iter->uprp.page, false);
 	if (ret)
 		return ret;
 
@@ -46,15 +45,17 @@ static int nvme_mdev_udata_iter_load_prplist(struct nvme_ext_data_iter *iter,
 	data_iova = le64_to_cpu(map[iter->uprp.index]);
 
 	if (OFFSET_IN_PAGE(data_iova) != 0) {
-		nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page);
+		nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page,
+					   false);
 		return -EINVAL;
 	}
 
 	/* translate the entry to complete the setup*/
-	ret =  nvme_mdev_viommu_translate(iter->viommu, data_iova,
-					  &iter->physical, &iter->host_iova);
+	ret = nvme_mdev_viommu_translate(iter->viommu, data_iova,
+					 &iter->physical, &iter->host_iova);
 	if (ret)
-		nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page);
+		nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page,
+					   false);
 
 	return ret;
 }
@@ -70,7 +71,8 @@ static int nvme_mdev_udata_iter_next_prplist(struct nvme_ext_data_iter *iter)
 		return 0;
 
 	if (--iter->count == 0) {
-		nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page);
+		nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page,
+					   false);
 		return 0;
 	}
 
@@ -83,12 +85,11 @@ static int nvme_mdev_udata_iter_next_prplist(struct nvme_ext_data_iter *iter)
 		if (OFFSET_IN_PAGE(iova) != 0)
 			return -EINVAL;
 
-		ret  = nvme_mdev_viommu_translate(iter->viommu, iova,
-						  &iter->physical,
-						  &iter->host_iova);
+		ret = nvme_mdev_viommu_translate(
+			iter->viommu, iova, &iter->physical, &iter->host_iova);
 		if (ret)
 			nvme_mdev_viommu_free_kmap(iter->viommu,
-						   &iter->uprp.page);
+						   &iter->uprp.page, false);
 		return ret;
 	}
 
@@ -98,14 +99,14 @@ static int nvme_mdev_udata_iter_next_prplist(struct nvme_ext_data_iter *iter)
 	if (OFFSET_IN_PAGE(iova) != 0)
 		return -EINVAL;
 
-	nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page);
+	nvme_mdev_viommu_free_kmap(iter->viommu, &iter->uprp.page, false);
 	return nvme_mdev_udata_iter_load_prplist(iter, iova);
 }
 
 /* ->next function when iterator points to user data pointer*/
 static int nvme_mdev_udata_iter_next_dptr(struct nvme_ext_data_iter *iter)
 {
-	dma_addr_t  iova;
+	dma_addr_t iova;
 
 	if (WARN_ON(iter->count <= 0))
 		return 0;
@@ -125,9 +126,8 @@ static int nvme_mdev_udata_iter_next_dptr(struct nvme_ext_data_iter *iter)
 		 */
 		if (OFFSET_IN_PAGE(iova) != 0)
 			return -EINVAL;
-		return nvme_mdev_viommu_translate(iter->viommu, iova,
-						  &iter->physical,
-						  &iter->host_iova);
+		return nvme_mdev_viommu_translate(
+			iter->viommu, iova, &iter->physical, &iter->host_iova);
 	} else {
 		/*
 		 * Second dptr entry is prp pointer, and it might not
@@ -157,8 +157,8 @@ int nvme_mdev_udata_iter_set_dptr(struct nvme_ext_data_iter *it,
 	it->next = nvme_mdev_udata_iter_next_dptr;
 	it->count = DIV_ROUND_UP_ULL(size + page_offset, PAGE_SIZE);
 
-	ret = nvme_mdev_viommu_translate(it->viommu, iova,
-					 &it->physical, &it->host_iova);
+	ret = nvme_mdev_viommu_translate(it->viommu, iova, &it->physical,
+					 &it->host_iova);
 	if (ret)
 		return ret;
 
@@ -187,8 +187,8 @@ static void nvme_mdev_kdata_iter_free(struct nvme_ext_data_iter *it)
 	struct device *dma_dev = it->viommu->hw_dev;
 
 	if (dma_dev)
-		dma_free_coherent(dma_dev, it->kmem.size,
-				  it->kmem.data, it->kmem.dma_addr);
+		dma_free_coherent(dma_dev, it->kmem.size, it->kmem.data,
+				  it->kmem.dma_addr);
 	else
 		kfree(it->kmem.data);
 	kfree(it);
@@ -200,18 +200,17 @@ nvme_mdev_kdata_iter_alloc(struct nvme_mdev_viommu *viommu, unsigned int size)
 {
 	struct nvme_ext_data_iter *it;
 
-	it = kzalloc(sizeof(*it), GFP_KERNEL);
+	it = kzalloc(sizeof(*it), GFP_ATOMIC);
 	if (!it)
 		return NULL;
 
 	it->viommu = viommu;
 	it->kmem.size = size;
 	if (viommu->hw_dev) {
-		it->kmem.data = dma_alloc_coherent(viommu->hw_dev, size,
-						   &it->kmem.dma_addr,
-						   GFP_KERNEL);
+		it->kmem.data = dma_alloc_coherent(
+			viommu->hw_dev, size, &it->kmem.dma_addr, GFP_ATOMIC);
 	} else {
-		it->kmem.data = kzalloc(size, GFP_KERNEL);
+		it->kmem.data = kzalloc(size, GFP_ATOMIC);
 		it->kmem.dma_addr = 0;
 	}
 
@@ -223,8 +222,8 @@ nvme_mdev_kdata_iter_alloc(struct nvme_mdev_viommu *viommu, unsigned int size)
 	it->physical = virt_to_phys(it->kmem.data);
 	it->host_iova = it->kmem.dma_addr;
 
-	it->count = DIV_ROUND_UP(size + OFFSET_IN_PAGE(it->physical),
-				 PAGE_SIZE);
+	it->count =
+		DIV_ROUND_UP(size + OFFSET_IN_PAGE(it->physical), PAGE_SIZE);
 
 	it->next = nvme_mdev_kdata_iter_next;
 	it->release = nvme_mdev_kdata_iter_free;
@@ -240,7 +239,7 @@ int nvme_mdev_read_from_udata(void *dst, struct nvme_ext_data_iter *srcit,
 
 	while (srcit->count && size > 0) {
 		struct page *page = pfn_to_page(PHYS_PFN(srcit->physical));
-		void *src = kmap(page);
+		void *src = kmap_atomic(page);
 
 		if (!src)
 			return -ENOMEM;
@@ -251,13 +250,13 @@ int nvme_mdev_read_from_udata(void *dst, struct nvme_ext_data_iter *srcit,
 		memcpy(dst, src + srcoffset, chunk_size);
 		dst += chunk_size;
 		size -= chunk_size;
-		kunmap(page);
+		kunmap_atomic(src);
 
 		ret = srcit->next(srcit);
 		if (ret)
 			return ret;
 	}
-	WARN_ON(size > 0);
+	WARN_ON_ONCE(size > 0);
 	return 0;
 }
 
@@ -317,10 +316,9 @@ static int nvme_mdev_queue_getpages_contiguous(struct nvme_mdev_viommu *viommu,
 	dma_addr_t host_page_iova;
 	phys_addr_t physical;
 
-	for (i = 0 ; i < npages; i++) {
+	for (i = 0; i < npages; i++) {
 		ret = nvme_mdev_viommu_translate(viommu, iova + (PAGE_SIZE * i),
-						 &physical,
-						 &host_page_iova);
+						 &physical, &host_page_iova);
 		if (ret)
 			return ret;
 		pages[i] = pfn_to_page(PHYS_PFN(physical));
@@ -337,8 +335,7 @@ static int nvme_mdev_queue_getpages_prplist(struct nvme_mdev_viommu *viommu,
 	int ret, i = 0;
 	struct nvme_ext_data_iter uprpit;
 
-	ret = nvme_mdev_udata_iter_set_queue_prplist(viommu,
-						     &uprpit, iova,
+	ret = nvme_mdev_udata_iter_set_queue_prplist(viommu, &uprpit, iova,
 						     npages * PAGE_SIZE);
 	if (ret)
 		return ret;
@@ -354,9 +351,7 @@ static int nvme_mdev_queue_getpages_prplist(struct nvme_mdev_viommu *viommu,
 
 /* map a SQ/CQ queue to host physical memory */
 void *nvme_mdev_udata_queue_vmap(struct nvme_mdev_viommu *viommu,
-				 dma_addr_t iova,
-				 unsigned int size,
-				 bool cont)
+				 dma_addr_t iova, unsigned int size, bool cont)
 {
 	int ret;
 	unsigned int npages;
@@ -372,16 +367,17 @@ void *nvme_mdev_udata_queue_vmap(struct nvme_mdev_viommu *viommu,
 	if (!pages)
 		return ERR_PTR(-ENOMEM);
 
-	ret = cont ?
-		nvme_mdev_queue_getpages_contiguous(viommu, iova, pages, npages)
-		: nvme_mdev_queue_getpages_prplist(viommu, iova, pages, npages);
+	ret = cont ? nvme_mdev_queue_getpages_contiguous(viommu, iova, pages,
+							 npages) :
+			   nvme_mdev_queue_getpages_prplist(viommu, iova, pages,
+						      npages);
 
 	if (ret) {
 		map = ERR_PTR(ret);
 		goto out;
 	}
 
-	map =  vmap(pages, npages, VM_MAP, PAGE_KERNEL);
+	map = vmap(pages, npages, VM_MAP, PAGE_KERNEL);
 	if (!map)
 		map = ERR_PTR(-ENOMEM);
 out:
diff --git a/drivers/nvme/mdev/vcq.c b/drivers/nvme/mdev/vcq.c
index 7702137eb8bc..f53b53155aec 100644
--- a/drivers/nvme/mdev/vcq.c
+++ b/drivers/nvme/mdev/vcq.c
@@ -9,14 +9,19 @@
 #include "priv.h"
 
 /* Create new virtual completion queue */
-int nvme_mdev_vcq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
-		       dma_addr_t iova, bool cont, u16 size, int irq)
+int __nvme_mdev_vcq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
+			 dma_addr_t iova, bool cont, u16 size, int irq)
 {
-	struct nvme_vcq *q = &vctrl->vcqs[qid];
+	struct nvme_vcq *q;
 	int ret;
 
 	lockdep_assert_held(&vctrl->lock);
 
+	q = kzalloc(sizeof(*q), GFP_KERNEL);
+	if (!q)
+		return -ENOMEM;
+
+	q->vctrl = vctrl;
 	q->iova = iova;
 	q->cont = cont;
 	q->data = NULL;
@@ -28,51 +33,58 @@ int nvme_mdev_vcq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
 	q->pending = 0;
 	q->head = 0;
 
-	ret = nvme_mdev_vcq_viommu_update(&vctrl->viommu, q);
+	ret = __nvme_mdev_vcq_viommu_update(&vctrl->viommu, q);
 	if (ret && (ret != -EFAULT))
-		return ret;
+		goto fail;
 
-	_DBG(vctrl, "VCQ: create qid=%d contig=%d depth=%d irq=%d\n",
-	     qid, cont, size, irq);
+	_DBG(vctrl, "VCQ: create qid=%d contig=%d depth=%d irq=%d\n", qid, cont,
+	     size, irq);
 
-	set_bit(qid, vctrl->vcq_en);
+	rcu_assign_pointer(vctrl->vcqs[qid], q);
 
 	vctrl->mmio.dbs[q->qid].cqh = 0;
 	vctrl->mmio.eidxs[q->qid].cqh = 0;
 	return 0;
+
+fail:
+	kfree(q);
+	return ret;
 }
 
 /* Update the kernel mapping of the queue */
-int nvme_mdev_vcq_viommu_update(struct nvme_mdev_viommu *viommu,
-				struct nvme_vcq *q)
+int __nvme_mdev_vcq_viommu_update(struct nvme_mdev_viommu *viommu,
+				  struct nvme_vcq *q)
 {
 	void *data;
 
 	if (q->data)
 		vunmap((void *)q->data);
 
-	data = nvme_mdev_udata_queue_vmap(viommu, q->iova,
-					  (unsigned int)q->size *
-					  sizeof(struct nvme_completion),
-					  q->cont);
+	data = nvme_mdev_udata_queue_vmap(
+		viommu, q->iova,
+		(unsigned int)q->size * sizeof(struct nvme_completion),
+		q->cont);
 
 	q->data = IS_ERR(data) ? NULL : data;
 	return IS_ERR(data) ? PTR_ERR(data) : 0;
 }
 
 /* Delete a virtual completion queue */
-void nvme_mdev_vcq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid)
+void __nvme_mdev_vcq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid)
 {
-	struct nvme_vcq *q = &vctrl->vcqs[qid];
+	struct nvme_vcq *q;
 
-	lockdep_assert_held(&vctrl->lock);
+	q = rcu_replace_pointer(vctrl->vcqs[qid], NULL,
+				lockdep_is_held(&vctrl->lock));
+	_DBG(vctrl, "VCQ: delete qid=%d\n", q->qid);
+
+	synchronize_rcu();
 
 	if (q->data)
 		vunmap((void *)q->data);
 	q->data = NULL;
 
-	_DBG(vctrl, "VCQ: delete qid=%d\n", q->qid);
-	clear_bit(qid, vctrl->vcq_en);
+	kfree(q);
 }
 
 /* Move queue tail one item forward */
@@ -93,17 +105,16 @@ static void nvme_mdev_vcq_advance_head(struct nvme_vcq *q)
 }
 
 /* Process a virtual completion queue*/
-void nvme_mdev_vcq_process(struct nvme_mdev_vctrl *vctrl, u16 qid,
+void nvme_mdev_vcq_process(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q,
 			   bool trigger_irqs)
 {
-	struct nvme_vcq *q = &vctrl->vcqs[qid];
 	u16 new_head;
 	u32 eidx;
 
 	if (!vctrl->mmio.dbs || !vctrl->mmio.eidxs)
 		return;
 
-	new_head = le32_to_cpu(vctrl->mmio.dbs[qid].cqh);
+	new_head = le32_to_cpu(vctrl->mmio.dbs[q->qid].cqh);
 
 	if (new_head != q->head) {
 		/* bad tail - can't process*/
@@ -132,10 +143,9 @@ void nvme_mdev_vcq_process(struct nvme_mdev_vctrl *vctrl, u16 qid,
 }
 
 /* flush interrupts on a completion queue */
-bool nvme_mdev_vcq_flush(struct nvme_mdev_vctrl *vctrl, u16 qid)
+bool nvme_mdev_vcq_flush(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q)
 {
-	struct nvme_vcq *q = &vctrl->vcqs[qid];
-	u16 new_head = le32_to_cpu(vctrl->mmio.dbs[qid].cqh);
+	u16 new_head = le32_to_cpu(vctrl->mmio.dbs[q->qid].cqh);
 
 	if (new_head == q->tail || q->irq == -1)
 		return false;
@@ -165,18 +175,14 @@ bool nvme_mdev_vcq_reserve_space(struct nvme_vcq *q)
 }
 
 /* Write a new item into the completion queue (IO version) */
-void nvme_mdev_vcq_write_io(struct nvme_mdev_vctrl *vctrl,
-			    struct nvme_vcq *q, u16 sq_head,
-			    u16 sqid, u16 cid, u16 status)
+void nvme_mdev_vcq_write_io(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q,
+			    u16 sq_head, u16 sqid, u16 cid, u16 status)
 {
 	volatile u64 *qw = (__le64 *)(&q->data[q->tail]);
 
 	u64 phase = q->phase ? (0x1ULL << 48) : 0;
-	u64 qw1 =
-		((u64)sq_head) |
-		((u64)sqid << 16) |
-		((u64)cid << 32) |
-		((u64)status << 49) | phase;
+	u64 qw1 = ((u64)sq_head) | ((u64)sqid << 16) | ((u64)cid << 32) |
+		  ((u64)status << 49) | phase;
 
 	WRITE_ONCE(qw[1], cpu_to_le64(qw1));
 
@@ -186,17 +192,14 @@ void nvme_mdev_vcq_write_io(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Write a new item into the completion queue (ADMIN version) */
-void nvme_mdev_vcq_write_adm(struct nvme_mdev_vctrl *vctrl,
-			     struct nvme_vcq *q, u32 dw0,
-			     u16 sq_head, u16 cid, u16 status)
+void nvme_mdev_vcq_write_adm(struct nvme_mdev_vctrl *vctrl, struct nvme_vcq *q,
+			     u32 dw0, u16 sq_head, u16 cid, u16 status)
 {
 	volatile u64 *qw = (__le64 *)(&q->data[q->tail]);
 
 	u64 phase = q->phase ? (0x1ULL << 48) : 0;
 	u64 qw1 =
-		((u64)sq_head) |
-		((u64)cid << 32) |
-		((u64)status << 49) | phase;
+		((u64)sq_head) | ((u64)cid << 32) | ((u64)status << 49) | phase;
 
 	WRITE_ONCE(qw[0], cpu_to_le64(dw0));
 	/* ensure that hardware sees the phase bit flip last */
diff --git a/drivers/nvme/mdev/vctrl.c b/drivers/nvme/mdev/vctrl.c
index d23b543dfd52..4fc321de9c78 100644
--- a/drivers/nvme/mdev/vctrl.c
+++ b/drivers/nvme/mdev/vctrl.c
@@ -9,6 +9,24 @@
 #include <linux/mdev.h>
 #include <linux/nvme.h>
 #include "priv.h"
+#include "iothreads.h"
+
+#ifdef CONFIG_NVME_MDEV_BPF
+int nvme_mdev_vctrl_attach_prog(struct nvme_mdev_vctrl *vctrl,
+				struct bpf_prog *prog)
+{
+	struct bpf_prog *old;
+
+	mutex_lock(&vctrl->lock);
+	old = rcu_replace_pointer(vctrl->prog, prog,
+				  lockdep_is_held(&vctrl->lock));
+	mutex_unlock(&vctrl->lock);
+	synchronize_rcu();
+	if (old)
+		bpf_prog_put(old);
+	return 0;
+}
+#endif
 
 bool nvme_mdev_vctrl_is_dead(struct nvme_mdev_vctrl *vctrl)
 {
@@ -18,7 +36,7 @@ bool nvme_mdev_vctrl_is_dead(struct nvme_mdev_vctrl *vctrl)
 /* Setup the controller guid and serial */
 static void nvme_mdev_vctrl_init_id(struct nvme_mdev_vctrl *vctrl)
 {
-	const guid_t* guid = mdev_uuid(vctrl->mdev);
+	const guid_t *guid = mdev_uuid(vctrl->mdev);
 
 	snprintf(vctrl->subnqn, sizeof(vctrl->subnqn),
 		 "nqn.2014-08.org.nvmexpress:uuid:%pUl", guid->b);
@@ -26,33 +44,18 @@ static void nvme_mdev_vctrl_init_id(struct nvme_mdev_vctrl *vctrl)
 	snprintf(vctrl->serial, sizeof(vctrl->serial), "%pUl", guid->b);
 }
 
-/* Change the IO thread CPU pinning */
-void nvme_mdev_vctrl_bind_iothread(struct nvme_mdev_vctrl *vctrl,
-				   unsigned int cpu)
-{
-	mutex_lock(&vctrl->lock);
-
-	if (cpu == vctrl->iothread_cpu)
-		goto out;
-
-	nvme_mdev_io_free(vctrl);
-	nvme_mdev_io_create(vctrl, cpu);
-out:
-	mutex_unlock(&vctrl->lock);
-}
-
 /* Change the status of support for shadow doorbell */
 int nvme_mdev_vctrl_set_shadow_doorbell_supported(struct nvme_mdev_vctrl *vctrl,
 						  bool enable)
 {
-	if (vctrl->inuse)
+	if (atomic_read(&vctrl->inuse))
 		return -EBUSY;
 	vctrl->mmio.shadow_db_supported = enable;
 	return 0;
 }
 
 /* Called when memory mapping are changed. Propagate this to all kmap users */
-static void nvme_mdev_vctrl_viommu_update(struct nvme_mdev_vctrl *vctrl)
+static void __nvme_mdev_vctrl_viommu_update(struct nvme_mdev_vctrl *vctrl)
 {
 	u16 qid;
 
@@ -62,14 +65,79 @@ static void nvme_mdev_vctrl_viommu_update(struct nvme_mdev_vctrl *vctrl)
 		return;
 
 	/* update mappings for submission and completion queues */
-	for_each_set_bit(qid, vctrl->vsq_en, MAX_VIRTUAL_QUEUES)
-		nvme_mdev_vsq_viommu_update(&vctrl->viommu, &vctrl->vsqs[qid]);
+	for (qid = 0; qid < MAX_VIRTUAL_QUEUES; qid++) {
+		struct nvme_vsq *vsq =
+			rcu_dereference_protected(vctrl->vsqs[qid], 1);
+		if (vsq)
+			__nvme_mdev_vsq_viommu_update(&vctrl->viommu, vsq);
+	}
 
-	for_each_set_bit(qid, vctrl->vcq_en, MAX_VIRTUAL_QUEUES)
-		nvme_mdev_vcq_viommu_update(&vctrl->viommu, &vctrl->vcqs[qid]);
+	for (qid = 0; qid < MAX_VIRTUAL_QUEUES; qid++) {
+		struct nvme_vcq *vcq =
+			rcu_dereference_protected(vctrl->vcqs[qid], 1);
+		if (vcq)
+			__nvme_mdev_vcq_viommu_update(&vctrl->viommu, vcq);
+	}
 
 	/* update mapping for the shadow doorbells */
-	nvme_mdev_mmio_viommu_update(vctrl);
+	__nvme_mdev_mmio_viommu_update(vctrl);
+}
+
+void __nvme_mdev_vctrl_get(struct nvme_mdev_vctrl *vctrl)
+{
+	lockdep_assert_held(&vctrl->lock);
+	atomic_inc(&vctrl->inuse);
+	_DBG(vctrl, "get -> %d\n", atomic_read(&vctrl->inuse));
+}
+
+void __nvme_mdev_vctrl_put(struct nvme_mdev_vctrl *vctrl)
+{
+	int ret;
+
+	lockdep_assert_held(&vctrl->lock);
+	if (atomic_dec_and_test(&vctrl->inuse)) {
+		mutex_unlock(&vctrl->lock);
+		ret = vfio_unregister_notifier(mdev_dev(vctrl->mdev),
+					       VFIO_IOMMU_NOTIFY,
+					       &vctrl->vfio_unmap_notifier);
+		WARN_ON(ret);
+
+		ret = vfio_unregister_notifier(mdev_dev(vctrl->mdev),
+					       VFIO_IOMMU_NOTIFY,
+					       &vctrl->vfio_map_notifier);
+		WARN_ON(ret);
+		mutex_lock(&vctrl->lock);
+
+		__nvme_mdev_io_pause(vctrl);
+
+		/* Remove the guest DMA mappings - new user that will open the
+		* device might be a different guest
+		*/
+		nvme_mdev_viommu_reset(&vctrl->viommu);
+
+		/* Reset the controller to a clean state for a new user */
+		__nvme_mdev_vctrl_reset(vctrl, false);
+		__nvme_mdev_irqs_reset(vctrl);
+
+		WARN_ON(!list_empty(&vctrl->host_hw_queues));
+
+		_INFO(vctrl, "device is released\n");
+		wake_up_var(&vctrl->inuse);
+	}
+	_DBG(vctrl, "put -> %d\n", atomic_read(&vctrl->inuse));
+}
+
+struct nvme_mdev_vctrl *nvme_mdev_vctrl_lookup(struct nvme_mdev_hctrl *hctrl)
+{
+	struct nvme_mdev_vctrl *vctrl, *tmp;
+
+	mutex_lock(&nvme_mdev_vctrl_list_mutex);
+	list_for_each_entry (tmp, &nvme_mdev_vctrl_list, link)
+		if (tmp->hctrl == hctrl)
+			vctrl = tmp;
+	mutex_unlock(&nvme_mdev_vctrl_list_mutex);
+
+	return vctrl;
 }
 
 /* Create a new virtual controller */
@@ -78,8 +146,8 @@ struct nvme_mdev_vctrl *nvme_mdev_vctrl_create(struct mdev_device *mdev,
 					       unsigned int max_host_queues)
 {
 	int ret;
-	struct nvme_mdev_vctrl *vctrl = kzalloc_node(sizeof(*vctrl),
-						     GFP_KERNEL, hctrl->node);
+	struct nvme_mdev_vctrl *vctrl =
+		kzalloc_node(sizeof(*vctrl), GFP_KERNEL, hctrl->node);
 	if (!vctrl)
 		return ERR_PTR(-ENOMEM);
 
@@ -89,10 +157,14 @@ struct nvme_mdev_vctrl *nvme_mdev_vctrl_create(struct mdev_device *mdev,
 	vctrl->max_host_hw_queues = max_host_queues;
 	vctrl->viommu.vctrl = vctrl;
 
-	kref_init(&vctrl->ref);
 	mutex_init(&vctrl->lock);
+	init_waitqueue_head(&vctrl->quiescent_wqh);
 	nvme_mdev_vctrl_init_id(vctrl);
 	INIT_LIST_HEAD(&vctrl->host_hw_queues);
+	atomic_set(&vctrl->inuse, 0);
+#ifdef CONFIG_NVME_MDEV_BPF
+	atomic_set(&vctrl->notifier_cnt, 0);
+#endif
 
 	get_device(mdev_dev(mdev));
 	mdev_set_drvdata(mdev, vctrl);
@@ -105,6 +177,7 @@ struct nvme_mdev_vctrl *nvme_mdev_vctrl_create(struct mdev_device *mdev,
 
 	/* default feature values*/
 	vctrl->arb_burst_shift = 3;
+	vctrl->arb_burst = 1 << vctrl->arb_burst_shift;
 	vctrl->mmio.shadow_db_supported = use_shadow_doorbell;
 
 	ret = nvme_mdev_pci_create(vctrl);
@@ -117,14 +190,13 @@ struct nvme_mdev_vctrl *nvme_mdev_vctrl_create(struct mdev_device *mdev,
 
 	nvme_mdev_irqs_setup(vctrl);
 
-	/* Create the IO thread */
-	/*TODOLATER: IO: smp_processor_id() is not an ideal pinning choice */
-	ret = nvme_mdev_io_create(vctrl, smp_processor_id());
+	ret = nvme_mdev_io_bind(vctrl, &iothreads[0], false);
 	if (ret)
 		goto error4;
 
 	_INFO(vctrl, "device created using %d host queues\n", max_host_queues);
 	return vctrl;
+
 error4:
 	nvme_mdev_mmio_free(vctrl);
 error3:
@@ -140,16 +212,33 @@ struct nvme_mdev_vctrl *nvme_mdev_vctrl_create(struct mdev_device *mdev,
 /*Try to destroy an vctrl */
 int nvme_mdev_vctrl_destroy(struct nvme_mdev_vctrl *vctrl)
 {
+	struct bpf_prog *old;
+
 	mutex_lock(&vctrl->lock);
 
-	if (vctrl->inuse) {
-		/* vctrl has mdev users */
+#ifdef CONFIG_NVME_MDEV_BPF
+	while (atomic_read(&vctrl->inuse) ||
+	       atomic_read(&vctrl->notifier_cnt)) {
 		mutex_unlock(&vctrl->lock);
-		return -EBUSY;
+		wait_var_event_interruptible(&vctrl->inuse,
+					     !atomic_read(&vctrl->inuse));
+		wait_var_event_interruptible(
+			&vctrl->notifier_cnt,
+			!atomic_read(&vctrl->notifier_cnt));
+		mutex_lock(&vctrl->lock);
+	}
+#else
+	while (atomic_read(&vctrl->inuse)) {
+		mutex_unlock(&vctrl->lock);
+		wait_var_event_interruptible(&vctrl->inuse,
+					     !atomic_read(&vctrl->inuse));
+		mutex_lock(&vctrl->lock);
 	}
+#endif
 
 	_INFO(vctrl, "device is destroying\n");
 
+	__nvme_mdev_io_pause(vctrl);
 	mdev_set_drvdata(vctrl->mdev, NULL);
 	mutex_unlock(&vctrl->lock);
 
@@ -157,9 +246,9 @@ int nvme_mdev_vctrl_destroy(struct nvme_mdev_vctrl *vctrl)
 	list_del_init(&vctrl->link);
 	mutex_unlock(&nvme_mdev_vctrl_list_mutex);
 
-	mutex_lock(&vctrl->lock); /*only for lockdep checks */
-	nvme_mdev_io_free(vctrl);
-	nvme_mdev_vns_destroy_all(vctrl);
+	mutex_lock(&vctrl->lock);
+	__nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_vns_destroy_all(vctrl);
 	__nvme_mdev_vctrl_reset(vctrl, true);
 
 	nvme_mdev_hctrl_hqs_unreserve(vctrl->hctrl, vctrl->max_host_hw_queues);
@@ -167,8 +256,21 @@ int nvme_mdev_vctrl_destroy(struct nvme_mdev_vctrl *vctrl)
 	nvme_mdev_pci_free(vctrl);
 	nvme_mdev_mmio_free(vctrl);
 
+#ifdef CONFIG_NVME_MDEV_BPF
+	old = rcu_replace_pointer(vctrl->prog, NULL,
+				  lockdep_is_held(&vctrl->lock));
+#endif
+
 	mutex_unlock(&vctrl->lock);
 
+#ifdef CONFIG_NVME_MDEV_BPF
+	synchronize_rcu();
+	if (old)
+		bpf_prog_put(old);
+#endif
+
+	nvme_mdev_io_unbind_all(vctrl);
+
 	put_device(mdev_dev(vctrl->mdev));
 	_INFO(vctrl, "device is destroyed\n");
 	kfree(vctrl);
@@ -184,7 +286,7 @@ void nvme_mdev_vctrl_pause(struct nvme_mdev_vctrl *vctrl)
 	if (!vctrl->vctrl_paused) {
 		_INFO(vctrl, "pausing the virtual controller\n");
 		if (vctrl->mmio.csts & NVME_CSTS_RDY)
-			nvme_mdev_io_pause(vctrl);
+			__nvme_mdev_io_pause(vctrl);
 		vctrl->vctrl_paused = true;
 	}
 	mutex_unlock(&vctrl->lock);
@@ -197,18 +299,18 @@ void nvme_mdev_vctrl_pause(struct nvme_mdev_vctrl *vctrl)
 void nvme_mdev_vctrl_resume(struct nvme_mdev_vctrl *vctrl)
 {
 	mutex_lock(&vctrl->lock);
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
 	if (vctrl->vctrl_paused) {
 		_INFO(vctrl, "resuming the virtual controller\n");
 
 		if (vctrl->mmio.csts & NVME_CSTS_RDY) {
 			/* handle all pending admin commands*/
-			nvme_mdev_adm_process_sq(vctrl);
+			__nvme_mdev_adm_process_sq(vctrl);
 			/* start the IO thread again if it was stopped or
 			 * if we had doorbell writes during the pause
 			 */
-			nvme_mdev_io_resume(vctrl);
+			__nvme_mdev_io_resume(vctrl);
 		}
 		vctrl->vctrl_paused = false;
 	}
@@ -228,11 +330,13 @@ int nvme_mdev_vctrl_open(struct nvme_mdev_vctrl *vctrl)
 		goto out;
 	}
 
-	if (vctrl->inuse) {
-		ret = -EBUSY;
+	__nvme_mdev_vctrl_get(vctrl);
+	/* we're locked, this is safe */
+	if (atomic_read(&vctrl->inuse) > 1) {
 		goto out;
 	}
 
+	/* only do all of this on first open */
 	_INFO(vctrl, "device is opened\n");
 
 	if (vctrl->hctrl->nvme_ctrl->ops->flags & NVME_F_MDEV_DMA_SUPPORTED)
@@ -241,7 +345,6 @@ int nvme_mdev_vctrl_open(struct nvme_mdev_vctrl *vctrl)
 	nvme_mdev_viommu_init(&vctrl->viommu, mdev_dev(vctrl->mdev), dma_dev);
 
 	nvme_mdev_mmio_open(vctrl);
-	vctrl->inuse = true;
 
 	memset(&vctrl->perf, 0, sizeof(vctrl->perf));
 
@@ -254,28 +357,8 @@ int nvme_mdev_vctrl_open(struct nvme_mdev_vctrl *vctrl)
 void nvme_mdev_vctrl_release(struct nvme_mdev_vctrl *vctrl)
 {
 	mutex_lock(&vctrl->lock);
-	nvme_mdev_io_pause(vctrl);
-
-	/* Remove the guest DMA mappings - new user that will open the
-	 * device might be a different guest
-	 */
-	nvme_mdev_viommu_reset(&vctrl->viommu);
-
-	/* Reset the controller to a clean state for a new user */
-	__nvme_mdev_vctrl_reset(vctrl, false);
-	nvme_mdev_irqs_reset(vctrl);
-	vctrl->inuse = false;
+	__nvme_mdev_vctrl_put(vctrl);
 	mutex_unlock(&vctrl->lock);
-
-	WARN_ON(!list_empty(&vctrl->host_hw_queues));
-
-	_INFO(vctrl, "device is released\n");
-
-	/* If we are released after request to remove the host controller
-	 * we are dead, won't be opened again ever, so remove ourselves
-	 */
-	if (vctrl->hctrl->removing)
-		nvme_mdev_vctrl_destroy(vctrl);
 }
 
 /* Called each time the controller is reset (CC.EN <= 0 or VM level reset) */
@@ -286,20 +369,20 @@ void __nvme_mdev_vctrl_reset(struct nvme_mdev_vctrl *vctrl, bool pci_reset)
 	if ((vctrl->mmio.csts & NVME_CSTS_RDY) &&
 	    !(vctrl->mmio.csts & NVME_CSTS_SHST_MASK)) {
 		_DBG(vctrl, "unsafe reset (CSTS.RDY==1)\n");
-		nvme_mdev_io_pause(vctrl);
-		nvme_mdev_vctrl_disable(vctrl);
+		__nvme_mdev_io_pause(vctrl);
+		__nvme_mdev_vctrl_disable(vctrl);
 	}
 	nvme_mdev_mmio_reset(vctrl, pci_reset);
 }
 
 /* setups initial admin queues and doorbells */
-bool nvme_mdev_vctrl_enable(struct nvme_mdev_vctrl *vctrl,
-			    dma_addr_t cqiova, dma_addr_t sqiova, u32 sizes)
+bool __nvme_mdev_vctrl_enable(struct nvme_mdev_vctrl *vctrl, dma_addr_t cqiova,
+			      dma_addr_t sqiova, u32 sizes)
 {
 	int ret;
 	u16 cqentries, sqentries;
 
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
 	lockdep_assert_held(&vctrl->lock);
 
@@ -311,15 +394,15 @@ bool nvme_mdev_vctrl_enable(struct nvme_mdev_vctrl *vctrl,
 	if (sqentries > 4096 || sqentries < 2)
 		return false;
 
-	ret = nvme_mdev_mmio_enable_dbs(vctrl);
+	ret = __nvme_mdev_mmio_enable_dbs(vctrl);
 	if (ret)
 		goto error0;
 
-	ret = nvme_mdev_vcq_init(vctrl, 0, cqiova, true, cqentries, 0);
+	ret = __nvme_mdev_vcq_init(vctrl, 0, cqiova, true, cqentries, 0);
 	if (ret)
 		goto error1;
 
-	ret = nvme_mdev_vsq_init(vctrl, 0, sqiova, true, sqentries, 0);
+	ret = __nvme_mdev_vsq_init(vctrl, 0, sqiova, true, sqentries, 0);
 	if (ret)
 		goto error2;
 
@@ -328,42 +411,48 @@ bool nvme_mdev_vctrl_enable(struct nvme_mdev_vctrl *vctrl,
 	if (!vctrl->mmio.shadow_db_supported) {
 		/* start polling right away to support admin queue */
 		vctrl->io_idle = false;
-		nvme_mdev_io_resume(vctrl);
+		__nvme_mdev_io_resume(vctrl);
 	}
 
 	return true;
 error2:
-	nvme_mdev_mmio_disable_dbs(vctrl);
+	__nvme_mdev_mmio_disable_dbs(vctrl);
 error1:
-	nvme_mdev_vcq_delete(vctrl, 0);
+	__nvme_mdev_vcq_delete(vctrl, 0);
 error0:
 	return false;
 }
 
 /* destroy all io/admin queues on the controller  */
-void nvme_mdev_vctrl_disable(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_vctrl_disable(struct nvme_mdev_vctrl *vctrl)
 {
 	u16 sqid, cqid;
 
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
 	lockdep_assert_held(&vctrl->lock);
 
 	nvme_mdev_events_reset(vctrl);
 	nvme_mdev_vns_log_reset(vctrl);
 
-	sqid = 1;
-	for_each_set_bit_from(sqid, vctrl->vsq_en, MAX_VIRTUAL_QUEUES)
-		nvme_mdev_vsq_delete(vctrl, sqid);
+	for (sqid = 1; sqid < MAX_VIRTUAL_QUEUES; sqid++) {
+		struct nvme_vsq *vsq = rcu_dereference_protected(
+			vctrl->vsqs[sqid], lockdep_is_held(&vctrl->lock));
+		if (vsq)
+			__nvme_mdev_vsq_delete(vctrl, sqid);
+	}
 
-	cqid = 1;
-	for_each_set_bit_from(cqid, vctrl->vcq_en, MAX_VIRTUAL_QUEUES)
-		nvme_mdev_vcq_delete(vctrl, cqid);
+	for (cqid = 1; cqid < MAX_VIRTUAL_QUEUES; cqid++) {
+		struct nvme_vcq *vcq = rcu_dereference_protected(
+			vctrl->vcqs[cqid], lockdep_is_held(&vctrl->lock));
+		if (vcq)
+			__nvme_mdev_vcq_delete(vctrl, cqid);
+	}
 
-	nvme_mdev_vsq_delete(vctrl, 0);
-	nvme_mdev_vcq_delete(vctrl, 0);
+	__nvme_mdev_vsq_delete(vctrl, 0);
+	__nvme_mdev_vcq_delete(vctrl, 0);
 
-	nvme_mdev_mmio_disable_dbs(vctrl);
+	__nvme_mdev_mmio_disable_dbs(vctrl);
 	vctrl->io_idle = true;
 }
 
@@ -390,8 +479,7 @@ void nvme_mdev_vctrl_add_region(struct nvme_mdev_vctrl *vctrl,
 
 /* Enable mmap window on an IO region */
 void nvme_mdev_vctrl_region_set_mmap(struct nvme_mdev_vctrl *vctrl,
-				     unsigned int index,
-				     unsigned int offset,
+				     unsigned int index, unsigned int offset,
 				     unsigned int size,
 				     const struct vm_operations_struct *ops)
 {
@@ -414,16 +502,16 @@ void nvme_mdev_vctrl_region_disable_mmap(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Allocate a host IO queue */
-int nvme_mdev_vctrl_hq_alloc(struct nvme_mdev_vctrl *vctrl)
+int __nvme_mdev_vctrl_hq_alloc(struct nvme_mdev_vctrl *vctrl)
 {
 	struct nvme_mdev_hq *hq = NULL, *tmp;
 	int hwqcount = 0, ret;
 
 	lockdep_assert_held(&vctrl->lock);
 
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
-	list_for_each_entry(tmp, &vctrl->host_hw_queues, link) {
+	list_for_each_entry (tmp, &vctrl->host_hw_queues, link) {
 		if (!hq || tmp->usecount < hq->usecount)
 			hq = tmp;
 		hwqcount++;
@@ -450,14 +538,14 @@ int nvme_mdev_vctrl_hq_alloc(struct nvme_mdev_vctrl *vctrl)
 }
 
 /* Free a host IO queue */
-void nvme_mdev_vctrl_hq_free(struct nvme_mdev_vctrl *vctrl, u16 hqid)
+void __nvme_mdev_vctrl_hq_free(struct nvme_mdev_vctrl *vctrl, u16 hqid)
 {
 	struct nvme_mdev_hq *hq;
 
 	lockdep_assert_held(&vctrl->lock);
-	nvme_mdev_assert_io_not_running(vctrl);
+	__nvme_mdev_assert_io_not_running(vctrl);
 
-	list_for_each_entry(hq, &vctrl->host_hw_queues, link)
+	list_for_each_entry (hq, &vctrl->host_hw_queues, link)
 		if (hq->hqid == hqid) {
 			if (--hq->usecount > 0)
 				return;
@@ -475,7 +563,7 @@ unsigned int nvme_mdev_vctrl_hqs_list(struct nvme_mdev_vctrl *vctrl, u16 *out)
 	struct nvme_mdev_hq *q;
 	unsigned int i = 0;
 
-	list_for_each_entry(q, &vctrl->host_hw_queues, link) {
+	list_for_each_entry (q, &vctrl->host_hw_queues, link) {
 		out[i++] = q->hqid;
 		if (WARN_ON(i > MAX_HOST_QUEUES))
 			break;
@@ -491,27 +579,27 @@ int nvme_mdev_vctrl_viommu_map(struct nvme_mdev_vctrl *vctrl, u32 flags,
 
 	mutex_lock(&vctrl->lock);
 
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 	ret = nvme_mdev_viommu_add(&vctrl->viommu, flags, iova, size);
-	nvme_mdev_vctrl_viommu_update(vctrl);
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_vctrl_viommu_update(vctrl);
+	__nvme_mdev_io_resume(vctrl);
 
 	mutex_unlock(&vctrl->lock);
 	return ret;
 }
 
 /* remove a user memory mapping */
-int nvme_mdev_vctrl_viommu_unmap(struct nvme_mdev_vctrl *vctrl,
-				 dma_addr_t iova, u64 size)
+int nvme_mdev_vctrl_viommu_unmap(struct nvme_mdev_vctrl *vctrl, dma_addr_t iova,
+				 u64 size)
 {
 	int ret;
 
 	mutex_lock(&vctrl->lock);
 
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 	ret = nvme_mdev_viommu_remove(&vctrl->viommu, iova, size);
-	nvme_mdev_vctrl_viommu_update(vctrl);
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_vctrl_viommu_update(vctrl);
+	__nvme_mdev_io_resume(vctrl);
 
 	mutex_unlock(&vctrl->lock);
 	return ret;
diff --git a/drivers/nvme/mdev/viommu.c b/drivers/nvme/mdev/viommu.c
index 31b86e8f5768..4f561e72af09 100644
--- a/drivers/nvme/mdev/viommu.c
+++ b/drivers/nvme/mdev/viommu.c
@@ -21,10 +21,10 @@ struct mem_mapping {
 
 	dma_addr_t __subtree_last;
 	dma_addr_t iova_start; /* first iova in this mapping*/
-	dma_addr_t iova_last;  /* last iova in this mapping*/
+	dma_addr_t iova_last; /* last iova in this mapping*/
 
-	unsigned long pfn;  /* physical address of this mapping */
-	dma_addr_t host_iova;  /* dma mapping to the real device*/
+	unsigned long pfn; /* physical address of this mapping */
+	dma_addr_t host_iova; /* dma mapping to the real device*/
 };
 
 #define map_len(m) (((m)->iova_last - (m)->iova_start) + 1ULL)
@@ -32,20 +32,19 @@ struct mem_mapping {
 #define START(node) ((node)->iova_start)
 #define LAST(node) ((node)->iova_last)
 
-INTERVAL_TREE_DEFINE(struct mem_mapping, rb, dma_addr_t, __subtree_last,
-		     START, LAST, static inline, viommu_int_tree);
+INTERVAL_TREE_DEFINE(struct mem_mapping, rb, dma_addr_t, __subtree_last, START,
+		     LAST, static inline, viommu_int_tree);
 
 static void nvme_mdev_viommu_dbg_dma_range(struct nvme_mdev_viommu *viommu,
 					   struct mem_mapping *map,
 					   const char *action)
 {
-	dma_addr_t iova_start  = map->iova_start;
-	dma_addr_t iova_end    = map->iova_start + map_len(map) - 1;
+	dma_addr_t iova_start = map->iova_start;
+	dma_addr_t iova_end = map->iova_start + map_len(map) - 1;
 	dma_addr_t hiova_start = map->host_iova;
-	dma_addr_t hiova_end   = map->host_iova  + map_len(map) - 1;
+	dma_addr_t hiova_end = map->host_iova + map_len(map) - 1;
 
-	_DBG(viommu->vctrl,
-	     "vIOMMU: %s RW IOVA %pad-%pad -> DMA %pad-%pad\n",
+	_DBG(viommu->vctrl, "vIOMMU: %s RW IOVA %pad-%pad -> DMA %pad-%pad\n",
 	     action, &iova_start, &iova_end, &hiova_start, &hiova_end);
 }
 
@@ -56,7 +55,7 @@ static void nvme_mdev_viommu_unpin_pages(struct nvme_mdev_viommu *viommu,
 	int i;
 
 	for (i = 0; i < n; i++) {
-		unsigned long  user_pfn = (iova >> PAGE_SHIFT) + i;
+		unsigned long user_pfn = (iova >> PAGE_SHIFT) + i;
 		int ret = vfio_unpin_pages(viommu->sw_dev, &user_pfn, 1);
 
 		WARN_ON(ret != 1);
@@ -65,8 +64,7 @@ static void nvme_mdev_viommu_unpin_pages(struct nvme_mdev_viommu *viommu,
 
 /* User memory init code*/
 void nvme_mdev_viommu_init(struct nvme_mdev_viommu *viommu,
-			   struct device *sw_dev,
-			   struct device *hw_dev)
+			   struct device *sw_dev, struct device *hw_dev)
 {
 	viommu->sw_dev = sw_dev;
 	viommu->hw_dev = hw_dev;
@@ -82,10 +80,8 @@ void nvme_mdev_viommu_reset(struct nvme_mdev_viommu *viommu)
 }
 
 /* Adds a new range of user memory*/
-int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu,
-			 u32 flags,
-			 dma_addr_t iova,
-			 u64 size)
+int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu, u32 flags,
+			 dma_addr_t iova, u64 size)
 {
 	u64 offset;
 	dma_addr_t iova_end = iova + size - 1;
@@ -102,8 +98,8 @@ int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu,
 		else if (flags & VFIO_DMA_MAP_FLAG_WRITE)
 			type = "WO";
 
-		_DBG(viommu->vctrl, "vIOMMU: IGN %s IOVA %pad-%pad\n",
-		     type, &iova, &iova_end);
+		_DBG(viommu->vctrl, "vIOMMU: IGN %s IOVA %pad-%pad\n", type,
+		     &iova, &iova_end);
 		return 0;
 	}
 
@@ -116,10 +112,9 @@ int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu,
 	for (offset = 0; offset < size; offset += PAGE_SIZE) {
 		unsigned long vapfn = ((iova + offset) >> PAGE_SHIFT), pa_pfn;
 
-		ret = vfio_pin_pages(viommu->sw_dev,
-				     &vapfn, 1,
+		ret = vfio_pin_pages(viommu->sw_dev, &vapfn, 1,
 				     VFIO_DMA_MAP_FLAG_READ |
-				     VFIO_DMA_MAP_FLAG_WRITE,
+					     VFIO_DMA_MAP_FLAG_WRITE,
 				     &pa_pfn);
 
 		if (ret != 1) {
@@ -127,15 +122,15 @@ int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu,
 			ret = -EFAULT;
 
 			_DBG(viommu->vctrl,
-			     "vIOMMU: ADD RW IOVA %pad - pin failed\n",
-			     &iova);
+			     "vIOMMU: ADD RW IOVA %pad - pin failed\n", &iova);
 			goto unwind;
 		}
 
 		// new mapping needed
 		if (!map || map->pfn + map_pages(map) != pa_pfn) {
 			int node = viommu->hw_dev ?
-				dev_to_node(viommu->hw_dev) : NUMA_NO_NODE;
+						 dev_to_node(viommu->hw_dev) :
+						 NUMA_NO_NODE;
 
 			map = kzalloc_node(sizeof(*map), GFP_KERNEL, node);
 
@@ -156,14 +151,12 @@ int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu,
 	}
 
 	// DMA mapping the pages
-	list_for_each_entry_safe(map, tmp, &new_mappings_list, link) {
+	list_for_each_entry_safe (map, tmp, &new_mappings_list, link) {
 		if (viommu->hw_dev) {
 			map->host_iova =
 				dma_map_page(viommu->hw_dev,
-					     pfn_to_page(map->pfn),
-					     0,
-					     map_len(map),
-					     DMA_BIDIRECTIONAL);
+					     pfn_to_page(map->pfn), 0,
+					     map_len(map), DMA_BIDIRECTIONAL);
 
 			ret = dma_mapping_error(viommu->hw_dev, map->host_iova);
 			if (ret) {
@@ -181,7 +174,7 @@ int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu,
 	}
 	return 0;
 unwind:
-	list_for_each_entry_safe(map, tmp, &new_mappings_list, link) {
+	list_for_each_entry_safe (map, tmp, &new_mappings_list, link) {
 		nvme_mdev_viommu_unpin_pages(viommu, map->iova_start,
 					     map_pages(map));
 
@@ -193,12 +186,11 @@ int nvme_mdev_viommu_add(struct nvme_mdev_viommu *viommu,
 }
 
 /* Removes a  range of user memory*/
-int nvme_mdev_viommu_remove(struct nvme_mdev_viommu *viommu,
-			    dma_addr_t iova,
+int nvme_mdev_viommu_remove(struct nvme_mdev_viommu *viommu, dma_addr_t iova,
 			    u64 size)
 {
 	struct mem_mapping *map = NULL, *tmp;
-	dma_addr_t last_iova = iova + (size) - 1ULL;
+	dma_addr_t last_iova = iova + (size)-1ULL;
 	LIST_HEAD(remove_list);
 	int count = 0;
 
@@ -211,7 +203,7 @@ int nvme_mdev_viommu_remove(struct nvme_mdev_viommu *viommu,
 	}
 
 	/* remove them */
-	list_for_each_entry_safe(map, tmp, &remove_list, link) {
+	list_for_each_entry_safe (map, tmp, &remove_list, link) {
 		count++;
 
 		nvme_mdev_viommu_dbg_dma_range(viommu, map, "DEL");
@@ -229,10 +221,8 @@ int nvme_mdev_viommu_remove(struct nvme_mdev_viommu *viommu,
 }
 
 /* Translate an IOVA to a physical address and read device bus address */
-int nvme_mdev_viommu_translate(struct nvme_mdev_viommu *viommu,
-			       dma_addr_t iova,
-			       dma_addr_t *physical,
-			       dma_addr_t *host_iova)
+int nvme_mdev_viommu_translate(struct nvme_mdev_viommu *viommu, dma_addr_t iova,
+			       dma_addr_t *physical, dma_addr_t *host_iova)
 {
 	struct mem_mapping *mapping;
 	u64 offset;
@@ -240,11 +230,11 @@ int nvme_mdev_viommu_translate(struct nvme_mdev_viommu *viommu,
 	if (WARN_ON_ONCE(OFFSET_IN_PAGE(iova) != 0))
 		return -EINVAL;
 
-	mapping = viommu_int_tree_iter_first(&viommu->maps_tree,
-					     iova, iova + PAGE_SIZE - 1);
+	mapping = viommu_int_tree_iter_first(&viommu->maps_tree, iova,
+					     iova + PAGE_SIZE - 1);
 	if (!mapping) {
-		_DBG(viommu->vctrl,
-		     "vIOMMU: translation of IOVA %pad failed\n", &iova);
+		_DBG(viommu->vctrl, "vIOMMU: translation of IOVA %pad failed\n",
+		     &iova);
 		return -EFAULT;
 	}
 
@@ -259,7 +249,8 @@ int nvme_mdev_viommu_translate(struct nvme_mdev_viommu *viommu,
 
 /* map an IOVA to kernel address space  */
 int nvme_mdev_viommu_create_kmap(struct nvme_mdev_viommu *viommu,
-				 dma_addr_t iova, struct page_map *page)
+				 dma_addr_t iova, struct page_map *page,
+				 bool global)
 {
 	dma_addr_t host_iova;
 	phys_addr_t physical;
@@ -274,7 +265,10 @@ int nvme_mdev_viommu_create_kmap(struct nvme_mdev_viommu *viommu,
 
 	new_page = pfn_to_page(PHYS_PFN(physical));
 
-	page->kmap = kmap(new_page);
+	if (global)
+		page->kmap = kmap(new_page);
+	else
+		page->kmap = kmap_atomic(new_page);
 	if (!page->kmap)
 		return -ENOMEM;
 
@@ -284,17 +278,17 @@ int nvme_mdev_viommu_create_kmap(struct nvme_mdev_viommu *viommu,
 
 /* update IOVA <-> kernel mapping. If fails, removes the previous mapping */
 void nvme_mdev_viommu_update_kmap(struct nvme_mdev_viommu *viommu,
-				  struct page_map *page)
+				  struct page_map *page, bool global)
 {
 	dma_addr_t host_iova;
 	phys_addr_t physical;
 	struct page *new_page;
 	int ret;
 
-	ret = nvme_mdev_viommu_translate(viommu, page->iova,
-					 &physical, &host_iova);
+	ret = nvme_mdev_viommu_translate(viommu, page->iova, &physical,
+					 &host_iova);
 	if (ret) {
-		nvme_mdev_viommu_free_kmap(viommu, page);
+		nvme_mdev_viommu_free_kmap(viommu, page, global);
 		return;
 	}
 
@@ -302,9 +296,12 @@ void nvme_mdev_viommu_update_kmap(struct nvme_mdev_viommu *viommu,
 	if (new_page == page->page)
 		return;
 
-	nvme_mdev_viommu_free_kmap(viommu, page);
+	nvme_mdev_viommu_free_kmap(viommu, page, global);
 
-	page->kmap = kmap(new_page);
+	if (global)
+		page->kmap = kmap(new_page);
+	else
+		page->kmap = kmap_atomic(new_page);
 	if (!page->kmap)
 		return;
 	page->page = new_page;
@@ -312,10 +309,13 @@ void nvme_mdev_viommu_update_kmap(struct nvme_mdev_viommu *viommu,
 
 /* unmap an IOVA to kernel address space  */
 void nvme_mdev_viommu_free_kmap(struct nvme_mdev_viommu *viommu,
-				struct page_map *page)
+				struct page_map *page, bool global)
 {
 	if (page->page) {
-		kunmap(page->page);
+		if (global)
+			kunmap(page->page);
+		else
+			kunmap_atomic(page->kmap);
 		page->page = NULL;
 		page->kmap = NULL;
 	}
diff --git a/drivers/nvme/mdev/vns.c b/drivers/nvme/mdev/vns.c
index 42d4f8d7423b..2a764c21122a 100644
--- a/drivers/nvme/mdev/vns.c
+++ b/drivers/nvme/mdev/vns.c
@@ -15,7 +15,7 @@ void nvme_mdev_vns_log_reset(struct nvme_mdev_vctrl *vctrl)
 }
 
 /* This adds entry to NS changed log and sends to the user a notification */
-static void nvme_mdev_vns_send_event(struct nvme_mdev_vctrl *vctrl, u32 ns)
+static void __nvme_mdev_vns_send_event(struct nvme_mdev_vctrl *vctrl, u32 ns)
 {
 	unsigned int i;
 	unsigned int log_size = vctrl->ns_log_size;
@@ -42,9 +42,9 @@ static void nvme_mdev_vns_send_event(struct nvme_mdev_vctrl *vctrl, u32 ns)
 }
 
 /* Read host NS/partition parameters to update our virtual NS */
-static void nvme_mdev_vns_read_host_properties(struct nvme_mdev_vctrl *vctrl,
-					       struct nvme_mdev_vns *vns,
-					       struct nvme_ns *host_ns)
+static void __nvme_mdev_vns_read_host_properties(struct nvme_mdev_vctrl *vctrl,
+						 struct nvme_mdev_vns *vns,
+						 struct nvme_ns *host_ns)
 {
 	unsigned int sector_to_lba_shift;
 	u64 host_ns_size, start, nr, align_mask;
@@ -64,7 +64,7 @@ static void nvme_mdev_vns_read_host_properties(struct nvme_mdev_vctrl *vctrl,
 
 	/* read the partition start and size*/
 	start = get_start_sect(vns->host_part);
-	nr = part_nr_sects_read(vns->host_part->bd_part);
+	nr = vns->host_part->bd_part->nr_sects;
 
 	/* check that partition is aligned on LBA size*/
 	if (sector_to_lba_shift != 0) {
@@ -115,15 +115,16 @@ static void nvme_mdev_vns_read_host_properties(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Open new reference to a host namespace */
-int nvme_mdev_vns_open(struct nvme_mdev_vctrl *vctrl,
-		       u32 host_nsid, unsigned int host_partid)
+int nvme_mdev_vns_open(struct nvme_mdev_vctrl *vctrl, u32 host_nsid,
+		       unsigned int host_partid)
 {
 	struct nvme_mdev_vns *vns;
 	u32 user_nsid;
 	int ret;
+	char *devname;
 
-	_INFO(vctrl, "open host_namespace=%u, partition=%u\n",
-	      host_nsid, host_partid);
+	_INFO(vctrl, "open host_namespace=%u, partition=%u\n", host_nsid,
+	      host_partid);
 
 	mutex_lock(&vctrl->lock);
 	ret = -ENODEV;
@@ -154,24 +155,33 @@ int nvme_mdev_vns_open(struct nvme_mdev_vctrl *vctrl,
 		goto error2;
 	}
 
-	/* get the block device for the partition that we will use */
-	vns->host_part = bdget_disk(vns->host_ns->disk, host_partid);
-	if (!vns->host_part) {
-		ret = -ENODEV;
-		goto error2;
-	}
-
 	/* get exclusive access to the block device (partition) */
 	vns->fmode = FMODE_READ | FMODE_EXCL;
 	if (!vns->readonly)
 		vns->fmode |= FMODE_WRITE;
 
-	ret = blkdev_get(vns->host_part, vns->fmode, vns);
-	if (ret)
+	if (host_partid)
+		devname = kasprintf(GFP_KERNEL, "/dev/nvme%dn%up%u",
+				    vctrl->hctrl->nvme_ctrl->instance,
+				    vns->host_nsid, host_partid);
+	else
+		devname = kasprintf(GFP_KERNEL, "/dev/nvme%dn%u",
+				    vctrl->hctrl->nvme_ctrl->instance,
+				    vns->host_nsid);
+	if (!devname) {
+		ret = -ENOMEM;
 		goto error2;
+	}
+	vns->host_part = blkdev_get_by_path(devname, vns->fmode, vns);
+	kfree(devname);
+
+	if (IS_ERR(vns->host_part)) {
+		ret = PTR_ERR(vns->host_part);
+		goto error2;
+	}
 
 	/* read properties of the host namespace */
-	nvme_mdev_vns_read_host_properties(vctrl, vns, vns->host_ns);
+	__nvme_mdev_vns_read_host_properties(vctrl, vns, vns->host_ns);
 
 	/* Allocate a user namespace ID for this namespace */
 	ret = -ENOSPC;
@@ -182,14 +192,14 @@ int nvme_mdev_vns_open(struct nvme_mdev_vctrl *vctrl,
 	if (user_nsid > MAX_VIRTUAL_NAMESPACES)
 		goto error3;
 
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 
 	vctrl->namespaces[user_nsid - 1] = vns;
 	vns->nsid = user_nsid;
 
 	/* Announce the new namespace to the user */
-	nvme_mdev_vns_send_event(vctrl, user_nsid);
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_vns_send_event(vctrl, user_nsid);
+	__nvme_mdev_io_resume(vctrl);
 	ret = 0;
 	goto out;
 error3:
@@ -206,15 +216,15 @@ int nvme_mdev_vns_open(struct nvme_mdev_vctrl *vctrl,
 /* Re-open new reference to a host namespace, after notification
  * of change in the host namespace
  */
-static bool nvme_mdev_vns_reopen(struct nvme_mdev_vctrl *vctrl,
-				 struct nvme_mdev_vns *vns)
+static bool __nvme_mdev_vns_reopen(struct nvme_mdev_vctrl *vctrl,
+				   struct nvme_mdev_vns *vns)
 {
 	struct nvme_ns *host_ns;
 
 	lockdep_assert_held(&vctrl->lock);
 
-	_INFO(vctrl, "reopen host namespace %u, partition=%u\n",
-	      vns->host_nsid, vns->host_partid);
+	_INFO(vctrl, "reopen host namespace %u, partition=%u\n", vns->host_nsid,
+	      vns->host_partid);
 
 	/* namespace disappeared on the host - invalid*/
 	host_ns = nvme_find_get_ns(vctrl->hctrl->nvme_ctrl, vns->host_nsid);
@@ -227,14 +237,13 @@ static bool nvme_mdev_vns_reopen(struct nvme_mdev_vctrl *vctrl,
 
 	// basic checks on the namespace
 	if (test_bit(NVME_NS_DEAD, &host_ns->flags) ||
-	    test_bit(NVME_NS_REMOVING, &host_ns->flags) ||
-	    !host_ns->disk)
+	    test_bit(NVME_NS_REMOVING, &host_ns->flags) || !host_ns->disk)
 		goto error1;
 
 	/* read properties of the host namespace */
-	nvme_mdev_io_pause(vctrl);
-	nvme_mdev_vns_read_host_properties(vctrl, vns, host_ns);
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_vns_read_host_properties(vctrl, vns, host_ns);
+	__nvme_mdev_io_resume(vctrl);
 
 	nvme_put_ns(host_ns);
 	return true;
@@ -254,14 +263,14 @@ static int __nvme_mdev_vns_destroy(struct nvme_mdev_vctrl *vctrl, u32 user_nsid)
 	if (!vns)
 		return -ENODEV;
 
-	nvme_mdev_vns_send_event(vctrl, user_nsid);
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_vns_send_event(vctrl, user_nsid);
+	__nvme_mdev_io_pause(vctrl);
 
 	vctrl->namespaces[user_nsid - 1] = NULL;
 	blkdev_put(vns->host_part, vns->fmode);
 	nvme_put_ns(vns->host_ns);
 	kfree(vns);
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_io_resume(vctrl);
 	return 0;
 }
 
@@ -271,22 +280,22 @@ int nvme_mdev_vns_destroy(struct nvme_mdev_vctrl *vctrl, u32 user_nsid)
 	int ret;
 
 	mutex_lock(&vctrl->lock);
-	nvme_mdev_io_pause(vctrl);
+	__nvme_mdev_io_pause(vctrl);
 	ret = __nvme_mdev_vns_destroy(vctrl, user_nsid);
-	nvme_mdev_io_resume(vctrl);
+	__nvme_mdev_io_resume(vctrl);
 	mutex_unlock(&vctrl->lock);
 
 	return ret;
 }
 
 /* Destroy all virtual namespaces */
-void nvme_mdev_vns_destroy_all(struct nvme_mdev_vctrl *vctrl)
+void __nvme_mdev_vns_destroy_all(struct nvme_mdev_vctrl *vctrl)
 {
 	u32 user_nsid;
 
 	lockdep_assert_held(&vctrl->lock);
 
-	for (user_nsid = 1 ; user_nsid <= MAX_VIRTUAL_NAMESPACES ; user_nsid++)
+	for (user_nsid = 1; user_nsid <= MAX_VIRTUAL_NAMESPACES; user_nsid++)
 		__nvme_mdev_vns_destroy(vctrl, user_nsid);
 }
 
@@ -300,8 +309,8 @@ struct nvme_mdev_vns *nvme_mdev_vns_from_vnsid(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Print description off all virtual namespaces */
-int nvme_mdev_vns_print_description(struct nvme_mdev_vctrl *vctrl,
-				    char *buf, unsigned int size)
+int nvme_mdev_vns_print_description(struct nvme_mdev_vctrl *vctrl, char *buf,
+				    unsigned int size)
 {
 	int nsid, ret = 0;
 
@@ -309,19 +318,17 @@ int nvme_mdev_vns_print_description(struct nvme_mdev_vctrl *vctrl,
 
 	for (nsid = 1; nsid <= MAX_VIRTUAL_NAMESPACES; nsid++) {
 		int n;
-		struct nvme_mdev_vns *vns = nvme_mdev_vns_from_vnsid(vctrl,
-				nsid);
+		struct nvme_mdev_vns *vns =
+			nvme_mdev_vns_from_vnsid(vctrl, nsid);
 		if (!vns)
 			continue;
 
 		else if (vns->host_partid == 0)
-			n = snprintf(buf, size, "VNS%d: nvme%dn%d\n",
-				     nsid, vctrl->hctrl->id,
-				     (int)vns->host_nsid);
+			n = snprintf(buf, size, "VNS%d: nvme%dn%d\n", nsid,
+				     vctrl->hctrl->id, (int)vns->host_nsid);
 		else
-			n = snprintf(buf, size, "VNS%d: nvme%dn%dp%d\n",
-				     nsid, vctrl->hctrl->id,
-				     (int)vns->host_nsid,
+			n = snprintf(buf, size, "VNS%d: nvme%dn%dp%d\n", nsid,
+				     vctrl->hctrl->id, (int)vns->host_nsid,
 				     (int)vns->host_partid);
 		if (n > size)
 			return -ENOMEM;
@@ -334,23 +341,23 @@ int nvme_mdev_vns_print_description(struct nvme_mdev_vctrl *vctrl,
 }
 
 /* Processes an update on the host namespace */
-void nvme_mdev_vns_host_ns_update(struct nvme_mdev_vctrl *vctrl,
-				  u32 host_nsid, bool removed)
+void nvme_mdev_vns_host_ns_update(struct nvme_mdev_vctrl *vctrl, u32 host_nsid,
+				  bool removed)
 {
 	int nsid;
 
 	mutex_lock(&vctrl->lock);
 
 	for (nsid = 1; nsid <= MAX_VIRTUAL_NAMESPACES; nsid++) {
-		struct nvme_mdev_vns *vns = nvme_mdev_vns_from_vnsid(vctrl,
-								     nsid);
+		struct nvme_mdev_vns *vns =
+			nvme_mdev_vns_from_vnsid(vctrl, nsid);
 		if (!vns || vns->host_nsid != host_nsid)
 			continue;
 
-		if (removed || !nvme_mdev_vns_reopen(vctrl, vns))
+		if (removed || !__nvme_mdev_vns_reopen(vctrl, vns))
 			__nvme_mdev_vns_destroy(vctrl, nsid);
 		else
-			nvme_mdev_vns_send_event(vctrl, nsid);
+			__nvme_mdev_vns_send_event(vctrl, nsid);
 	}
 	mutex_unlock(&vctrl->lock);
 }
diff --git a/drivers/nvme/mdev/vsq.c b/drivers/nvme/mdev/vsq.c
index 5b63081c144d..adf981fc22ea 100644
--- a/drivers/nvme/mdev/vsq.c
+++ b/drivers/nvme/mdev/vsq.c
@@ -11,83 +11,110 @@
 #include "priv.h"
 
 /* Create new virtual completion queue */
-int nvme_mdev_vsq_init(struct nvme_mdev_vctrl *vctrl,
-		       u16 qid, dma_addr_t iova, bool cont, u16 size, u16 cqid)
+int __nvme_mdev_vsq_init(struct nvme_mdev_vctrl *vctrl, u16 qid,
+			 dma_addr_t iova, bool cont, u16 size, u16 cqid)
 {
-	struct nvme_vsq *q = &vctrl->vsqs[qid];
+	struct nvme_vsq *q;
 	int ret;
 
 	lockdep_assert_held(&vctrl->lock);
 
+	q = kzalloc(sizeof(*q), GFP_KERNEL);
+	if (!q)
+		return -ENOMEM;
+
+	q->vctrl = vctrl;
 	q->iova = iova;
 	q->cont = cont;
 	q->qid = qid;
 	q->size = size;
 	q->head = 0;
-	q->vcq = &vctrl->vcqs[cqid];
+	q->vcq = rcu_dereference_protected(vctrl->vcqs[cqid], 1);
 	q->data = NULL;
 	q->hsq = 0;
 
-	ret = nvme_mdev_vsq_viommu_update(&vctrl->viommu, q);
+#ifdef CONFIG_NVME_MDEV_BPF
+	q->ctx_data =
+		vzalloc(((long)U16_MAX + 1) * sizeof(struct nvme_cmd_ctx));
+	if (!q->ctx_data)
+		goto fail1;
+#endif
+
+	ret = __nvme_mdev_vsq_viommu_update(&vctrl->viommu, q);
 	if (ret && (ret != -EFAULT))
-		return ret;
+		goto fail2;
 
 	if (qid > 0) {
-		ret = nvme_mdev_vctrl_hq_alloc(vctrl);
+		ret = __nvme_mdev_vctrl_hq_alloc(vctrl);
 		if (ret < 0) {
 			vunmap(q->data);
-			return ret;
+			goto fail2;
 		}
 		q->hsq = ret;
 	}
 
-	_DBG(vctrl, "VSQ: create qid=%d contig=%d, depth=%d cqid=%d\n",
-	     qid, cont, size, cqid);
+	_DBG(vctrl, "VSQ: create qid=%d contig=%d, depth=%d cqid=%d\n", qid,
+	     cont, size, cqid);
 
-	set_bit(qid, vctrl->vsq_en);
+	rcu_assign_pointer(vctrl->vsqs[qid], q);
 
 	vctrl->mmio.dbs[q->qid].sqt = 0;
 	vctrl->mmio.eidxs[q->qid].sqt = 0;
 
 	return 0;
+
+fail2:
+#ifdef CONFIG_NVME_MDEV_BPF
+	vfree(q->ctx_data);
+#endif
+
+fail1:
+	kfree(q);
+	return ret;
 }
 
 /* Update the kernel mapping of the queue */
-int nvme_mdev_vsq_viommu_update(struct nvme_mdev_viommu *viommu,
-				struct nvme_vsq *q)
+int __nvme_mdev_vsq_viommu_update(struct nvme_mdev_viommu *viommu,
+				  struct nvme_vsq *q)
 {
 	void *data;
 
 	if (q->data)
 		vunmap((void *)q->data);
 
-	data = nvme_mdev_udata_queue_vmap(viommu, q->iova,
-					  (unsigned int)q->size *
-					  sizeof(struct nvme_command),
-					  q->cont);
+	data = nvme_mdev_udata_queue_vmap(
+		viommu, q->iova,
+		(unsigned int)q->size * sizeof(struct nvme_command), q->cont);
 
 	q->data = IS_ERR(data) ? NULL : data;
 	return IS_ERR(data) ? PTR_ERR(data) : 0;
 }
 
 /* Delete an virtual completion queue */
-void nvme_mdev_vsq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid)
+void __nvme_mdev_vsq_delete(struct nvme_mdev_vctrl *vctrl, u16 qid)
 {
-	struct nvme_vsq *q = &vctrl->vsqs[qid];
+	struct nvme_vsq *q;
 
-	lockdep_assert_held(&vctrl->lock);
+	q = rcu_replace_pointer(vctrl->vsqs[qid], NULL,
+				lockdep_is_held(&vctrl->lock));
 	_DBG(vctrl, "VSQ: delete qid=%d\n", q->qid);
 
+	synchronize_rcu();
 	if (q->data)
 		vunmap(q->data);
 	q->data = NULL;
 
+#ifdef CONFIG_NVME_MDEV_BPF
+	if (q->ctx_data)
+		vunmap(q->ctx_data);
+	q->ctx_data = NULL;
+#endif
+
 	if (q->hsq) {
-		nvme_mdev_vctrl_hq_free(vctrl, q->hsq);
+		__nvme_mdev_vctrl_hq_free(vctrl, q->hsq);
 		q->hsq = 0;
 	}
-
-	clear_bit(qid, vctrl->vsq_en);
+	kfree(q);
 }
 
 /* Move queue head one item forward */
@@ -98,15 +125,14 @@ static void nvme_mdev_vsq_advance_head(struct nvme_vsq *q)
 		q->head = 0;
 }
 
-bool nvme_mdev_vsq_has_data(struct nvme_mdev_vctrl *vctrl,
-			    struct nvme_vsq *q)
+bool nvme_mdev_vsq_has_data(struct nvme_mdev_vctrl *vctrl, struct nvme_vsq *q)
 {
 	u16 tail = le32_to_cpu(vctrl->mmio.dbs[q->qid].sqt);
 
 	if (!vctrl->mmio.dbs || !vctrl->mmio.eidxs || !q->data)
 		return false;
 
-	if  (tail == q->head)
+	if (tail == q->head)
 		return false;
 
 	if (!nvme_mdev_mmio_db_check(vctrl, q->qid, q->size, tail))
@@ -136,9 +162,9 @@ const struct nvme_command *nvme_mdev_vsq_get_cmd(struct nvme_mdev_vctrl *vctrl,
 	return &q->data[oldhead];
 }
 
-bool nvme_mdev_vsq_suspend_io(struct nvme_mdev_vctrl *vctrl, u16 sqid)
+bool __nvme_mdev_vsq_suspend_io(struct nvme_mdev_vctrl *vctrl,
+				struct nvme_vsq *q)
 {
-	struct nvme_vsq *q = &vctrl->vsqs[sqid];
 	u16 tail = le32_to_cpu(vctrl->mmio.dbs[q->qid].sqt);
 
 	/* If the queue is not in working state don't allow the idle code
@@ -164,18 +190,23 @@ bool nvme_mdev_vsq_suspend_io(struct nvme_mdev_vctrl *vctrl, u16 sqid)
 
 /* complete a command (IO version)*/
 void nvme_mdev_vsq_cmd_done_io(struct nvme_mdev_vctrl *vctrl,
-			       u16 sqid, u16 cid, u16 status)
+			       struct nvme_vsq *q, u16 cid, u16 status)
 {
-	struct nvme_vsq *q = &vctrl->vsqs[sqid];
-
+#ifdef CONFIG_NMBPF_DEBUG
+	_INFO(vctrl, "IOQ: QID %d CID %d done with status %#x\n", q->qid, cid,
+	      status);
+#endif
 	nvme_mdev_vcq_write_io(vctrl, q->vcq, q->head, q->qid, cid, status);
 }
 
 /* complete a command (ADMIN version)*/
-void nvme_mdev_vsq_cmd_done_adm(struct nvme_mdev_vctrl *vctrl,
-				u32 dw0, u16 cid, u16 status)
+void __nvme_mdev_vsq_cmd_done_adm(struct nvme_mdev_vctrl *vctrl, u32 dw0,
+				  u16 cid, u16 status)
 {
-	struct nvme_vsq *q = &vctrl->vsqs[0];
+	struct nvme_vsq *adm_sq = rcu_dereference_protected(
+		vctrl->vsqs[0], lockdep_is_held(&vctrl->lock));
+	BUG_ON(!adm_sq);
 
-	nvme_mdev_vcq_write_adm(vctrl, q->vcq, dw0, q->head, cid, status);
+	nvme_mdev_vcq_write_adm(vctrl, adm_sq->vcq, dw0, adm_sq->head, cid,
+				status);
 }
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index b821ac00c51c..181b1eeba836 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -2282,24 +2282,6 @@ static void vfio_iommu_unmap_unpin_reaccount(struct vfio_iommu *iommu)
 	}
 }
 
-static void vfio_sanity_check_pfn_list(struct vfio_iommu *iommu)
-{
-	struct rb_node *n;
-
-	n = rb_first(&iommu->dma_list);
-	for (; n; n = rb_next(n)) {
-		struct vfio_dma *dma;
-
-		dma = rb_entry(n, struct vfio_dma, node);
-
-		if (WARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list)))
-			break;
-	}
-	/* mdev vendor driver must unregister notifier */
-	WARN_ON(iommu->map_notifiers.head);
-	WARN_ON(iommu->unmap_notifiers.head);
-}
-
 /*
  * Called when a domain is removed in detach. It is possible that
  * the removed domain decided the iova aperture window. Modify the
@@ -2400,7 +2382,7 @@ static void vfio_iommu_type1_detach_group(void *iommu_data,
 
 			if (list_empty(&iommu->external_domain->group_list)) {
 				if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)) {
-					WARN_ON(iommu->notifier.head);
+					WARN_ON(iommu->unmap_notifiers.head);
 					vfio_iommu_unmap_unpin_all(iommu);
 				}
 
@@ -2437,7 +2419,7 @@ static void vfio_iommu_type1_detach_group(void *iommu_data,
 		if (list_empty(&domain->group_list)) {
 			if (list_is_singular(&iommu->domain_list)) {
 				if (!iommu->external_domain) {
-					WARN_ON(iommu->notifier.head);
+					WARN_ON(iommu->unmap_notifiers.head);
 					vfio_iommu_unmap_unpin_all(iommu);
 				} else {
 					vfio_iommu_unmap_unpin_reaccount(iommu);
diff --git a/include/linux/bpf_nvme_mdev.h b/include/linux/bpf_nvme_mdev.h
new file mode 100644
index 000000000000..916795bf2b8f
--- /dev/null
+++ b/include/linux/bpf_nvme_mdev.h
@@ -0,0 +1,74 @@
+#ifndef _BPF_NVME_MDEV_H
+#define _BPF_NVME_MDEV_H
+
+#include <linux/types.h>
+#include <linux/nvme.h>
+
+#ifdef CONFIG_NVME_MDEV_BPF
+
+/* request lifecycle:
+ * - bpf registers list of hooks it's interested in
+ * - bpf is called at each registered hook
+ * - bpf can have one of the following outcomes:
+ *   - immediate return with status
+ *   - send to predefined targets (sync or async)
+ *     and/or
+ *     install additional hook for the given request
+ * - request is completed (vcq is written) when either:
+ *   - bpf prog returns BPF_COMPLETE + status
+ *   - a request with BPF_WILL_COMPLETE_* finishes all of its synchronous sends
+ *   A request completed with BPF_COMPLETE may need to wait for remaining sync
+ *   sends. Such request enters a grace period until the sync sends are complete.
+ */
+
+/* hook order - a bpf program called on a request by a later hook
+ * cannot register for an earlier hook for that request:
+ * 1. vsq
+ * 2. hcq
+ * 2. notifyfd completion
+ * 3. pre-vcq
+ * vcq cannot be hooked
+ */
+
+struct bpf_io_ctx {
+	/* RW only during vsq hook; modifications during non-vsq hook lead to
+	 * undefined behavior
+	 */
+	struct nvme_command cmd;
+
+	/* RO */
+	__u32 sqid;
+	__u32 current_hook;
+	/* snapshot of iostate for the cmpxchg loop */
+	__u32 iostate;
+	__u32 data;
+};
+
+#define NMBPF_STATUS_MASK 0xFFFF
+#define NMBPF_HOOK_MASK 0x3F0000
+#define NMBPF_SEND_MASK 0xFC00000
+#define NMBPF_COMPLETION_MASK 0x70000000
+#define NMBPF_SEND_SHIFT 22
+#define NMBPF_COMPLETE_SHIFT 28
+#define WILL_COMPLETE_TO_SEND(cmpl_type)                                       \
+	(1 << (((cmpl_type) >> NMBPF_COMPLETE_SHIFT) + NMBPF_SEND_SHIFT - 1))
+enum nmbpf_actions {
+	NMBPF_HOOK_VSQ = 1 << 16,
+	NMBPF_HOOK_HCQ = 1 << 17,
+	NMBPF_HOOK_NFD_WRITE = 1 << 18,
+	NMBPF_HOOK_PRE_VCQ = 1 << 19,
+	/* SEND_* and WILL_COMPLETE_* must be in the same order
+	 * such that COMPLETE_TO_SEND(WILL_COMPLETE_X) == SEND_X
+	 */
+	NMBPF_SEND_HQ = 1 << (NMBPF_SEND_SHIFT + 0),
+	NMBPF_SEND_FD = 1 << (NMBPF_SEND_SHIFT + 1),
+	/* exclusive enum values */
+	NMBPF_COMPLETE = 0 << NMBPF_COMPLETE_SHIFT,
+	NMBPF_WILL_COMPLETE_HQ = 1 << NMBPF_COMPLETE_SHIFT,
+	NMBPF_WILL_COMPLETE_FD = 2 << NMBPF_COMPLETE_SHIFT,
+	NMBPF_WAIT_FOR_HOOK = 7 << NMBPF_COMPLETE_SHIFT,
+};
+
+#endif
+
+#endif
diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index a8137bb6dd3c..586683c8144f 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -77,6 +77,10 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LSM, lsm,
 	       void *, void *)
 #endif /* CONFIG_BPF_LSM */
 #endif
+#ifdef CONFIG_NVME_MDEV_BPF
+BPF_PROG_TYPE(BPF_PROG_TYPE_NVME_MDEV, nvme_mdev,
+	      void *, void *)
+#endif
 
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)
diff --git a/include/linux/notifier.h b/include/linux/notifier.h
index 2fb373a5c1ed..32325f0e0e6d 100644
--- a/include/linux/notifier.h
+++ b/include/linux/notifier.h
@@ -7,7 +7,7 @@
  *
  *				Alan Cox <Alan.Cox@linux.org>
  */
- 
+
 #ifndef _LINUX_NOTIFIER_H
 #define _LINUX_NOTIFIER_H
 #include <linux/errno.h>
@@ -163,6 +163,9 @@ extern int atomic_notifier_call_chain(struct atomic_notifier_head *nh,
 		unsigned long val, void *v);
 extern int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
 		unsigned long val, void *v);
+extern int __blocking_notifier_call_chain(struct blocking_notifier_head *nh,
+					  unsigned long val, void *v,
+					  int nr_to_call, int *nr_calls);
 extern int raw_notifier_call_chain(struct raw_notifier_head *nh,
 		unsigned long val, void *v);
 extern int srcu_notifier_call_chain(struct srcu_notifier_head *nh,
@@ -203,12 +206,12 @@ static inline int notifier_to_errno(int ret)
 
 /*
  *	Declared notifiers so far. I can imagine quite a few more chains
- *	over time (eg laptop power reset chains, reboot chain (to clean 
+ *	over time (eg laptop power reset chains, reboot chain (to clean
  *	device units up), device [un]mount chain, module load/unload chain,
- *	low memory chain, screenblank chain (for plug in modular screenblankers) 
+ *	low memory chain, screenblank chain (for plug in modular screenblankers)
  *	VC switch chains (for loadable kernel svgalib VC switch helpers) etc...
  */
- 
+
 /* CPU notfiers are defined in include/linux/cpu.h. */
 
 /* netdevice notifiers are defined in include/linux/netdevice.h */
diff --git a/include/uapi/linux/bpf.h b/include/uapi/linux/bpf.h
index 762bf87c26a3..1037f7d8d736 100644
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@ -199,6 +199,7 @@ enum bpf_prog_type {
 	BPF_PROG_TYPE_EXT,
 	BPF_PROG_TYPE_LSM,
 	BPF_PROG_TYPE_SK_LOOKUP,
+	BPF_PROG_TYPE_NVME_MDEV,
 };
 
 enum bpf_attach_type {
diff --git a/include/uapi/linux/nvme_mdev.h b/include/uapi/linux/nvme_mdev.h
new file mode 100644
index 000000000000..f4cd5277b249
--- /dev/null
+++ b/include/uapi/linux/nvme_mdev.h
@@ -0,0 +1,43 @@
+#ifndef _UAPI_LINUX_NVME_MDEV_H
+#define _UAPI_LINUX_NVME_MDEV_H
+
+#include <linux/types.h>
+
+#define NVME_MDEV_ATTACH_BPF _IOW('N', 0x80, int)
+#define NVME_MDEV_DETACH_BPF _IO('N', 0x81)
+#define NVME_MDEV_OPEN_FD _IOW('N', 0x82, struct nmntfy_open_arg)
+
+#define NVME_MDEV_NOTIFYFD_ID_VCTRL _IOR('N', 0x90, struct nvme_mdev_id_vctrl)
+#define NVME_MDEV_NOTIFYFD_ID_VNS _IOR('N', 0x91, struct nvme_mdev_id_vns)
+
+#define NMNTFY_SQ_DATA_OFFSET 0
+#define NMNTFY_SQ_DATA_NR_PAGES 8
+
+#define NMNTFY_SQH_OFFSET (NMNTFY_SQ_DATA_OFFSET + NMNTFY_SQ_DATA_NR_PAGES)
+#define NMNTFY_SQT_OFFSET (NMNTFY_SQH_OFFSET + 1)
+
+#define NMNTFY_CQ_DATA_OFFSET (NMNTFY_SQT_OFFSET + 1)
+#define NMNTFY_CQ_DATA_NR_PAGES 1
+
+#define NMNTFY_CQH_OFFSET (NMNTFY_CQ_DATA_OFFSET + NMNTFY_CQ_DATA_NR_PAGES)
+#define NMNTFY_CQT_OFFSET (NMNTFY_CQH_OFFSET + 1)
+
+struct nvme_mdev_id_vctrl {
+	__u8 data[4096];
+};
+
+struct nvme_mdev_id_vns {
+	__u32 nsid;
+	__u8 data[4096];
+};
+
+struct nmntfy_response {
+	__u16 ucid;
+	__u16 status;
+};
+
+struct nmntfy_open_arg {
+	__u16 sqid;
+};
+
+#endif /* _UAPI_LINUX_NVME_MDEV_IOCTL_H */
diff --git a/kernel/fork.c b/kernel/fork.c
index 9705439439fe..4c9a716a8d5a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1126,6 +1126,7 @@ void mmput_async(struct mm_struct *mm)
 		schedule_work(&mm->async_put_work);
 	}
 }
+EXPORT_SYMBOL(mmput_async);
 #endif
 
 /**
diff --git a/kernel/notifier.c b/kernel/notifier.c
index 1b019cbca594..15e39586925c 100644
--- a/kernel/notifier.c
+++ b/kernel/notifier.c
@@ -307,10 +307,12 @@ int blocking_notifier_call_chain_robust(struct blocking_notifier_head *nh,
 EXPORT_SYMBOL_GPL(blocking_notifier_call_chain_robust);
 
 /**
- *	blocking_notifier_call_chain - Call functions in a blocking notifier chain
+ *	__blocking_notifier_call_chain - Call functions in a blocking notifier chain
  *	@nh: Pointer to head of the blocking notifier chain
  *	@val: Value passed unmodified to notifier function
  *	@v: Pointer passed unmodified to notifier function
+ *	@nr_to_call: See comment for notifier_call_chain.
+ *	@nr_calls: See comment for notifier_call_chain.
  *
  *	Calls each function in a notifier chain in turn.  The functions
  *	run in a process context, so they are allowed to block.
@@ -322,8 +324,9 @@ EXPORT_SYMBOL_GPL(blocking_notifier_call_chain_robust);
  *	Otherwise the return value is the return value
  *	of the last notifier function called.
  */
-int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
-		unsigned long val, void *v)
+int __blocking_notifier_call_chain(struct blocking_notifier_head *nh,
+				   unsigned long val, void *v,
+				   int nr_to_call, int *nr_calls)
 {
 	int ret = NOTIFY_DONE;
 
@@ -334,11 +337,19 @@ int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
 	 */
 	if (rcu_access_pointer(nh->head)) {
 		down_read(&nh->rwsem);
-		ret = notifier_call_chain(&nh->head, val, v, -1, NULL);
+		ret = notifier_call_chain(&nh->head, val, v, nr_to_call,
+					  nr_calls);
 		up_read(&nh->rwsem);
 	}
 	return ret;
 }
+EXPORT_SYMBOL_GPL(__blocking_notifier_call_chain);
+
+int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
+		unsigned long val, void *v)
+{
+	return __blocking_notifier_call_chain(nh, val, v, -1, NULL);
+}
 EXPORT_SYMBOL_GPL(blocking_notifier_call_chain);
 
 /*
diff --git a/samples/bpf/.gitignore b/samples/bpf/.gitignore
index b2f29bc8dc43..52a4901bbc58 100644
--- a/samples/bpf/.gitignore
+++ b/samples/bpf/.gitignore
@@ -6,6 +6,7 @@ ibumad
 lathist
 lwt_len_hist
 map_perf_test
+nvme_mdev
 offwaketime
 per_socket_stats_example
 sampleip
diff --git a/samples/bpf/Makefile b/samples/bpf/Makefile
index aeebf5d12f32..2d28db343faa 100644
--- a/samples/bpf/Makefile
+++ b/samples/bpf/Makefile
@@ -54,6 +54,7 @@ tprogs-y += task_fd_query
 tprogs-y += xdp_sample_pkts
 tprogs-y += ibumad
 tprogs-y += hbm
+tprogs-y += nvme_mdev
 
 # Libbpf dependencies
 LIBBPF = $(TOOLS_PATH)/lib/bpf/libbpf.a
@@ -111,6 +112,7 @@ task_fd_query-objs := bpf_load.o task_fd_query_user.o $(TRACE_HELPERS)
 xdp_sample_pkts-objs := xdp_sample_pkts_user.o $(TRACE_HELPERS)
 ibumad-objs := bpf_load.o ibumad_user.o $(TRACE_HELPERS)
 hbm-objs := bpf_load.o hbm.o $(CGROUP_HELPERS)
+nvme_mdev-objs := bpf_load.o nvme_mdev_user.o
 
 # Tell kbuild to always build the programs
 always-y := $(tprogs-y)
@@ -172,6 +174,11 @@ always-y += ibumad_kern.o
 always-y += hbm_out_kern.o
 always-y += hbm_edt_kern.o
 always-y += xdpsock_kern.o
+always-y += nvme_mdev_kern.o
+always-y += nmbpf_encrypt_kern.o
+always-y += nmbpf_encrypt_ip_kern.o
+always-y += nmbpf_passthrough.o
+always-y += nmbpf_replicate.o
 
 ifeq ($(ARCH), arm)
 # Strip all except -D__LINUX_ARM_ARCH__ option needed to handle linux
diff --git a/samples/bpf/nmbpf_encrypt_ip_kern.c b/samples/bpf/nmbpf_encrypt_ip_kern.c
new file mode 100644
index 000000000000..0ddd9fff9f10
--- /dev/null
+++ b/samples/bpf/nmbpf_encrypt_ip_kern.c
@@ -0,0 +1,47 @@
+#include <linux/bpf.h>
+#include <linux/nvme.h>
+#include <linux/bpf_nvme_mdev.h>
+#include <bpf/bpf_helpers.h>
+
+char _license[] SEC("license") = "GPL";
+
+static int nm_do_vsq(struct bpf_io_ctx *ctx)
+{
+	switch (ctx->cmd.common.opcode) {
+	case nvme_cmd_read:
+		/* read commands that need ciphertext to be read first */
+		return NMBPF_SEND_HQ | NMBPF_HOOK_HCQ | NMBPF_WAIT_FOR_HOOK;
+	case nvme_cmd_write:
+		/* write commands that need encrypting */
+		return NMBPF_SEND_FD | NMBPF_HOOK_NFD_WRITE |
+		       NMBPF_WAIT_FOR_HOOK;
+	case nvme_cmd_write_zeroes:
+		return NMBPF_SEND_FD | NMBPF_WILL_COMPLETE_FD;
+	default:
+		return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+	}
+}
+
+SEC("nvme_mdev")
+int nvme_run_bpf(struct bpf_io_ctx *ctx)
+{
+	switch (ctx->current_hook) {
+	case NMBPF_HOOK_VSQ:
+		return nm_do_vsq(ctx);
+
+	case NMBPF_HOOK_HCQ:
+		/* read commands that have completed reading ciphertext */
+		if (ctx->data)
+			return ctx->data | NMBPF_COMPLETE;
+		return NMBPF_SEND_FD | NMBPF_WILL_COMPLETE_FD;
+
+	case NMBPF_HOOK_NFD_WRITE:
+		/* write command that has been encrypted */
+		if (ctx->data)
+			return ctx->data | NMBPF_COMPLETE;
+		return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+
+	default:
+		return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+	}
+}
diff --git a/samples/bpf/nmbpf_encrypt_kern.c b/samples/bpf/nmbpf_encrypt_kern.c
new file mode 100644
index 000000000000..af238f9349c5
--- /dev/null
+++ b/samples/bpf/nmbpf_encrypt_kern.c
@@ -0,0 +1,41 @@
+#include <linux/bpf.h>
+#include <linux/nvme.h>
+#include <linux/bpf_nvme_mdev.h>
+#include <bpf/bpf_helpers.h>
+
+char _license[] SEC("license") = "GPL";
+
+static int nm_do_vsq(struct bpf_io_ctx *ctx)
+{
+	switch (ctx->cmd.common.opcode) {
+	case nvme_cmd_read:
+		/* read commands that need ciphertext to be read first */
+		return NMBPF_SEND_HQ | NMBPF_HOOK_HCQ | NMBPF_WAIT_FOR_HOOK;
+	case nvme_cmd_write:
+		/* write commands that need encrypting */
+		/* blkdev writing will be done by userspace */
+		return NMBPF_SEND_FD | NMBPF_WILL_COMPLETE_FD;
+	case nvme_cmd_write_zeroes:
+		return NMBPF_SEND_FD | NMBPF_WILL_COMPLETE_FD;
+	default:
+		return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+	}
+}
+
+SEC("nvme_mdev")
+int nvme_run_bpf(struct bpf_io_ctx *ctx)
+{
+	switch (ctx->current_hook) {
+	case NMBPF_HOOK_VSQ:
+		return nm_do_vsq(ctx);
+
+	case NMBPF_HOOK_HCQ:
+		/* read commands that have completed reading ciphertext */
+		if (ctx->data)
+			return ctx->data | NMBPF_COMPLETE;
+		return NMBPF_SEND_FD | NMBPF_WILL_COMPLETE_FD;
+
+	default:
+		return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+	}
+}
diff --git a/samples/bpf/nmbpf_passthrough.c b/samples/bpf/nmbpf_passthrough.c
new file mode 100644
index 000000000000..ac61af8cfc49
--- /dev/null
+++ b/samples/bpf/nmbpf_passthrough.c
@@ -0,0 +1,12 @@
+#include <linux/bpf.h>
+#include <linux/nvme.h>
+#include <linux/bpf_nvme_mdev.h>
+#include <bpf/bpf_helpers.h>
+
+char _license[] SEC("license") = "GPL";
+
+SEC("nvme_mdev")
+int nvme_run_bpf(struct bpf_io_ctx *ctx)
+{
+	return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+}
diff --git a/samples/bpf/nmbpf_replicate.c b/samples/bpf/nmbpf_replicate.c
new file mode 100644
index 000000000000..79978dd929c7
--- /dev/null
+++ b/samples/bpf/nmbpf_replicate.c
@@ -0,0 +1,18 @@
+#include <linux/bpf.h>
+#include <linux/nvme.h>
+#include <linux/bpf_nvme_mdev.h>
+#include <bpf/bpf_helpers.h>
+
+char _license[] SEC("license") = "GPL";
+
+SEC("nvme_mdev")
+int nvme_run_bpf(struct bpf_io_ctx *ctx)
+{
+	switch (ctx->cmd.common.opcode) {
+	case nvme_cmd_write:
+	case nvme_cmd_write_zeroes:
+		return NMBPF_SEND_HQ | NMBPF_SEND_FD | NMBPF_WILL_COMPLETE_HQ;
+	default:
+		return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+	}
+}
diff --git a/samples/bpf/nvme_mdev_kern.c b/samples/bpf/nvme_mdev_kern.c
new file mode 100644
index 000000000000..3582aa4f39db
--- /dev/null
+++ b/samples/bpf/nvme_mdev_kern.c
@@ -0,0 +1,18 @@
+#include <linux/bpf.h>
+#include <linux/nvme.h>
+#include <linux/bpf_nvme_mdev.h>
+#include <bpf/bpf_helpers.h>
+
+char _license[] SEC("license") = "GPL";
+
+SEC("nvme_mdev")
+int nvme_run_bpf(struct bpf_io_ctx *ctx)
+{
+	if (ctx->cmd.common.opcode == nvme_cmd_read) {
+		return NMBPF_SEND_FD | NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+	} else if (ctx->cmd.common.opcode == nvme_cmd_write ||
+		   ctx->cmd.common.opcode == nvme_cmd_write_zeroes) {
+		return NMBPF_SEND_FD | NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+	}
+	return NMBPF_SEND_HQ | NMBPF_WILL_COMPLETE_HQ;
+}
diff --git a/samples/bpf/nvme_mdev_user.c b/samples/bpf/nvme_mdev_user.c
new file mode 100644
index 000000000000..68b7939e070f
--- /dev/null
+++ b/samples/bpf/nvme_mdev_user.c
@@ -0,0 +1,100 @@
+#include <stdio.h>
+#include <errno.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <linux/bpf.h>
+#include <sys/ioctl.h>
+#include <linux/nvme_mdev.h>
+#include <linux/vfio.h>
+#include <bpf/bpf.h>
+#include <bpf/libbpf.h>
+
+int main(int argc, char **argv)
+{
+	int vcfd;
+
+	char *vg;
+	int vgfd;
+	struct vfio_group_status group_status = { .argsz = sizeof(
+							  group_status) };
+
+	char *mdev;
+	int mdevfd;
+
+	struct bpf_object *obj;
+	char *prog;
+	int progfd;
+
+	if (argc < 3) {
+		fprintf(stderr,
+			"usage: nvme_mdev_user <path to vfio group> <mdev uuid> [<bpf object>]\n");
+		return 1;
+	}
+
+	vcfd = open("/dev/vfio/vfio", O_RDWR);
+	if (vcfd < 0) {
+		perror("cannot open vfio container");
+		return 1;
+	}
+
+	if (ioctl(vcfd, VFIO_GET_API_VERSION) != VFIO_API_VERSION) {
+		fprintf(stderr, "unknown vfio version\n");
+		return 1;
+	}
+
+	if (!ioctl(vcfd, VFIO_CHECK_EXTENSION, VFIO_TYPE1_IOMMU)) {
+		fprintf(stderr, "unsupported iommu\n");
+		return 1;
+	}
+
+	vg = argv[1];
+	vgfd = open(vg, O_RDWR);
+	if (vgfd < 0) {
+		perror("cannot open vfio group");
+		return 1;
+	}
+
+	ioctl(vgfd, VFIO_GROUP_GET_STATUS, &group_status);
+	if (!(group_status.flags & VFIO_GROUP_FLAGS_VIABLE)) {
+		fprintf(stderr, "group not viable\n");
+		return 1;
+	}
+
+	if (ioctl(vgfd, VFIO_GROUP_SET_CONTAINER, &vcfd) < 0) {
+		perror("cannot set container");
+		return 1;
+	}
+
+	if (ioctl(vcfd, VFIO_SET_IOMMU, VFIO_TYPE1_IOMMU) < 0) {
+		perror("cannot set iommu");
+		return 1;
+	}
+
+	mdev = argv[2];
+	mdevfd = ioctl(vgfd, VFIO_GROUP_GET_DEVICE_FD, mdev);
+	if (mdevfd < 0) {
+		perror("cannot open mdev");
+		return 1;
+	}
+
+	if (argc == 4) {
+		prog = argv[3];
+		if (bpf_prog_load(prog, BPF_PROG_TYPE_NVME_MDEV, &obj,
+				  &progfd)) {
+			fprintf(stderr, "cannot load bpf program\n");
+			return 1;
+		}
+
+		if (ioctl(mdevfd, NVME_MDEV_ATTACH_BPF, progfd) < 0) {
+			perror("cannot attach program");
+			return 1;
+		}
+	} else {
+		if (ioctl(mdevfd, NVME_MDEV_DETACH_BPF) < 0) {
+			perror("cannot detach program");
+			return 1;
+		}
+	}
+
+	return 0;
+}
diff --git a/tools/bpf/bpftool/prog.c b/tools/bpf/bpftool/prog.c
index 14237ffb90ba..4a800a3254d9 100644
--- a/tools/bpf/bpftool/prog.c
+++ b/tools/bpf/bpftool/prog.c
@@ -64,6 +64,7 @@ const char * const prog_type_name[] = {
 	[BPF_PROG_TYPE_EXT]			= "ext",
 	[BPF_PROG_TYPE_LSM]			= "lsm",
 	[BPF_PROG_TYPE_SK_LOOKUP]		= "sk_lookup",
+	[BPF_PROG_TYPE_NVME_MDEV]		= "nvme_mdev",
 };
 
 const size_t prog_type_name_size = ARRAY_SIZE(prog_type_name);
diff --git a/tools/include/uapi/linux/bpf.h b/tools/include/uapi/linux/bpf.h
index 762bf87c26a3..1037f7d8d736 100644
--- a/tools/include/uapi/linux/bpf.h
+++ b/tools/include/uapi/linux/bpf.h
@@ -199,6 +199,7 @@ enum bpf_prog_type {
 	BPF_PROG_TYPE_EXT,
 	BPF_PROG_TYPE_LSM,
 	BPF_PROG_TYPE_SK_LOOKUP,
+	BPF_PROG_TYPE_NVME_MDEV,
 };
 
 enum bpf_attach_type {
diff --git a/tools/lib/bpf/libbpf.c b/tools/lib/bpf/libbpf.c
index 28923b776cdc..b0de43ce4331 100644
--- a/tools/lib/bpf/libbpf.c
+++ b/tools/lib/bpf/libbpf.c
@@ -6567,7 +6567,7 @@ static int bpf_object__collect_relos(struct bpf_object *obj)
 
 	for (i = 0; i < obj->nr_programs; i++) {
 		struct bpf_program *p = &obj->programs[i];
-		
+
 		if (!p->nr_reloc)
 			continue;
 
diff --git a/tools/lib/bpf/libbpf_probes.c b/tools/lib/bpf/libbpf_probes.c
index d38284a3aaf0..3c1b3e9b2c07 100644
--- a/tools/lib/bpf/libbpf_probes.c
+++ b/tools/lib/bpf/libbpf_probes.c
@@ -111,6 +111,7 @@ probe_load(enum bpf_prog_type prog_type, const struct bpf_insn *insns,
 	case BPF_PROG_TYPE_STRUCT_OPS:
 	case BPF_PROG_TYPE_EXT:
 	case BPF_PROG_TYPE_LSM:
+	case BPF_PROG_TYPE_NVME_MDEV:
 	default:
 		break;
 	}
-- 
2.25.1

